var documenterSearchIndex = {"docs":
[{"location":"showcase/missing_physics/#autocomplete","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","text":"One of the most time-consuming parts of modeling is building the model. How do you know when your model is correct? When you solve an inverse problem to calibrate your model to data, who you gonna call if there are no parameters that make the model the data? This is the problem that the Universal Differential Equation (UDE) approach solves: the ability to start from the model you have, and suggest minimal mechanistic extensions that would allow the model to fit the data. In this showcase, we will show how to take a partially correct model and auto-complete it to find the missing physics.\n\nnote: Note\nFor a scientific background on the universal differential equation approach, check out Universal Differential Equations for Scientific Machine Learning","category":"section"},{"location":"showcase/missing_physics/#Starting-Point:-The-Packages-To-Use","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Starting Point: The Packages To Use","text":"There are many packages which are used as part of this showcase. Let's detail what they are and how they are used. For the neural network training:\n\nModule Description\nOrdinaryDiffEq.jl (DifferentialEquations.jl) The numerical differential equation solvers\nSciMLSensitivity.jl The adjoint methods, defines gradients of ODE solvers\nOptimization.jl The optimization library\nOptimizationOptimisers.jl The optimization solver package with Adam\nOptimizationOptimJL.jl The optimization solver package with LBFGS\nLineSearches.jl Line search algorithms package to be used with LBFGS\nComponentArrays.jl For the ComponentArray type to match Lux to SciML\n\nFor the symbolic model discovery:\n\nModule Description\nModelingToolkit.jl The symbolic modeling environment\nDataDrivenDiffEq.jl The symbolic regression interface\nDataDrivenSparse.jl The sparse regression symbolic regression solvers\nZygote.jl The automatic differentiation library for fast gradients\n\nJulia standard libraries:\n\nModule Description\nLinearAlgebra Required for the norm function\nStatistics Required for the mean function\n\nAnd external libraries:\n\nModule Description\nLux.jl The deep learning (neural network) framework\nPlots.jl The plotting and visualization library\nStableRNGs.jl Stable random seeding\n\nnote: Note\nThe deep learning framework Flux.jl could be used in place of Lux, though most tutorials in SciML generally prefer Lux.jl due to its explicit parameter interface, leading to nicer code. Both share the same internal implementations of core kernels, and thus have very similar feature support and performance.\n\n# SciML Tools\nimport OrdinaryDiffEq as ODE\nimport ModelingToolkit as MTK\nimport DataDrivenDiffEq\nimport SciMLSensitivity as SMS\nimport DataDrivenSparse\nimport Optimization as OPT\nimport OptimizationOptimisers\nimport OptimizationOptimJL\nimport LineSearches\n\n# Standard Libraries\nimport LinearAlgebra\nimport Statistics\n\n# External Libraries\nimport ComponentArrays\nimport Lux\nimport Zygote\nimport Plots\nimport StableRNGs\nPlots.gr()\n\n# Set a random seed for reproducible behaviour\nrng = StableRNGs.StableRNG(1111)","category":"section"},{"location":"showcase/missing_physics/#Problem-Setup","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Problem Setup","text":"In order to know that we have automatically discovered the correct model, we will use generated data from a known model. This model will be the Lotka-Volterra equations. These equations are given by:\n\nbeginaligned\nfracdxdt = alpha x - beta x y      \nfracdydt = -delta y + gamma x y    \nendaligned\n\nThis is a model of rabbits and wolves. alpha x is the exponential growth of rabbits in isolation, -beta x y and gamma x y are the interaction effects of wolves eating rabbits, and -delta y is the term for how wolves die hungry in isolation.\n\nNow assume that we have never seen rabbits and wolves in the same room. We only know the two effects alpha x and -delta y. Can we use Scientific Machine Learning to automatically discover an extension to what we already know? That is what we will solve with the universal differential equation.","category":"section"},{"location":"showcase/missing_physics/#Generating-the-Training-Data","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Generating the Training Data","text":"First, let's generate training data from the Lotka-Volterra equations. This is straightforward and standard DifferentialEquations.jl usage. Our sample data is thus generated as follows:\n\nfunction lotka!(du, u, p, t)\n    Î±, Î², Î³, Î´ = p\n    du[1] = Î± * u[1] - Î² * u[2] * u[1]\n    du[2] = Î³ * u[1] * u[2] - Î´ * u[2]\nend\n\n# Define the experimental parameter\ntspan = (0.0, 5.0)\nu0 = 5.0f0 * rand(rng, 2)\np_ = [1.3, 0.9, 0.8, 1.8]\nprob = ODE.ODEProblem(lotka!, u0, tspan, p_)\nsolution = ODE.solve(prob, ODE.Vern7(), abstol = 1e-12, reltol = 1e-12, saveat = 0.25)\n\n# Add noise in terms of the mean\nX = Array(solution)\nt = solution.t\n\nxÌ„ = Statistics.mean(X, dims = 2)\nnoise_magnitude = 5e-3\nXâ‚™ = X .+ (noise_magnitude * xÌ„) .* randn(rng, eltype(X), size(X))\n\nPlots.Plots.plot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\nPlots.scatter!(t, transpose(Xâ‚™), color = :red, label = [\"Noisy Data\" nothing])","category":"section"},{"location":"showcase/missing_physics/#Definition-of-the-Universal-Differential-Equation","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Definition of the Universal Differential Equation","text":"Now let's define our UDE. We will use Lux.jl to define the neural network as follows:\n\nrbf(x) = exp.(-(x .^ 2))\n\n# Multilayer FeedForward\nconst U = Lux.Chain(Lux.Dense(2, 5, rbf), Lux.Dense(5, 5, rbf), Lux.Dense(5, 5, rbf),\n    Lux.Dense(5, 2))\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\nconst _st = st\n\nWe then define the UDE as a dynamical system that is u' = known(u) + NN(u) like:\n\n# Define the hybrid model\nfunction ude_dynamics!(du, u, p, t, p_true)\n    uÌ‚ = U(u, p, _st)[1] # Network prediction\n    du[1] = p_true[1] * u[1] + uÌ‚[1]\n    du[2] = -p_true[4] * u[2] + uÌ‚[2]\nend\n\n# Closure with the known parameter\nnn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n# Define the problem\nprob_nn = ODE.ODEProblem(nn_dynamics!, Xâ‚™[:, 1], tspan, p)\n\nNotice that the most important part of this is that the neural network does not have hard-coded weights. The weights of the neural network are the parameters of the ODE system. This means that if we change the parameters of the ODE system, then we will have updated the internal neural networks to new weights. Keep that in mind for the next part.\n\n... and tada: now we have a neural network integrated into our dynamical system!\n\nnote: Note\nEven if the known physics is only approximate or correct, it can be helpful to improve the fitting process! Check out this JuliaCon talk which dives into this issue.","category":"section"},{"location":"showcase/missing_physics/#Setting-Up-the-Training-Loop","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Setting Up the Training Loop","text":"Now let's build a training loop around our UDE. First, let's make a function predict which runs our simulation at new neural network weights. Recall that the weights of the neural network are the parameters of the ODE, so what we need to do in predict is update our ODE to our new parameters and then run it.\n\nFor this update step, we will use the remake function from the SciMLProblem interface. remake works by specifying key = value pairs to update in the problem fields. Thus to update u0, we would add a keyword argument u0 = .... To update the parameters, we'd do p = .... The field names can be acquired from the problem documentation (or the docstrings!).\n\nKnowing this, our predict function looks like:\n\nfunction predict(Î¸, X = Xâ‚™[:, 1], T = t)\n    _prob = ODE.remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = Î¸)\n    Array(ODE.solve(_prob, ODE.Vern7(), saveat = T,\n        abstol = 1e-6, reltol = 1e-6,\n        sensealg = SMS.QuadratureAdjoint(autojacvec = SMS.ReverseDiffVJP(true))))\nend\n\nThere are many choices for the combination of sensitivity algorithm and automatic differentiation library (see Choosing a Sensitivity Algorithm. For example, you could have used sensealg=ForwardDiffSensitivity().\n\nNow, for our loss function, we solve the ODE at our new parameters and check its L2 loss against the dataset. Using our predict function, this looks like:\n\nfunction loss(Î¸)\n    XÌ‚ = predict(Î¸)\n    Statistics.mean(abs2, Xâ‚™ .- XÌ‚)\nend\n\nLastly, what we will need to track our optimization is to define a callback as defined by the OptimizationProblem's solve interface. Because our function only returns one value, the loss l, the callback will be a function of the current parameters Î¸ and l. Let's setup a callback prints every 50 steps the current loss:\n\nlosses = Float64[]\n\ncallback = function (state, l)\n    push!(losses, l)\n    if length(losses) % 50 == 0\n        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n    end\n    return false\nend","category":"section"},{"location":"showcase/missing_physics/#Training","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Training","text":"Now we're ready to train! To run the training process, we will need to build an OptimizationProblem. Because we have a lot of parameters, we will use Zygote.jl. Optimization.jl makes the choice of automatic differentiation easy, just by specifying an adtype in the OptimizationFunction construction\n\nKnowing this, we can build our OptimizationProblem as follows:\n\nadtype = OPT.AutoZygote()\noptf = OPT.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = OPT.OptimizationProblem(optf, ComponentArrays.ComponentVector{Float64}(p))\n\nNow... we optimize it. We will use a mixed strategy. First, let's do some iterations of ADAM because it's better at finding a good general area of parameter space, but then we will move to BFGS which will quickly hone in on a local minimum. Note that if we only use ADAM it will take a ton of iterations, and if we only use BFGS we normally end up in a bad local minimum, so this combination tends to be a good one for UDEs.\n\nThus we first solve the optimization problem with ADAM. Choosing a learning rate of 0.1 (tuned to be as high as possible that doesn't tend to make the loss shoot up), we see:\n\nres1 = OPT.solve(\n    optprob, OptimizationOptimisers.Adam(), callback = callback, maxiters = 5000)\nprintln(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n\nNow we use the optimization result of the first run as the initial condition of the second optimization, and run it with BFGS. This looks like:\n\noptprob2 = OPT.OptimizationProblem(optf, res1.u)\nres2 = OPT.solve(\n    optprob2, OptimizationOptimJL.BFGS(linesearch = LineSearches.BackTracking()), callback = callback, maxiters = 1000)\nprintln(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n\n# Rename the best candidate\np_trained = res2.u\n\nand bingo, we have a trained UDE.","category":"section"},{"location":"showcase/missing_physics/#Visualizing-the-Trained-UDE","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Visualizing the Trained UDE","text":"How well did our neural network do? Let's take a look:\n\n# Plot the losses\npl_losses = Plots.Plots.plot(1:5000, losses[1:5000], yaxis = :log10, xaxis = :log10,\n    xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\nPlots.Plots.plot!(5001:length(losses), losses[5001:end], yaxis = :log10, xaxis = :log10,\n    xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\nNext, we compare the original data to the output of the UDE predictor. Note that we can even create more samples from the underlying model by simply adjusting the time steps!\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):(Statistics.mean(diff(solution.t)) / 2):last(solution.t)\nXÌ‚ = predict(p_trained, Xâ‚™[:, 1], ts)\n# Trained on noisy data vs real solution\npl_trajectory = Plots.Plots.plot(ts, transpose(XÌ‚), xlabel = \"t\", ylabel = \"x(t), y(t)\", color = :red,\n    label = [\"UDE Approximation\" nothing])\nPlots.scatter!(solution.t, transpose(Xâ‚™), color = :black, label = [\"Measurements\" nothing])\n\nLet's see how well the unknown term has been approximated:\n\n# Ideal unknown interactions of the predictor\nYÌ„ = [-p_[2] * (XÌ‚[1, :] .* XÌ‚[2, :])'; p_[3] * (XÌ‚[1, :] .* XÌ‚[2, :])']\n# Neural network guess\nYÌ‚ = U(XÌ‚, p_trained, st)[1]\n\npl_reconstruction = Plots.plot(ts, transpose(YÌ‚), xlabel = \"t\", ylabel = \"U(x,y)\", color = :red,\n    label = [\"UDE Approximation\" nothing])\nPlots.plot!(ts, transpose(YÌ„), color = :black, label = [\"True Interaction\" nothing])\n\nAnd have a nice look at all the information:\n\n# Plot the error\npl_reconstruction_error = Plots.plot(ts, LinearAlgebra.norm.(eachcol(YÌ„ - YÌ‚)), yaxis = :log, xlabel = \"t\",\n    ylabel = \"L2-Error\", label = nothing, color = :red)\npl_missing = Plots.plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1))\n\npl_overall = Plots.plot(pl_trajectory, pl_missing)\n\nThat looks pretty good. And if we are happy with deep learning, we can leave it at that: we have trained a neural network to capture our missing dynamics.\n\nBut...\n\nCan we also make it print out the LaTeX for what the missing equations were? Find out more after the break!","category":"section"},{"location":"showcase/missing_physics/#Symbolic-regression-via-sparse-regression-(SINDy-based)","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Symbolic regression via sparse regression (SINDy based)","text":"","category":"section"},{"location":"showcase/missing_physics/#This-part-of-the-showcase-is-still-a-work-in-progress...-shame-on-us.-But-be-back-in-a-jiffy-and-we'll-have-it-done.","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"This part of the showcase is still a work in progress... shame on us. But be back in a jiffy and we'll have it done.","text":"Okay, that was a quick break, and that's good because this next part is pretty cool. Let's use DataDrivenDiffEq.jl to transform our trained neural network from machine learning mumbo jumbo into predictions of missing mechanistic equations. To do this, we first generate a symbolic basis that represents the space of mechanistic functions we believe this neural network should map to. Let's choose a bunch of polynomial functions:\n\nMTK.@variables u[1:2]\nb = DataDrivenDiffEq.polynomial_basis(u, 4)\nbasis = DataDrivenDiffEq.Basis(b, u);\n\nNow let's define our DataDrivenProblems for the sparse regressions. To assess the capability of the sparse regression, we will look at 3 cases:\n\nWhat if we trained no neural network and tried to automatically uncover the equations from the original noisy data? This is the approach in the literature known as structural identification of dynamical systems (SINDy). We will call this the full problem. This will assess whether this incorporation of prior information was helpful.\nWhat if we trained the neural network using the ideal right-hand side missing derivative functions? This is the value computed in the plots above as YÌ„. This will tell us whether the symbolic discovery could work in ideal situations.\nDo the symbolic regression directly on the function y = NN(x), i.e. the trained learned neural network. This is what we really want, and will tell us how to extend our known equations.\n\nTo define the full problem, we need to define a DataDrivenProblem that has the time series of the solution X, the time points of the solution t, and the derivative at each time point of the solution, obtained by the ODE solution's interpolation. We can just use an interpolation to get the derivative:\n\nfull_problem = DataDrivenDiffEq.ContinuousDataDrivenProblem(Xâ‚™, t)\n\nNow for the other two symbolic regressions, we are regressing input/outputs of the missing terms, and thus we directly define the datasets as the input/output mappings like:\n\nideal_problem = DataDrivenDiffEq.DirectDataDrivenProblem(XÌ‚, YÌ„)\nnn_problem = DataDrivenDiffEq.DirectDataDrivenProblem(XÌ‚, YÌ‚)\n\nLet's solve the data-driven problems using sparse regression. We will use the ADMM method, which requires we define a set of shrinking cutoff values Î», and we do this like:\n\nÎ» = 1e-1\nopt = DataDrivenSparse.ADMM(Î»)\n\nThis is one of many methods for sparse regression, consult the DataDrivenDiffEq.jl documentation for more information on the algorithm choices. Taking this, let's solve each of the sparse regressions:\n\noptions = DataDrivenDiffEq.DataDrivenCommonOptions(maxiters = 10_000,\n    normalize = DataDrivenDiffEq.DataNormalization(DataDrivenDiffEq.ZScoreTransform),\n    selector = DataDrivenDiffEq.bic, digits = 1,\n    data_processing = DataDrivenDiffEq.DataProcessing(split = 0.9,\n        batchsize = 30,\n        shuffle = true,\n        rng = StableRNGs.StableRNG(1111)))\n\nfull_res = DataDrivenDiffEq.solve(full_problem, basis, opt, options = options)\nfull_eqs = DataDrivenDiffEq.get_basis(full_res)\nprintln(full_res)\n\noptions = DataDrivenDiffEq.DataDrivenCommonOptions(maxiters = 10_000,\n    normalize = DataDrivenDiffEq.DataNormalization(DataDrivenDiffEq.ZScoreTransform),\n    selector = DataDrivenDiffEq.bic, digits = 1,\n    data_processing = DataDrivenDiffEq.DataProcessing(split = 0.9,\n        batchsize = 30,\n        shuffle = true,\n        rng = StableRNGs.StableRNG(1111)))\n\nideal_res = DataDrivenDiffEq.solve(ideal_problem, basis, opt, options = options)\nideal_eqs = DataDrivenDiffEq.get_basis(ideal_res)\nprintln(ideal_res)\n\noptions = DataDrivenDiffEq.DataDrivenCommonOptions(maxiters = 10_000,\n    normalize = DataDrivenDiffEq.DataNormalization(DataDrivenDiffEq.ZScoreTransform),\n    selector = DataDrivenDiffEq.bic, digits = 1,\n    data_processing = DataDrivenDiffEq.DataProcessing(split = 0.9,\n        batchsize = 30,\n        shuffle = true,\n        rng = StableRNGs.StableRNG(1111)))\n\nnn_res = DataDrivenDiffEq.solve(nn_problem, basis, opt, options = options)\nnn_eqs = DataDrivenDiffEq.get_basis(nn_res)\nprintln(nn_res)\n\nNote that we passed the identical options into each of the solve calls to get the same data for each call.\n\nWe already saw that the full problem has failed to identify the correct equations of motion. To have a closer look, we can inspect the corresponding equations:\n\nfor eqs in (full_eqs, ideal_eqs, nn_eqs)\n    println(eqs)\n    println(DataDrivenDiffEq.get_parameter_map(eqs))\n    println()\nend\n\nNext, we want to predict with our model. To do so, we embed the basis into a function like before:\n\n# Define the recovered, hybrid model\nfunction recovered_dynamics!(du, u, p, t)\n    uÌ‚ = nn_eqs(u, p) # Recovered equations\n    du[1] = p_[1] * u[1] + uÌ‚[1]\n    du[2] = -p_[4] * u[2] + uÌ‚[2]\nend\n\nestimation_prob = ODE.ODEProblem(recovered_dynamics!, u0, tspan, DataDrivenDiffEq.get_parameter_values(nn_eqs))\nestimate = ODE.solve(estimation_prob, ODE.Tsit5(), saveat = solution.t)\n\n# Plot\nPlots.plot(solution)\nPlots.plot!(estimate)\n\nWe are still a bit off, so we fine tune the parameters by simply minimizing the residuals between the UDE predictor and our recovered parametrized equations:\n\nfunction parameter_loss(p)\n    Y = reduce(hcat, map(Base.Fix2(nn_eqs, p), eachcol(XÌ‚)))\n    sum(abs2, YÌ‚ .- Y)\nend\n\noptf = OPT.OptimizationFunction((x, p) -> parameter_loss(x), adtype)\noptprob = OPT.OptimizationProblem(optf, DataDrivenDiffEq.get_parameter_values(nn_eqs))\nparameter_res = OPT.solve(optprob, OptimizationOptimJL.LBFGS(), maxiters = 1000)","category":"section"},{"location":"showcase/missing_physics/#Simulation","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Simulation","text":"# Look at long term prediction\nt_long = (0.0, 50.0)\nestimation_prob = ODE.ODEProblem(recovered_dynamics!, u0, t_long, parameter_res)\nestimate_long = ODE.solve(estimation_prob, ODE.Tsit5(), saveat = 0.1) # Using higher tolerances here results in exit of julia\nPlots.plot(estimate_long)\n\ntrue_prob = ODE.ODEProblem(lotka!, u0, t_long, p_)\ntrue_solution_long = ODE.solve(true_prob, ODE.Tsit5(), saveat = estimate_long.t)\nPlots.plot!(true_solution_long)","category":"section"},{"location":"showcase/missing_physics/#Post-Processing-and-Plots","page":"Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations","title":"Post Processing and Plots","text":"c1 = 3 # RGBA(174/255,192/255,201/255,1) # Maroon\nc2 = :orange # RGBA(132/255,159/255,173/255,1) # Red\nc3 = :blue # RGBA(255/255,90/255,0,1) # Orange\nc4 = :purple # RGBA(153/255,50/255,204/255,1) # Purple\n\np1 = Plots.plot(t, abs.(Array(solution) .- estimate)' .+ eps(Float32),\n    lw = 3, yaxis = :log, title = \"Timeseries of UODE Error\",\n    color = [3 :orange], xlabel = \"t\",\n    label = [\"x(t)\" \"y(t)\"],\n    titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n    legend = :topright)\n\n# Plot Lâ‚‚\np2 = Plots.plot3d(XÌ‚[1, :], XÌ‚[2, :], YÌ‚[2, :], lw = 3,\n    title = \"Neural Network Fit of U2(t)\", color = c1,\n    label = \"Neural Network\", xaxis = \"x\", yaxis = \"y\",\n    titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n    legend = :bottomright)\nPlots.plot!(XÌ‚[1, :], XÌ‚[2, :], YÌ„[2, :], lw = 3, label = \"True Missing Term\", color = c2)\n\np3 = Plots.scatter(solution, color = [c1 c2], label = [\"x data\" \"y data\"],\n    title = \"Extrapolated Fit From Short Training Data\",\n    titlefont = \"Helvetica\", legendfont = \"Helvetica\",\n    markersize = 5)\n\nPlots.plot!(p3, true_solution_long, color = [c1 c2], linestyle = :dot, lw = 5,\n    label = [\"True x(t)\" \"True y(t)\"])\nPlots.plot!(p3, estimate_long, color = [c3 c4], lw = 1,\n    label = [\"Estimated x(t)\" \"Estimated y(t)\"])\nPlots.plot!(p3, [2.99, 3.01], [0.0, 10.0], lw = 1, color = :black, label = nothing)\nPlots.annotate!([(1.5, 13, Plots.text(\"Training \\nData\", 10, :center, :top, :black, \"Helvetica\"))])\nl = Plots.@layout [Plots.grid(1, 2)\n                   Plots.grid(1, 1)]\nPlots.plot(p1, p2, p3, layout = l)","category":"section"},{"location":"highlevels/function_approximation/#Function-Approximation","page":"Function Approximation","title":"Function Approximation","text":"While SciML is not an ecosystem for machine learning, SciML has many libraries for doing machine learning with its equation solver libraries and machine learning libraries which are integrated into the equation solvers.","category":"section"},{"location":"highlevels/function_approximation/#Surrogates.jl:-Easy-Generation-of-Differentiable-Surrogate-Models","page":"Function Approximation","title":"Surrogates.jl: Easy Generation of Differentiable Surrogate Models","text":"Surrogates.jl is a library for generating surrogate approximations to computationally expensive simulations. It has the following high-dimensional function approximators:\n\nKriging\nKriging using Stheno\nRadial Basis\nWendland\nLinear\nSecond Order Polynomial\nSupport Vector Machines (Wait for LIBSVM resolution)\nNeural Networks\nRandom Forests\nLobachevsky splines\nInverse-distance\nPolynomial expansions\nVariable fidelity\nMixture of experts\nEarth\nGradient Enhanced Kriging","category":"section"},{"location":"highlevels/function_approximation/#ReservoirComputing.jl:-Fast-and-Flexible-Reservoir-Computing-Methods","page":"Function Approximation","title":"ReservoirComputing.jl: Fast and Flexible Reservoir Computing Methods","text":"ReservoirComputing.jl is a library for doing machine learning using reservoir computing techniques, such as with methods like Echo State Networks (ESNs). Its reservoir computing methods make it stabilized for usage with difficult equations like stiff dynamics, chaotic equations, and more.","category":"section"},{"location":"highlevels/function_approximation/#Third-Party-Libraries-to-Note","page":"Function Approximation","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/function_approximation/#Flux.jl:-the-ML-library-that-doesn't-make-you-tensor","page":"Function Approximation","title":"Flux.jl: the ML library that doesn't make you tensor","text":"Flux.jl is the most popular machine learning library in the Julia programming language. SciML's libraries are heavily tested with it and its automatic differentiation engine Zygote.jl for composability and compatibility.","category":"section"},{"location":"highlevels/function_approximation/#Lux.jl:-Explicitly-Parameterized-Neural-Networks-in-Julia","page":"Function Approximation","title":"Lux.jl: Explicitly Parameterized Neural Networks in Julia","text":"Lux.jl is a library for fully explicitly parameterized neural networks. Thus, while alternative interfaces are required to use Flux with many equation solvers (i.e. Flux.destructure), Lux.jl's explicit design marries effortlessly with the SciML equation solver libraries. For this reason, SciML's library are also heavily tested with Lux to ensure compatibility with neural network definitions from here.","category":"section"},{"location":"highlevels/function_approximation/#SimpleChains.jl:-Fast-Small-Scale-Machine-Learning","page":"Function Approximation","title":"SimpleChains.jl: Fast Small-Scale Machine Learning","text":"SimpleChains.jl is a library specialized for small-scale machine learning. It uses non-allocating mutating forms to be highly efficient for the cases where matrix multiplication kernels cannot overcome the common overheads of machine learning libraries. Thus for SciML cases with small neural networks (<100 node layers) and non-batched usage (many/most use cases), SimpleChains.jl can be the fastest choice for the neural network definitions.","category":"section"},{"location":"highlevels/function_approximation/#NNLib.jl:-Neural-Network-Primitives-with-Multiple-Backends","page":"Function Approximation","title":"NNLib.jl: Neural Network Primitives with Multiple Backends","text":"NNLib.jl is the core library which defines the handling of common functions, like conv and how they map to device accelerators such as the NVIDIA cudnn. This library can thus be used to directly grab many of the core functions used in machine learning, such as common activation functions and gather/scatter operations, without depending on the given style of any machine learning library.","category":"section"},{"location":"highlevels/function_approximation/#GeometricFlux.jl:-Geometric-Deep-Learning-and-Graph-Neural-Networks","page":"Function Approximation","title":"GeometricFlux.jl: Geometric Deep Learning and Graph Neural Networks","text":"GeometricFlux.jl is a library for graph neural networks and geometric deep learning. It is the one that is used and tested by the SciML developers for mixing with equation solver applications.","category":"section"},{"location":"highlevels/function_approximation/#AbstractGPs.jl:-Fast-and-Flexible-Gaussian-Processes","page":"Function Approximation","title":"AbstractGPs.jl: Fast and Flexible Gaussian Processes","text":"AbstractGPs.jl is the fast and flexible Gaussian Process library that is used by the SciML packages and recommended for downstream usage.","category":"section"},{"location":"highlevels/function_approximation/#MLDatasets.jl:-Common-Machine-Learning-Datasets","page":"Function Approximation","title":"MLDatasets.jl: Common Machine Learning Datasets","text":"MLDatasets.jl  is a common interface for accessing common machine learning datasets. For example, if you want to run a test on MNIST data, MLDatasets is the quickest way to obtain it.","category":"section"},{"location":"highlevels/function_approximation/#MLUtils.jl:-Utility-Functions-for-Machine-Learning-Pipelines","page":"Function Approximation","title":"MLUtils.jl: Utility Functions for Machine Learning Pipelines","text":"MLUtils.jl is a library of utility functions for making writing common machine learning pipelines easier. This includes functionality for:\n\nAn extensible dataset interface  (numobs and getobs).\nData iteration and data loaders (eachobs and DataLoader).\nLazy data views (obsview).\nResampling procedures (undersample and oversample).\nTrain/test splits (splitobs)\nData partitioning and aggregation tools (batch, unbatch, chunk, group_counts, group_indices).\nFolds for cross-validation (kfolds, leavepout).\nDatasets lazy transformations (mapobs, filterobs, groupobs, joinobs, shuffleobs).\nToy datasets for demonstration purpose.\nOther data handling utilities (flatten, normalise, unsqueeze, stack, unstack).","category":"section"},{"location":"highlevels/interfaces/#The-SciML-Interface-Libraries","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"","category":"section"},{"location":"highlevels/interfaces/#SciMLBase.jl:-The-SciML-Common-Interface","page":"The SciML Interface Libraries","title":"SciMLBase.jl: The SciML Common Interface","text":"SciMLBase.jl defines the core interfaces of the SciML libraries, such as the definitions of abstract types like SciMLProblem, along with their instantiations like ODEProblem. While SciMLBase.jl is insufficient to solve any equations, it holds all the equation definitions, and thus downstream libraries which wish to allow for using SciML solvers without depending on any solvers can directly depend on SciMLBase.jl.","category":"section"},{"location":"highlevels/interfaces/#SciMLOperators.jl:-The-AbstractSciMLOperator-Interface","page":"The SciML Interface Libraries","title":"SciMLOperators.jl: The AbstractSciMLOperator Interface","text":"SciMLOperators.jl defines the interface for how matrix-free linear and affine operators are defined and used throughout the SciML ecosystem.","category":"section"},{"location":"highlevels/interfaces/#DiffEqNoiseProcess.jl:-The-SciML-Common-Noise-Interface","page":"The SciML Interface Libraries","title":"DiffEqNoiseProcess.jl: The SciML Common Noise Interface","text":"DiffEqNoiseProcess.jl defines the common interface for stochastic noise processes used by the equation solvers of the SciML ecosystem.","category":"section"},{"location":"highlevels/interfaces/#CommonSolve.jl:-The-Common-Definition-of-Solve","page":"The SciML Interface Libraries","title":"CommonSolve.jl: The Common Definition of Solve","text":"CommonSolve.jl is the library that defines the solve, solve!, and init interfaces which are used throughout all the SciML equation solvers. It's defined as an extremely lightweight library so that other ecosystems can build on the same solve definition without clashing with SciML when both export.","category":"section"},{"location":"highlevels/interfaces/#Static.jl:-A-Shared-Interface-for-Static-Compile-Time-Computation","page":"The SciML Interface Libraries","title":"Static.jl: A Shared Interface for Static Compile-Time Computation","text":"Static.jl is a set of statically parameterized types for performing operations in a statically-defined (compiler-optimized) way with respect to values.","category":"section"},{"location":"highlevels/interfaces/#DiffEqBase.jl:-A-Library-of-Shared-Components-for-Differential-Equation-Solvers","page":"The SciML Interface Libraries","title":"DiffEqBase.jl: A Library of Shared Components for Differential Equation Solvers","text":"DiffEqBase.jl is the core shared component of the DifferentialEquations.jl ecosystem. It's not intended for non-developer users to interface directly with, instead it's used for the common functionality for uniformity of implementation between the solver libraries.","category":"section"},{"location":"highlevels/interfaces/#Third-Party-Libraries-to-Note","page":"The SciML Interface Libraries","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/interfaces/#ArrayInterface.jl:-Extensions-to-the-Julia-AbstractArray-Interface","page":"The SciML Interface Libraries","title":"ArrayInterface.jl: Extensions to the Julia AbstractArray Interface","text":"ArrayInterface.jl are traits and functions which extend the Julia Base AbstractArray interface, giving a much larger set of queries to allow for writing high-performance generic code over all array types. For example, functions include can_change_size to know if an AbstractArray type is compatible with resize!, fast_scalar_indexing to know whether direct scalar indexing A[i] is optimized, and functions like findstructralnz to get the structural non-zeros of arbitrary sparse and structured matrices.","category":"section"},{"location":"highlevels/interfaces/#Adapt.jl:-Conversion-to-Allow-Chip-Generic-Programs","page":"The SciML Interface Libraries","title":"Adapt.jl: Conversion to Allow Chip-Generic Programs","text":"Adapt.jl makes it possible to write code that is generic to the compute devices, i.e. code that works on both CPUs and GPUs. It defines the adapt function which acts like convert(T, x), but without the restriction of returning a T. This allows you to â€œconvertâ€ wrapper types, like Adjoint to be GPU compatible (for example) without throwing away the wrapper.\n\nExample usage:\n\nadapt(CuArray, ::Adjoint{Array})::Adjoint{CuArray}","category":"section"},{"location":"highlevels/interfaces/#AbstractFFTs.jl:-High-Level-Shared-Interface-for-Fast-Fourier-Transformation-Libraries","page":"The SciML Interface Libraries","title":"AbstractFFTs.jl: High Level Shared Interface for Fast Fourier Transformation Libraries","text":"AbstractFFTs.jl defines the common interface for Fast Fourier Transformations (FFTs) in Julia. Similar to SciMLBase.jl, AbstractFFTs.jl is not a solver library but instead a shared API which is extended by solver libraries such as FFTW.jl. Code written using AbstractFFTs.jl can be made compatible with FFT libraries without having an explicit dependency on a solver.","category":"section"},{"location":"highlevels/interfaces/#GPUArrays.jl:-Common-Interface-for-GPU-Based-Array-Types","page":"The SciML Interface Libraries","title":"GPUArrays.jl: Common Interface for GPU-Based Array Types","text":"GPUArrays.jl defines the shared higher-level operations for GPU-based array types, like CUDA.jl's CuArray and AMDGPU.jl's ROCmArray. Packages in SciML use the designation x isa AbstractGPUArray in order to find out if a user's operation is on the GPU and specialize computations.","category":"section"},{"location":"highlevels/interfaces/#RecipesBase.jl:-Standard-Plotting-Recipe-Interface","page":"The SciML Interface Libraries","title":"RecipesBase.jl: Standard Plotting Recipe Interface","text":"RecipesBase.jl defines the common interface for plotting recipes, composable transformations of Julia data types into simpler data types for visualization with libraries such as Plots.jl and Makie.jl. SciML libraries attempt to always include plot recipes wherever possible for ease of visualization.","category":"section"},{"location":"highlevels/interfaces/#Tables.jl:-Common-Interface-for-Tabular-Data-Types","page":"The SciML Interface Libraries","title":"Tables.jl: Common Interface for Tabular Data Types","text":"Tables.jl is a common interface for defining tabular data structures, such as DataFrames.jl. SciML's libraries extend the Tables.jl interface to allow for automated conversions into data frame libraries without explicit dependence on any singular implementation.","category":"section"},{"location":"highlevels/parameter_analysis/#Parameter-Analysis-Utilities","page":"Parameter Analysis Utilities","title":"Parameter Analysis Utilities","text":"","category":"section"},{"location":"highlevels/parameter_analysis/#GlobalSensitivity.jl:-Global-Sensitivity-Analysis","page":"Parameter Analysis Utilities","title":"GlobalSensitivity.jl: Global Sensitivity Analysis","text":"Derivatives calculate the local sensitivity of a model, i.e. the change in the simulation's outcome if one were to change the parameter with respect to some chosen part of the parameter space. But how does a simulation's output change â€œin generalâ€ with respect to a given parameter? That is what global sensitivity analysis (GSA) computes, and thus GlobalSensitivity.jl is the way to answer that question. GlobalSensitivity.jl includes a wide array of methods, including:\n\nMorris's method\nSobol's method\nRegression methods (PCC, SRC, Pearson)\neFAST\nDelta Moment-Independent method\nDerivative-based Global Sensitivity Measures (DGSM)\nEASI\nFractional Factorial method\nRandom Balance Design FAST method","category":"section"},{"location":"highlevels/parameter_analysis/#StructuralIdentifiability.jl:-Identifiability-Analysis-Made-Simple","page":"Parameter Analysis Utilities","title":"StructuralIdentifiability.jl: Identifiability Analysis Made Simple","text":"Performing parameter estimation from a data set means attempting to recover parameters like reaction rates by fitting some model to the data. But how do you know whether you have enough data to even consider getting the â€œcorrectâ€ parameters back? StructuralIdentifiability.jl allows for running a structural identifiability analysis on a given model to determine whether it's theoretically possible to recover the correct parameters. It can state whether a given type of output data can be used to globally recover the parameters (i.e. only a unique parameter set for the model produces a given output), whether the parameters are only locally identifiable (i.e. there are finitely many parameter sets which could generate the seen data), or whether it's unidentifiable (there are infinitely many parameters which generate the same output data).\n\nFor more information on what StructuralIdentifiability.jl is all about, see the SciMLCon 2022 tutorial video.","category":"section"},{"location":"highlevels/parameter_analysis/#MinimallyDisruptiveCurves.jl","page":"Parameter Analysis Utilities","title":"MinimallyDisruptiveCurves.jl","text":"MinimallyDisruptiveCurves.jl is a library for finding relationships between parameters of models, finding the curves on which the solution is constant.","category":"section"},{"location":"highlevels/parameter_analysis/#Third-Party-Libraries-to-Note","page":"Parameter Analysis Utilities","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/parameter_analysis/#SIAN.jl:-Structural-Identifiability-Analyzer","page":"Parameter Analysis Utilities","title":"SIAN.jl: Structural Identifiability Analyzer","text":"SIAN.jl is a structural identifiability analysis package which uses an entirely different algorithm from StructuralIdentifiability.jl. For information on the differences between the two approaches, see the Structural Identifiability Tools in Julia tutorial.","category":"section"},{"location":"highlevels/parameter_analysis/#DynamicalSystems.jl:-A-Suite-of-Dynamical-Systems-Analysis","page":"Parameter Analysis Utilities","title":"DynamicalSystems.jl: A Suite of Dynamical Systems Analysis","text":"DynamicalSystems.jl is an entire ecosystem of dynamical systems analysis methods, for computing measures of chaos (dimension estimation, Lyapunov coefficients), generating delay embeddings, and much more. It uses the SciML tools for its internal equation solving and thus shares much of its API, adding a layer of new tools for extended analyses.\n\nFor more information, watch the tutorial Introduction to DynamicalSystems.jl.","category":"section"},{"location":"highlevels/parameter_analysis/#BifurcationKit.jl","page":"Parameter Analysis Utilities","title":"BifurcationKit.jl","text":"BifurcationKit.jl is a tool for performing bifurcation analysis. It uses and composes with many SciML equation solvers.","category":"section"},{"location":"highlevels/parameter_analysis/#ReachabilityAnalysis.jl","page":"Parameter Analysis Utilities","title":"ReachabilityAnalysis.jl","text":"ReachabilityAnalysis.jl is a library for performing reachability analysis of dynamical systems, determining for a given uncertainty interval the full set of possible outcomes from a dynamical system.","category":"section"},{"location":"highlevels/parameter_analysis/#ControlSystems.jl","page":"Parameter Analysis Utilities","title":"ControlSystems.jl","text":"ControlSystems.jl is a library for building and analyzing control systems.","category":"section"},{"location":"showcase/optimization_under_uncertainty/#optimization_under_uncertainty","page":"Optimization Under Uncertainty","title":"Optimization Under Uncertainty","text":"This tutorial showcases how to leverage the efficient Koopman expectation method from SciMLExpectations to perform optimization under uncertainty. We demonstrate this by using a bouncing ball model with an uncertain model parameter. We also demonstrate its application to problems with probabilistic constraints, in particular a special class of constraints called chance constraints.","category":"section"},{"location":"showcase/optimization_under_uncertainty/#System-Model","page":"Optimization Under Uncertainty","title":"System Model","text":"First let's consider a 2D bouncing ball, where the states are the horizontal position x, horizontal velocity dotx, vertical position y, and vertical velocity doty. This model has two system parameters, acceleration due to gravity and coefficient of restitution (models energy loss when the ball impacts the ground). We can simulate such a system using ContinuousCallback as\n\nimport DifferentialEquations as DE\nimport Plots\nimport Statistics\n\nfunction ball!(du, u, p, t)\n    du[1] = u[2]\n    du[2] = 0.0\n    du[3] = u[4]\n    du[4] = -p[1]\nend\n\nground_condition(u, t, integrator) = u[3]\nground_affect!(integrator) = integrator.u[4] = -integrator.p[2] * integrator.u[4]\nground_cb = DE.ContinuousCallback(ground_condition, ground_affect!)\n\nu0 = [0.0, 2.0, 50.0, 0.0]\ntspan = (0.0, 50.0)\np = [9.807, 0.9]\n\nprob = DE.ODEProblem(ball!, u0, tspan, p)\nsol = DE.solve(prob, DE.Tsit5(), callback = ground_cb)\nPlots.plot(sol, vars = (1, 3), label = nothing, xlabel = \"x\", ylabel = \"y\")\n\nFor this particular problem, we wish to measure the impact distance from a point y=25 on a wall at x=25. So, we introduce an additional callback that terminates the simulation on wall impact.\n\nstop_condition(u, t, integrator) = u[1] - 25.0\nstop_cb = DE.ContinuousCallback(stop_condition, DE.terminate!)\ncbs = DE.CallbackSet(ground_cb, stop_cb)\n\ntspan = (0.0, 1500.0)\nprob = DE.ODEProblem(ball!, u0, tspan, p)\nsol = DE.solve(prob, DE.Tsit5(), callback = cbs)\nPlots.plot(sol, vars = (1, 3), label = nothing, xlabel = \"x\", ylabel = \"y\")\n\nTo help visualize this problem, we plot as follows, where the star indicates a desired impact location\n\nrectangle(xc, yc, w, h) = Plots.Shape(xc .+ [-w, w, w, -w] ./ 2.0, yc .+ [-h, -h, h, h] ./ 2.0)\n\nbegin\n    Plots.plot(sol, vars = (1, 3), label = nothing, lw = 3, c = :black)\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.ylims!(0.0, 50.0)\nend","category":"section"},{"location":"showcase/optimization_under_uncertainty/#Considering-Uncertainty","page":"Optimization Under Uncertainty","title":"Considering Uncertainty","text":"We now wish to introduce uncertainty in p[2], the coefficient of restitution. This is defined via a continuous univariate distribution from Distributions.jl. We can then run a Monte Carlo simulation of 100 trajectories via the EnsembleProblem interface.\n\nimport Distributions\n\ncor_dist = Distributions.truncated(Distributions.Normal(0.9, 0.02), 0.9 - 3 * 0.02, 1.0)\ntrajectories = 100\n\nprob_func(prob, i, repeat) = DE.remake(prob, p = [p[1], rand(cor_dist)])\nensemble_prob = DE.EnsembleProblem(prob, prob_func = prob_func)\nensemblesol = DE.solve(ensemble_prob, DE.Tsit5(), DE.EnsembleThreads(), trajectories = trajectories,\n    callback = cbs)\n\nbegin # plot\n    Plots.plot(ensemblesol, vars = (1, 3), lw = 1)\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.plot!(sol, vars = (1, 3), label = nothing, lw = 3, c = :black, ls = :dash)\n    Plots.xlims!(0.0, 27.5)\nend\n\nHere, we plot the first 350 Monte Carlo simulations along with the trajectory corresponding to the mean of the distribution (dashed line).\n\nWe now wish to compute the expected squared impact distance from the star. This is called an â€œobservationâ€ of our system or an â€œobservableâ€ of interest.\n\nWe define this observable as\n\nobs(sol, p) = abs2(sol[3, end] - 25)\n\nWith the observable defined, we can compute the expected squared miss distance from our Monte Carlo simulation results as\n\nmean_ensemble = Statistics.mean([obs(sol, p) for sol in ensemblesol])\n\nAlternatively, we can use the SciMLExpectations.Koopman() algorithm in SciMLExpectations.jl to compute this expectation much more efficiently as\n\nimport SciMLExpectations\ngd = SciMLExpectations.GenericDistribution(cor_dist)\nh(x, u, p) = u, [p[1]; x[1]]\nsm = SciMLExpectations.SystemMap(prob, DE.Tsit5(), callback = cbs)\nexprob = SciMLExpectations.ExpectationProblem(sm, obs, h, gd; nout = 1)\nsol = SciMLExpectations.solve(exprob, SciMLExpectations.Koopman(), ireltol = 1e-5)\nsol.u","category":"section"},{"location":"showcase/optimization_under_uncertainty/#Optimization-Under-Uncertainty","page":"Optimization Under Uncertainty","title":"Optimization Under Uncertainty","text":"We now wish to optimize the initial position (x_0y_0) and horizontal velocity (dotx_0) of the system to minimize the expected squared miss distance from the star, where x_0inleft-1000right, dotx_0inleft13right, and y_0inleft1050right. We will demonstrate this using a gradient-based optimization approach from NLopt.jl using ForwardDiff.jl AD through the expectation calculation.\n\nimport Optimization as OPT\nimport OptimizationNLopt\nimport OptimizationMOI\nmake_u0(Î¸) = [Î¸[1], Î¸[2], Î¸[3], 0.0]\nfunction ð”¼_loss(Î¸, pars)\n    prob = DE.ODEProblem(ball!, make_u0(Î¸), tspan, p)\n    sm = SciMLExpectations.SystemMap(prob, DE.Tsit5(), callback = cbs)\n    exprob = SciMLExpectations.ExpectationProblem(sm, obs, h, gd; nout = 1)\n    sol = SciMLExpectations.solve(exprob, SciMLExpectations.Koopman(), ireltol = 1e-5)\n    sol.u\nend\nopt_f = OPT.OptimizationFunction(ð”¼_loss, OPT.AutoForwardDiff())\nopt_ini = [-1.0, 2.0, 50.0]\nopt_lb = [-100.0, 1.0, 10.0]\nopt_ub = [0.0, 3.0, 50.0]\nopt_prob = OPT.OptimizationProblem(opt_f, opt_ini; lb = opt_lb, ub = opt_ub)\noptimizer = OptimizationMOI.MOI.OptimizerWithAttributes(OptimizationNLopt.NLopt.Optimizer,\n    \"algorithm\" => :LD_MMA)\nopt_sol = OPT.solve(opt_prob, optimizer)\nminx = opt_sol.u\n\nLet's now visualize 100 Monte Carlo simulations\n\nensembleprob = DE.EnsembleProblem(DE.remake(prob, u0 = make_u0(minx)), prob_func = prob_func)\nensemblesol = DE.solve(ensembleprob, DE.Tsit5(), DE.EnsembleThreads(), trajectories = 100,\n    callback = cbs)\n\nbegin\n    Plots.plot(ensemblesol, vars = (1, 3), lw = 1, alpha = 0.1)\n    Plots.plot!(DE.solve(DE.remake(prob, u0 = make_u0(minx)), DE.Tsit5(), callback = cbs),\n        vars = (1, 3), label = nothing, c = :black, lw = 3, ls = :dash)\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.ylims!(0.0, 50.0)\n    Plots.xlims!(minx[1], 27.5)\nend\n\nLooks pretty good! But, how long did it take? Let's benchmark.\n\n@time DE.solve(opt_prob, optimizer)\n\nNot bad for bound constrained optimization under uncertainty of a hybrid system!","category":"section"},{"location":"showcase/optimization_under_uncertainty/#Probabilistic-Constraints","page":"Optimization Under Uncertainty","title":"Probabilistic Constraints","text":"With this approach, we can also consider probabilistic constraints. Let us now consider a wall at x=20 with height 25.\n\nconstraint = [20.0, 25.0]\nbegin\n    Plots.plot(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!([constraint[1], constraint[1]], [0.0, constraint[2]], lw = 5, c = :black,\n        label = nothing)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.ylims!(0.0, 50.0)\n    Plots.xlims!(minx[1], 27.5)\nend\n\nWe now wish to minimize the same loss function as before, but introduce an inequality constraint such that the solution must have less than a 1% chance of colliding with the wall at x=20. This class of probabilistic constraints is called a chance constraint.\n\nTo do this, we first introduce a new callback and solve the system using the previous optimal solution\n\nconstraint_condition(u, t, integrator) = u[1] - constraint[1]\nfunction constraint_affect!(integrator)\n    integrator.u[3] < constraint[2] ? DE.terminate!(integrator) : nothing\nend\nconstraint_cb = DE.ContinuousCallback(constraint_condition, constraint_affect!,\n    save_positions = (true, false));\nconstraint_cbs = DE.CallbackSet(ground_cb, stop_cb, constraint_cb)\n\nensemblesol = DE.solve(ensembleprob, DE.Tsit5(), DE.EnsembleThreads(), trajectories = 500,\n    callback = constraint_cbs)\n\nbegin\n    Plots.plot(ensemblesol, vars = (1, 3), lw = 1, alpha = 0.1)\n    Plots.plot!(DE.solve(DE.remake(prob, u0 = make_u0(minx)), DE.Tsit5(), callback = constraint_cbs),\n        vars = (1, 3), label = nothing, c = :black, lw = 3, ls = :dash)\n\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.plot!([constraint[1], constraint[1]], [0.0, constraint[2]], lw = 5, c = :black)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.ylims!(0.0, 50.0)\n    Plots.xlims!(minx[1], 27.5)\nend\n\nThat doesn't look good!\n\nWe now need a second observable for the system. To compute a probability of impact, we use an indicator function for if a trajectory impacts the wall. In other words, this functions returns 1 if the trajectory hits the wall and 0 otherwise.\n\nfunction constraint_obs(sol, p)\n    sol((constraint[1] - sol[1, 1]) / sol[2, 1])[3] <= constraint[2] ? one(sol[1, end]) :\n    zero(sol[1, end])\nend\n\nUsing the previously computed optimal initial conditions, let's compute the probability of hitting this wall\n\nsm = SciMLExpectations.SystemMap(DE.remake(prob, u0 = make_u0(minx)), DE.Tsit5(), callback = cbs)\nexprob = SciMLExpectations.ExpectationProblem(sm, constraint_obs, h, gd; nout = 1)\nsol = DE.solve(exprob, SciMLExpectations.Koopman(), ireltol = 1e-5)\nsol.u\n\nWe then set up the constraint function for NLopt just as before.\n\nfunction ð”¼_constraint(res, Î¸, pars)\n    prob = DE.ODEProblem(ball!, make_u0(Î¸), tspan, p)\n    sm = SciMLExpectations.SystemMap(prob, DE.Tsit5(), callback = cbs)\n    exprob = SciMLExpectations.ExpectationProblem(sm, constraint_obs, h, gd; nout = 1)\n    sol = DE.solve(exprob, SciMLExpectations.Koopman(), ireltol = 1e-5)\n    res .= sol.u\nend\nopt_lcons = [-Inf]\nopt_ucons = [0.01]\noptimizer = OptimizationMOI.MOI.OptimizerWithAttributes(OptimizationNLopt.NLopt.Optimizer,\n    \"algorithm\" => :LD_MMA)\nopt_f = OPT.OptimizationFunction(ð”¼_loss, OPT.AutoForwardDiff(), cons = ð”¼_constraint)\nopt_prob = OPT.OptimizationProblem(opt_f, opt_ini; lb = opt_lb, ub = opt_ub, lcons = opt_lcons,\n    ucons = opt_ucons)\nopt_sol = DE.solve(opt_prob, optimizer)\nminx2 = opt_sol.u\n\nThe probability of impacting the wall is now\n\ncontainer = zeros(1)\nð”¼_constraint(container, minx2, nothing)\nÎ» = container[1]\n\nWe can check if this is within tolerance by\n\nÎ» <= 0.01 + 1e-5\n\nAgain, we plot some Monte Carlo simulations from this result as follows\n\nensembleprob = DE.EnsembleProblem(DE.remake(prob, u0 = make_u0(minx2)), prob_func = prob_func)\nensemblesol = DE.solve(ensembleprob, DE.Tsit5(), DE.EnsembleThreads(),\n    trajectories = 500, callback = constraint_cbs)\n\nbegin\n    Plots.plot(ensemblesol, vars = (1, 3), lw = 1, alpha = 0.1)\n    Plots.plot!(DE.solve(DE.remake(prob, u0 = make_u0(minx2)), DE.Tsit5(), callback = constraint_cbs),\n        vars = (1, 3), label = nothing, c = :black, lw = 3, ls = :dash)\n    Plots.plot!([constraint[1], constraint[1]], [0.0, constraint[2]], lw = 5, c = :black)\n\n    Plots.xlabel!(\"x [m]\")\n    Plots.ylabel!(\"y [m]\")\n    Plots.plot!(rectangle(27.5, 25, 5, 50), c = :red, label = nothing)\n    Plots.scatter!([25], [25], marker = :star, ms = 10, label = nothing, c = :green)\n    Plots.ylims!(0.0, 50.0)\n    Plots.xlims!(minx[1], 27.5)\nend","category":"section"},{"location":"getting_started/getting_started/#getting_started","page":"Getting Started with Julia's SciML","title":"Getting Started with Julia's SciML","text":"","category":"section"},{"location":"getting_started/getting_started/#Quickly:-What-is-Julia's-SciML-Ecosystem?","page":"Getting Started with Julia's SciML","title":"Quickly: What is Julia's SciML Ecosystem?","text":"Julia's SciML is:\n\nSciPy or MATLAB's standard library but in Julia, but\nRuns orders of magnitude faster, even outperforms C and Fortran libraries, and\nIs fully compatible with machine learning and automatic differentiation,\nAll while having an easy-to-use high level interactive development environment.\n\nInterested?","category":"section"},{"location":"getting_started/getting_started/#Introductory-Tutorials","page":"Getting Started with Julia's SciML","title":"Introductory Tutorials","text":"How do I install SciML software?\nBuild and run your first simulation\nSolve your first optimization problem\nFit a simulation to a dataset\nFind the root of an equation (i.e. solve f(x)=0)\n\nnote: Note\nEach of the SciML packages starts with its own introductory tutorial as well! Once you have started to get the hang of a few things, start checking out the introductory tutorials of the different packages. For example, the DifferentialEquations.jl getting started tutorial is a fun one!","category":"section"},{"location":"getting_started/getting_started/#Coming-from...","page":"Getting Started with Julia's SciML","title":"Coming from...","text":"Are you familiar with other scientific computing tools? Take a look at the guided introductions below.\n\nIntroduction to Julia's SciML for the Python User\nIntroduction to Julia's SciML for the MATLAB User\nIntroduction to Julia's SciML for the R User\nIntroduction to Julia's SciML for the C++/Fortran User","category":"section"},{"location":"showcase/symbolic_analysis/#symbolic_analysis","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","text":"The mixture of symbolic computing with numeric computing, which we call symbolic-numeric programming, is one of the central features of the SciML ecosystem. With core aspects like the Symbolics.jl Computer Algebra System and its integration via ModelingToolkit.jl, the SciML ecosystem gracefully mixes analytical symbolic computations with the numerical solver processes to accelerate solvers, give additional information (sparsity, identifiability), automatically fix numerical stability issues, and more.\n\nIn this showcase, we will highlight two aspects of symbolic-numeric programming.\n\nAutomated index reduction of DAEs. While arbitrary differential-algebraic equation systems can be written in DifferentialEquations.jl, not all mathematical formulations of a system are equivalent. Some are numerically difficult to solve, or even require special solvers. Some are easy. Can we recognize which formulations are hard and automatically transform them into the easy ones? Yes.\nStructural parameter identifiability. When fitting parameters to data, there's always assumptions about whether there is a unique parameter set that achieves such a data fit. But is this actually the case? The structural identifiability tooling allows one to analytically determine whether, in the limit of infinite data on a subset of observables, one could in theory uniquely identify the parameters (global identifiability), identify the parameters up to a discrete set (local identifiability), or whether there's an infinite manifold of solutions to the inverse problem (nonidentifiable).\n\nLet's dig into these two cases!","category":"section"},{"location":"showcase/symbolic_analysis/#Automated-Index-Reduction-of-DAEs","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Automated Index Reduction of DAEs","text":"In many cases one may accidentally write down a DAE that is not easily solvable by numerical methods. In this tutorial, we will walk through an example of a pendulum which accidentally generates an index-3 DAE, and show how to use the modelingtoolkitize to correct the model definition before solving.","category":"section"},{"location":"showcase/symbolic_analysis/#Copy-Pastable-Example","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Copy-Pastable Example","text":"import ModelingToolkit as MTK\nimport LinearAlgebra\nimport OrdinaryDiffEq as ODE\nimport Plots\n\nfunction pendulum!(du, u, p, t)\n    x, dx, y, dy, T = u\n    g, L = p\n    du[1] = dx\n    du[2] = T * x\n    du[3] = dy\n    du[4] = T * y - g\n    du[5] = x^2 + y^2 - L^2\n    return nothing\nend\npendulum_fun! = ODE.ODEFunction(pendulum!, mass_matrix = LinearAlgebra.Diagonal([1, 1, 1, 1, 0]))\nu0 = [1.0, 0, 0, 0, 0]\np = [9.8, 1]\ntspan = (0, 10.0)\npendulum_prob = ODE.ODEProblem(pendulum_fun!, u0, tspan, p)\ntraced_sys = MTK.modelingtoolkitize(pendulum_prob)\npendulum_sys = MTK.structural_simplify(MTK.dae_index_lowering(traced_sys))\nprob = ODE.ODEProblem(pendulum_sys, [], tspan)\nsol = ODE.solve(prob, ODE.Rodas5P(), abstol = 1e-8, reltol = 1e-8)\nPlots.plot(sol, vars = MTK.unknowns(traced_sys))","category":"section"},{"location":"showcase/symbolic_analysis/#Explanation","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Explanation","text":"","category":"section"},{"location":"showcase/symbolic_analysis/#Attempting-to-Solve-the-Equation","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Attempting to Solve the Equation","text":"In this tutorial, we will look at the pendulum system:\n\nbeginaligned\n    x^prime = v_x\n    v_x^prime = Tx\n    y^prime = v_y\n    v_y^prime = Ty - g\n    0 = x^2 + y^2 - L^2\nendaligned\n\nAs a good DifferentialEquations.jl user, one would follow the mass matrix DAE tutorial to arrive at code for simulating the model:\n\nimport OrdinaryDiffEq as ODE\nimport LinearAlgebra\nfunction pendulum!(du, u, p, t)\n    x, dx, y, dy, T = u\n    g, L = p\n    du[1] = dx\n    du[2] = T * x\n    du[3] = dy\n    du[4] = T * y - g\n    du[5] = x^2 + y^2 - L^2\nend\npendulum_fun! = ODE.ODEFunction(pendulum!, mass_matrix = LinearAlgebra.Diagonal([1, 1, 1, 1, 0]))\nu0 = [1.0, 0, 0, 0, 0];\np = [9.8, 1];\ntspan = (0, 10.0);\npendulum_prob = ODE.ODEProblem(pendulum_fun!, u0, tspan, p)\nODE.solve(pendulum_prob, ODE.Rodas5P())\n\nHowever, one will quickly be greeted with the unfortunate message:\n\nâ”Œ Warning: First function call produced NaNs. Exiting.\nâ”” @ OrdinaryDiffEq C:\\Users\\accou\\.julia\\packages\\OrdinaryDiffEq\\yCczp\\src\\initdt.jl:76\nâ”Œ Warning: Automatic dt set the starting dt as NaN, causing instability.\nâ”” @ OrdinaryDiffEq C:\\Users\\accou\\.julia\\packages\\OrdinaryDiffEq\\yCczp\\src\\solve.jl:485\nâ”Œ Warning: NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.\nâ”” @ SciMLBase C:\\Users\\accou\\.julia\\packages\\SciMLBase\\DrPil\\src\\integrator_interface.jl:325\n\nDid you implement the DAE incorrectly? No. Is the solver broken? No.","category":"section"},{"location":"showcase/symbolic_analysis/#Understanding-DAE-Index","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Understanding DAE Index","text":"It turns out that this is a property of the DAE that we are attempting to solve. This kind of DAE is known as an index-3 DAE. For a complete discussion of DAE index, see this article. Essentially, the issue here is that we have 4 differential variables (x, v_x, y, v_y) and one algebraic variable T (which we can know because there is no D(T) term in the equations). An index-1 DAE always satisfies that the Jacobian of the algebraic equations is non-singular. Here, the first 4 equations are differential equations, with the last term the algebraic relationship. However, the partial derivative of x^2 + y^2 - L^2 w.r.t. T is zero, and thus the Jacobian of the algebraic equations is the zero matrix, and thus it's singular. This is a quick way to see whether the DAE is index 1!\n\nThe problem with higher order DAEs is that the matrices used in Newton solves are singular or close to singular when applied to such problems. Because of this fact, the nonlinear solvers (or Rosenbrock methods) break down, making them difficult to solve. The classic paper DAEs are not ODEs goes into detail on this and shows that many methods are no longer convergent when index is higher than one. So, it's not necessarily the fault of the solver or the implementation: this is known.\n\nBut that's not a satisfying answer, so what do you do about it?","category":"section"},{"location":"showcase/symbolic_analysis/#Transforming-Higher-Order-DAEs-to-Index-1-DAEs","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Transforming Higher Order DAEs to Index-1 DAEs","text":"It turns out that higher order DAEs can be transformed into lower order DAEs. If you differentiate the last equation two times and perform a substitution, you can arrive at the following set of equations:\n\nbeginaligned\nx^prime = v_x \nv_x^prime = x T \ny^prime = v_y \nv_y^prime = y T - g \n0 = 2 left(v_x^2 + v_y^2 + y ( y T - g ) + T x^2 right)\nendaligned\n\nNote that this is mathematically equivalent to the equation that we had before, but the Jacobian w.r.t. T of the algebraic equation is no longer zero because of the substitution. This means that if you wrote down this version of the model, it will be index-1 and solve correctly! In fact, this is how DAE index is commonly defined: the number of differentiations it takes to transform the DAE into an ODE, where an ODE is an index-0 DAE by substituting out all of the algebraic relationships.","category":"section"},{"location":"showcase/symbolic_analysis/#Automating-the-Index-Reduction","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Automating the Index Reduction","text":"However, requiring the user to sit there and work through this process on potentially millions of equations is an unfathomable mental overhead. But, we can avoid this by using methods like the Pantelides algorithm for automatically performing this reduction to index 1. While this requires the ModelingToolkit symbolic form, we use modelingtoolkitize to transform the numerical code into symbolic code, run structural_simplify to simplify the system and lower the index, then transform back to numerical code with ODEProblem, and solve with a numerical solver. Let's try that out:\n\ntraced_sys = MTK.modelingtoolkitize(pendulum_prob)\npendulum_sys = MTK.structural_simplify(traced_sys)\nprob = ODE.ODEProblem(pendulum_sys, Pair[], tspan)\nsol = ODE.solve(prob, ODE.Rodas5P())\n\nPlots.plot(sol, vars = MTK.unknowns(traced_sys))\n\nNote that plotting using unknowns(traced_sys) is done so that any variables which are symbolically eliminated, or any variable reordering done for enhanced parallelism/performance, still show up in the resulting plot and the plot is shown in the same order as the original numerical code.\n\nNote that we can even go a bit further. If we use the ODEProblem constructor, we represent the mass matrix DAE of the index-reduced system, which can be solved via:\n\ntraced_sys = MTK.modelingtoolkitize(pendulum_prob)\npendulum_sys = MTK.structural_simplify(MTK.dae_index_lowering(traced_sys))\nprob = ODE.ODEProblem(pendulum_sys, Pair[], tspan)\nsol = ODE.solve(prob, ODE.Rodas5P(), abstol = 1e-8, reltol = 1e-8)\nPlots.plot(sol, vars = MTK.unknowns(traced_sys))\n\nAnd there you go: this has transformed the model from being too hard to solve with implicit DAE solvers, to something that is easily solved.","category":"section"},{"location":"showcase/symbolic_analysis/#Parameter-Identifiability-in-ODE-Models","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Parameter Identifiability in ODE Models","text":"Ordinary differential equations are commonly used for modeling real-world processes. The problem of parameter identifiability is one of the key design challenges for mathematical models. A parameter is said to be identifiable if one can recover its value from experimental data. Structural identifiability is a theoretical property of a model that answers this question. In this tutorial, we will show how to use StructuralIdentifiability.jl with ModelingToolkit.jl to assess identifiability of parameters in ODE models. The theory behind StructuralIdentifiability.jl is presented in paper [4].\n\nWe will start by illustrating local identifiability in which a parameter is known up to finitely many values, and then proceed to determining global identifiability, that is, which parameters can be identified uniquely.\n\nTo install StructuralIdentifiability.jl, simply run\n\nusing Pkg\nPkg.add(\"StructuralIdentifiability\")\n\nThe package has a standalone data structure for ordinary differential equations, but is also compatible with ODESystem type from ModelingToolkit.jl.","category":"section"},{"location":"showcase/symbolic_analysis/#Local-Identifiability","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Local Identifiability","text":"","category":"section"},{"location":"showcase/symbolic_analysis/#Input-System","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Input System","text":"We will consider the following model:\n\nbegincases\nfracdx_4dt = - frack_5 x_4k_6 + x_4\nfracdx_5dt = frack_5 x_4k_6 + x_4 - frack_7 x_5(k_8 + x_5 + x_6)\nfracdx_6dt = frack_7 x_5(k_8 + x_5 + x_6) - frack_9  x_6  (k_10 - x_6) k_10\nfracdx_7dt = frack_9  x_6  (k_10 - x_6) k_10\ny_1 = x_4\ny_2 = x_5endcases\n\nThis model describes the biohydrogenation[1] process[2] with unknown initial conditions.","category":"section"},{"location":"showcase/symbolic_analysis/#Using-the-ODESystem-object","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Using the ODESystem object","text":"To define the ode system in Julia, we use ModelingToolkit.jl.\n\nWe first define the parameters, variables, differential equations and the output equations.\n\nimport StructuralIdentifiability\nimport ModelingToolkit as MTK\n\n# define parameters and variables\n@variables t x4(t) x5(t) x6(t) x7(t) y1(t) y2(t)\n@parameters k5 k6 k7 k8 k9 k10\nD = Differential(t)\n\n# define equations\neqs = [\n    D(x4) ~ -k5 * x4 / (k6 + x4),\n    D(x5) ~ k5 * x4 / (k6 + x4) - k7 * x5 / (k8 + x5 + x6),\n    D(x6) ~ k7 * x5 / (k8 + x5 + x6) - k9 * x6 * (k10 - x6) / k10,\n    D(x7) ~ k9 * x6 * (k10 - x6) / k10\n]\n\n# define the output functions (quantities that can be measured)\nmeasured_quantities = [y1 ~ x4, y2 ~ x5]\n\n# define the system\nde = MTK.ODESystem(eqs, t, name = :Biohydrogenation)\n\nAfter that, we are ready to check the system for local identifiability:\n\n# query local identifiability\n# we pass the ode-system\nlocal_id_all = StructuralIdentifiability.assess_local_identifiability(de, measured_quantities = measured_quantities,\n    p = 0.99)\n# [ Info: Preproccessing `ModelingToolkit.ODESystem` object\n# 6-element Vector{Bool}:\n#  1\n#  1\n#  1\n#  1\n#  1\n#  1\n\nWe can see that all unknowns (except x_7) and all parameters are locally identifiable with probability 0.99.\n\nLet's try to check specific parameters and their combinations\n\nto_check = [k5, k7, k10 / k9, k5 + k6]\nlocal_id_some = StructuralIdentifiability.assess_local_identifiability(de, measured_quantities = measured_quantities,\n    funcs_to_check = to_check, p = 0.99)\n# 4-element Vector{Bool}:\n#  1\n#  1\n#  1\n#  1\n\nNotice that in this case, everything (except the state variable x_7) is locally identifiable, including combinations such as k_10k_9 k_5+k_6","category":"section"},{"location":"showcase/symbolic_analysis/#Global-Identifiability","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Global Identifiability","text":"In this part tutorial, let us cover an example problem of querying the ODE for globally identifiable parameters.","category":"section"},{"location":"showcase/symbolic_analysis/#Input-System-2","page":"Symbolic-Numeric Analysis of Parameter Identifiability and Model Stability","title":"Input System","text":"Let us consider the following four-dimensional model with two outputs:\n\nbegincases\nx_1(t) = -b  x_1(t) + frac1  c + x_4(t)\nx_2(t) = alpha  x_1(t) - beta  x_2(t)\nx_3(t) = gamma  x_2(t) - delta  x_3(t)\nx_4(t) = sigma  x_4(t)  frac(gamma x_2(t) - delta x_3(t)) x_3(t)\ny_1(t) = x_1(t) + x_2(t)\ny_2(t) = x_2(t)\nendcases\n\nWe will run a global identifiability check on this enzyme dynamics[3] model. We will use the default settings: the probability of correctness will be p=0.99 and we are interested in identifiability of all possible parameters.\n\nGlobal identifiability needs information about local identifiability first, but the function we chose here will take care of that extra step for us.\n\nNote: as of writing this tutorial, UTF-symbols such as Greek characters are not supported by one of the project's dependencies, see this issue.\n\nimport StructuralIdentifiability\nimport ModelingToolkit as MTK\n@parameters b c a beta g delta sigma\n@variables t x1(t) x2(t) x3(t) x4(t) y1(t) y2(t)\nD = Differential(t)\n\neqs = [\n    D(x1) ~ -b * x1 + 1 / (c + x4),\n    D(x2) ~ a * x1 - beta * x2,\n    D(x3) ~ g * x2 - delta * x3,\n    D(x4) ~ sigma * x4 * (g * x2 - delta * x3) / x3\n]\n\nmeasured_quantities = [y1 ~ x1 + x2, y2 ~ x2]\n\node = MTK.ODESystem(eqs, t, name = :GoodwinOsc)\n\n@time global_id = StructuralIdentifiability.assess_identifiability(ode, measured_quantities = measured_quantities)\n# 30.672594 seconds (100.97 M allocations: 6.219 GiB, 3.15% gc time, 0.01% compilation time)\n# Dict{Num, Symbol} with 7 entries:\n#   a     => :globally\n#   b     => :globally\n#   beta  => :globally\n#   c     => :globally\n#   sigma => :globally\n#   g     => :nonidentifiable\n#   delta => :globally\n\nWe can see that only parameters a, g are unidentifiable, and everything else can be uniquely recovered.\n\nLet us consider the same system but with two inputs, and we will find out identifiability with probability 0.9 for parameters c and b:\n\nimport StructuralIdentifiability\nimport ModelingToolkit as MTK\n@parameters b c a beta g delta sigma\n@variables t x1(t) x2(t) x3(t) x4(t) y(t) u1(t) [input = true] u2(t) [input = true]\nD = Differential(t)\n\neqs = [\n    D(x1) ~ -b * x1 + 1 / (c + x4),\n    D(x2) ~ a * x1 - beta * x2 - u1,\n    D(x3) ~ g * x2 - delta * x3 + u2,\n    D(x4) ~ sigma * x4 * (g * x2 - delta * x3) / x3\n]\nmeasured_quantities = [y1 ~ x1 + x2, y2 ~ x2]\n\n# check only 2 parameters\nto_check = [b, c]\n\node = MTK.ODESystem(eqs, t, name = :GoodwinOsc)\n\nglobal_id = StructuralIdentifiability.assess_identifiability(ode, measured_quantities = measured_quantities,\n    funcs_to_check = to_check, p = 0.9)\n# Dict{Num, Symbol} with 2 entries:\n#   b => :globally\n#   c => :globally\n\nBoth parameters b, c are globally identifiable with probability 0.9 in this case.\n\n[1]: R. Munoz-Tamayo, L. Puillet, J.B. Daniel, D. Sauvant, O. Martin, M. Taghipoor, P. Blavy Review: To be or not to be an identifiable model. Is this a relevant question in animal science modelling?, Animal, Vol 12 (4), 701-712, 2018. The model is the ODE system (3) in Supplementary Material 2, initial conditions are assumed to be unknown.\n\n[2]: Moate P.J., Boston R.C., Jenkins T.C. and Lean I.J., Kinetics of Ruminal Lipolysis of Triacylglycerol and Biohydrogenationof Long-Chain Fatty Acids: New Insights from Old Data, Journal of Dairy Science 91, 731â€“742, 2008\n\n[3]: Goodwin, B.C. Oscillatory behavior in enzymatic control processes, Advances in Enzyme Regulation, Vol 3 (C), 425-437, 1965\n\n[4]: Dong, R., Goodbrake, C., Harrington, H. A., & Pogudin, G. Computing input-output projections of dynamical models with applications to structural identifiability. arXiv preprint arXiv:2111.00991.","category":"section"},{"location":"highlevels/symbolic_learning/#Symbolic-Learning-and-Artificial-Intelligence","page":"Symbolic Learning and Artificial Intelligence","title":"Symbolic Learning and Artificial Intelligence","text":"Symbolic learning, the classical artificial intelligence, is a set of methods for learning symbolic equations from data and numerical functions. SciML offers an array of symbolic learning utilities which connect with the other machine learning and equation solver functionalities to make it easy to embed prior knowledge and discover missing physics. For more information, see Universal Differential Equations for Scientific Machine Learning.","category":"section"},{"location":"highlevels/symbolic_learning/#DataDrivenDiffEq.jl:-Data-Driven-Modeling-and-Automated-Discovery-of-Dynamical-Systems","page":"Symbolic Learning and Artificial Intelligence","title":"DataDrivenDiffEq.jl: Data-Driven Modeling and Automated Discovery of Dynamical Systems","text":"DataDrivenDiffEq.jl is a general interface for data-driven modeling, containing a large array of techniques such as:\n\nKoopman operator methods (Dynamic-Mode Decomposition (DMD) and variations)\nSparse Identification of Dynamical Systems (SINDy and variations like iSINDy)\nSparse regression methods (STSLQ, SR3, etc.)\nPDEFind\nWrappers for SymbolicRegression.jl\nAI Feynman\nOccamNet","category":"section"},{"location":"highlevels/symbolic_learning/#SymbolicNumericIntegration.jl:-Symbolic-Integration-via-Numerical-Methods","page":"Symbolic Learning and Artificial Intelligence","title":"SymbolicNumericIntegration.jl: Symbolic Integration via Numerical Methods","text":"SymbolicNumericIntegration.jl is a package computing the solution to symbolic integration problem using numerical methods (numerical integration mixed with sparse regression).","category":"section"},{"location":"highlevels/symbolic_learning/#Third-Party-Libraries-to-Note","page":"Symbolic Learning and Artificial Intelligence","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/symbolic_learning/#SymbolicRegression.jl","page":"Symbolic Learning and Artificial Intelligence","title":"SymbolicRegression.jl","text":"SymbolicRegression.jl is a symbolic regression library which uses genetic algorithms with parallelization to achieve fast and robust symbolic learning.","category":"section"},{"location":"highlevels/array_libraries/#Modeling-Array-Libraries","page":"Modeling Array Libraries","title":"Modeling Array Libraries","text":"","category":"section"},{"location":"highlevels/array_libraries/#RecursiveArrayTools.jl:-Arrays-of-Arrays-and-Even-Deeper","page":"Modeling Array Libraries","title":"RecursiveArrayTools.jl: Arrays of Arrays and Even Deeper","text":"Sometimes, when one is creating a model, basic array types are not enough for expressing a complex concept. RecursiveArrayTools.jl gives many types, such as VectorOfArray and ArrayPartition, which allow for easily building nested array models in a way that conforms to the standard AbstractArray interface. While standard Vector{Array{Float64,N}} types may not be compatible with many equation solver libraries, these wrapped forms like VectorOfArray{Vector{Array{Float64,N}}} are, making it easy to use these more exotic array constructions.\n\nNote that SciML's interfaces use RecursiveArrayTools.jl extensively, for example, with the timeseries solution types being AbstractVectorOfArray.","category":"section"},{"location":"highlevels/array_libraries/#LabelledArrays.jl:-Named-Variables-in-Arrays-without-Overhead","page":"Modeling Array Libraries","title":"LabelledArrays.jl: Named Variables in Arrays without Overhead","text":"Sometimes, you want to use a full domain-specific language like ModelingToolkit. Other times, you wish arrays just had a slightly nicer syntax. Don't you wish you could write the Lorenz equations like:\n\nfunction lorenz_f(du, u, p, t)\n    du.x = p.Ïƒ * (u.y - u.x)\n    du.y = u.x * (p.Ï - u.z) - u.y\n    du.z = u.x * u.y - p.Î² * u.z\nend\n\nwithout losing any efficiency? LabelledArrays.jl provides the array types to do just that. All the . accesses are resolved at compile-time, so it's a zero-overhead interface.\n\nnote: Note\nWe recommend using ComponentArrays.jl for any instance where nested accesses are required, or where the . accesses need to be views to subsets of the array.","category":"section"},{"location":"highlevels/array_libraries/#MultiScaleArrays.jl:-Multiscale-Modeling-to-Compose-with-Equation-Solvers","page":"Modeling Array Libraries","title":"MultiScaleArrays.jl: Multiscale Modeling to Compose with Equation Solvers","text":"(Image: )\n\nHow do you encode such real-world structures in a manner that is compatible with the SciML equation solver libraries? MultiScaleArrays.jl is an answer. MultiScaleArrays.jl gives a highly flexible interface for defining multi-level types, which generates a corresponding interface as an AbstractArray. MultiScaleArrays.jl's flexibility includes the ease of resizing, allowing for models where the number of equations grows and shrinks as agents (cells) in the model divide and die.\n\nnote: Note\nWe recommend using ComponentArrays.jl instead in any instance where the resizing functionality is not used.","category":"section"},{"location":"highlevels/array_libraries/#Third-Party-Libraries-to-Note","page":"Modeling Array Libraries","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/array_libraries/#ComponentArrays.jl:-Arrays-with-Arbitrarily-Nested-Named-Components","page":"Modeling Array Libraries","title":"ComponentArrays.jl: Arrays with Arbitrarily Nested Named Components","text":"What if you had a set of arrays of arrays with names, but you wanted to represent them on a single contiguous vector so that linear algebra was as fast as possible, while retaining . named accesses with zero-overhead? This is what ComponentArrays.jl provides, and as such it is one of the top recommendations of AbstractArray types to be used. Multi-level definitions such as x = ComponentArray(a=5, b=[(a=20., b=0), (a=33., b=0), (a=44., b=3)], c=c) are common-place, and allow for accessing via x.b.a etc. without any performance loss. ComponentArrays are fully compatible with the SciML equation solvers. They thus can be used as initial conditions. Here's a demonstration of the Lorenz equation using ComponentArrays with Parameters.jl's @unpack:\n\nimport ComponentArrays\nimport DifferentialEquations\nimport Parameters: @unpack\n\ntspan = (0.0, 20.0)\n\n## Lorenz system\nfunction lorenz!(D, u, p, t; f = 0.0)\n    @unpack Ïƒ, Ï, Î² = p\n    @unpack x, y, z = u\n\n    D.x = Ïƒ * (y - x)\n    D.y = x * (Ï - z) - y - f\n    D.z = x * y - Î² * z\n    return nothing\nend\n\nlorenz_p = (Ïƒ = 10.0, Ï = 28.0, Î² = 8 / 3)\nlorenz_ic = ComponentArrays.ComponentArray(x = 0.0, y = 0.0, z = 0.0)\nlorenz_prob = DifferentialEquations.ODEProblem(lorenz!, lorenz_ic, tspan, lorenz_p)\n\nIs that beautiful? Yes, it is.","category":"section"},{"location":"highlevels/array_libraries/#StaticArrays.jl:-Statically-Defined-Arrays","page":"Modeling Array Libraries","title":"StaticArrays.jl: Statically-Defined Arrays","text":"StaticArrays.jl is a library for statically-defined arrays. Because these arrays have type-level information for size, they recompile the solvers for every new size. They can be dramatically faster for small sizes (up to approximately size 10), but for larger equations they increase compile time with little to no benefit.","category":"section"},{"location":"highlevels/array_libraries/#CUDA.jl:-NVIDIA-CUDA-Based-GPU-Array-Computations","page":"Modeling Array Libraries","title":"CUDA.jl: NVIDIA CUDA-Based GPU Array Computations","text":"CUDA.jl is the library for defining arrays which live on NVIDIA GPUs (CuArray). SciML's libraries will respect the GPU-ness of the inputs, i.e., if the input arrays live on the GPU then the operations will all take place on the GPU or else the libraries will error if it's unable to do so. Thus, using CUDA.jl's CuArray is how one GPU-accelerates any computation with the SciML organization's libraries. Simply use a CuArray as the initial condition to an ODE solve or as the initial guess for a nonlinear solve, and the whole solve will recompile to take place on the GPU.","category":"section"},{"location":"highlevels/array_libraries/#AMDGPU.jl:-AMD-Based-GPU-Array-Computations","page":"Modeling Array Libraries","title":"AMDGPU.jl: AMD-Based GPU Array Computations","text":"AMDGPU.jl is the library for defining arrays which live on AMD GPUs (ROCArray). SciML's libraries will respect the GPU-ness of the inputs, i.e., if the input arrays live on the GPU then the operations will all take place on the GPU or else the libraries will error if it's unable to do so. Thus using AMDGPU.jl's ROCArray is how one GPU-accelerates any computation with the SciML organization's libraries. Simply use a ROCArray as the initial condition to an ODE solve or as the initial guess for a nonlinear solve, and the whole solve will recompile to take place on the GPU.","category":"section"},{"location":"highlevels/array_libraries/#FillArrays.jl:-Lazy-Arrays","page":"Modeling Array Libraries","title":"FillArrays.jl: Lazy Arrays","text":"FillArrays.jl is a library for defining arrays with lazy values. For example, an O(1) representation of the identity matrix is given by Eye{Int}(5). FillArrays.jl is used extensively throughout the ecosystem to improve runtime and memory performance.","category":"section"},{"location":"highlevels/array_libraries/#BandedMatrices.jl:-Fast-Banded-Matrices","page":"Modeling Array Libraries","title":"BandedMatrices.jl: Fast Banded Matrices","text":"Banded matrices show up in many equation solver contexts, such as the Jacobians of many partial differential equations. While the base SparseMatrixCSC sparse matrix type can represent such matrices, BandedMatrices.jl is a specialized format specifically for BandedMatrices which can be used to greatly improve performance of operations on a banded matrix.","category":"section"},{"location":"highlevels/array_libraries/#BlockBandedMatrices.jl:-Fast-Block-Banded-Matrices","page":"Modeling Array Libraries","title":"BlockBandedMatrices.jl: Fast Block-Banded Matrices","text":"Block banded matrices show up in many equation solver contexts, such as the Jacobians of many systems of partial differential equations. While the base SparseMatrixCSC sparse matrix type can represent such matrices, BlockBandedMatrices.jl is a specialized format specifically for BlockBandedMatrices which can be used to greatly improve performance of operations on a block-banded matrix.","category":"section"},{"location":"comparisons/r/#r","page":"Getting Started with Julia's SciML for the R User","title":"Getting Started with Julia's SciML for the R User","text":"If you're an R user who has looked into Julia, you're probably wondering where all of the scientific computing packages are. How do I solve ODEs? Solve f(x)=0 for x? Etc. SciML is the ecosystem for doing this with Julia.","category":"section"},{"location":"comparisons/r/#Why-SciML?-High-Level-Workflow-Reasons","page":"Getting Started with Julia's SciML for the R User","title":"Why SciML? High-Level Workflow Reasons","text":"Performance - The key reason people are moving from R to Julia's SciML in droves is performance. Even simple ODE solvers are much faster!, demonstrating orders of magnitude performance improvements for differential equations, nonlinear solving, optimization, and more. And the performance advantages continue to grow as more complex algorithms are required.\nComposable Library Components - In R environments, every package feels like a silo. Functions made for one file exchange library cannot easily compose with another. SciML's generic coding with JIT compilation these connections create new optimized code on the fly and allow for a more expansive feature set than can ever be documented. Take new high-precision number types from a package and stick them into a nonlinear solver. Take a package for Intel GPU arrays and stick it into the differential equation solver to use specialized hardware acceleration.\nA Global Harmonious Documentation for Scientific Computing - R's documentation for scientific computing is scattered in a bunch of individual packages where the developers do not talk to each other! This not only leads to documentation differences, but also â€œstyleâ€ differences: one package uses tol while the other uses atol. With Julia's SciML, the whole ecosystem is considered together, and inconsistencies are handled at the global level. The goal is to be working in one environment with one language.\nEasier High-Performance and Parallel Computing - With Julia's ecosystem, CUDA will automatically install of the required binaries and cu(A)*cu(B) is then all that's required to GPU-accelerate large-scale linear algebra. MPI is easy to install and use. Distributed computing through password-less SSH. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to automate the discovery of missing physical laws using neural networks embedded in differentiable simulations? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\n\nIn this plot, deSolve in blue represents R's most commonly used solver:\n\n(Image: )","category":"section"},{"location":"comparisons/r/#Need-Help-Translating-from-R-to-Julia?","page":"Getting Started with Julia's SciML for the R User","title":"Need Help Translating from R to Julia?","text":"The following resources can be particularly helpful when adopting Julia for SciML for the first time:\n\nThe Julia Manual's Noteworthy Differences from R page\nTutorials on Data Wrangling and Plotting in Julia (Sections 4 and 5) written for folks with a background in R.\nDouble-check your results with deSolveDiffEq.jl (automatically converts and runs ODE definitions with R's deSolve solvers)\nUse RCall.jl to more incrementally move code to Julia.\nComparisons between R and Julia from the DataFrames package. And an accessible starting point for Julia's DataFrames.","category":"section"},{"location":"comparisons/r/#R-to-Julia-SciML-Functionality-Translations","page":"Getting Started with Julia's SciML for the R User","title":"R to Julia SciML Functionality Translations","text":"The following chart will help you get quickly acquainted with Julia's SciML Tools:\n\nR Function/Package SciML-Supported Julia packages\ndata.frame DataFrames\nplot Plots, Makie\nggplot2 AlgebraOfGraphics\ndeSolve DifferentialEquations\nStan Turing","category":"section"},{"location":"comparisons/r/#Want-to-See-the-Power-of-Julia?","page":"Getting Started with Julia's SciML for the R User","title":"Want to See the Power of Julia?","text":"Check out this R-Bloggers blog post on diffeqr, a package which uses ModelingToolkit to translate R code to Julia, and achieves 350x acceleration over R's popular deSolve ODE solver package. But when the solve is done purely in Julia, it achieves 2777x acceleration over deSolve!","category":"section"},{"location":"showcase/showcase/#showcase","page":"The SciML Showcase","title":"The SciML Showcase","text":"The SciML Showcase is a display of some cool things that can be done by connecting SciML software.\n\nnote: Note\nThe SciML Showcase is not meant to be training/tutorials, but inspirational demonstrations! If you're looking for simple examples to get started with, check out the getting started section.\n\nWant to see some cool things that you can do with SciML? Check out the following:\n\nScientific machine learning: incorporating prior physics into automated model discovery\nAuto-complete mechanistic models by embedding machine learning into differential equations\nBayesian automated model discovery with quantified uncertainties and probability estimates\nEfficiently gathering data to fill in missing physics\nDiscovering the Relativistic Corrections to Binary Black Hole Dynamics\nSolving big difficult equations with parallelism, speed, and accuracy\nAutomated Efficient Solution of Nonlinear Partial Differential Equations\nGPU-Accelerated Physics-Informed Neural Network PDE Solvers\nMassively Data-Parallel ODE Solving on GPUs\nGPU-Accelerated Stochastic Partial Differential Equations\nUseful cool wonky things that are hard to find anywhere else\nAutomatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System\nSymbolic-Numeric Analysis of Parameter Identifiability and Model Stability\nOptimization Under Uncertainty","category":"section"},{"location":"highlevels/equation_solvers/#Equation-Solvers","page":"Equation Solvers","title":"Equation Solvers","text":"The SciML Equation Solvers cover a large set of SciMLProblems with SciMLAlgorithms that are efficient, numerically stable, and flexible. These methods tie into libraries like SciMLSensitivity.jl to be fully differentiable and compatible with machine learning pipelines, and are designed for integration with applications like parameter estimation, global sensitivity analysis, and more.","category":"section"},{"location":"highlevels/equation_solvers/#LinearSolve.jl:-Unified-Interface-for-Linear-Solvers","page":"Equation Solvers","title":"LinearSolve.jl: Unified Interface for Linear Solvers","text":"LinearSolve.jl is the canonical library for solving LinearProblems. It includes:\n\nFast pure Julia LU factorizations which outperform standard BLAS\nKLU for faster sparse LU factorization on unstructured matrices\nUMFPACK for faster sparse LU factorization on matrices with some repeated structure\nMKLPardiso wrappers for handling many sparse matrices faster than SuiteSparse (KLU, UMFPACK) methods\nGPU-offloading for large dense matrices\nWrappers to all of the Krylov implementations (Krylov.jl, IterativeSolvers.jl, KrylovKit.jl) for easy testing of all of them. LinearSolve.jl handles the API differences, especially with the preconditioner definitions\nA polyalgorithm that smartly chooses between these methods\nA caching interface which automates caching of symbolic factorizations and numerical factorizations as optimally as possible\nCompatible with arbitrary AbstractArray and Number types, such as GPU-based arrays, uncertainty quantification number types, and more.","category":"section"},{"location":"highlevels/equation_solvers/#NonlinearSolve.jl:-Unified-Interface-for-Nonlinear-Solvers","page":"Equation Solvers","title":"NonlinearSolve.jl: Unified Interface for Nonlinear Solvers","text":"NonlinearSolve.jl is the canonical library for solving NonlinearProblems. It includes:\n\nFast non-allocating implementations on static arrays of common methods (Newton-Rhapson)\nBracketing methods (Bisection, Falsi) for methods with known upper and lower bounds (IntervalNonlinearProblem)\nWrappers to common other solvers (NLsolve.jl, MINPACK, KINSOL from Sundials) for trust region methods, line search-based approaches, etc.\nBuilt over the LinearSolve.jl API for maximum flexibility and performance in the solving approach\nCompatible with arbitrary AbstractArray and Number types, such as GPU-based arrays, uncertainty quantification number types, and more.","category":"section"},{"location":"highlevels/equation_solvers/#DifferentialEquations.jl:-Unified-Interface-for-Differential-Equation-Solvers","page":"Equation Solvers","title":"DifferentialEquations.jl: Unified Interface for Differential Equation Solvers","text":"DifferentialEquations.jl is the canonical library for solving DEProblems. This includes:\n\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (DEProblems with callbacks and JumpProblem)\n\nThe well-optimized DifferentialEquations solvers benchmark as some of the fastest implementations of classic algorithms. It also includes algorithms from recent research which routinely outperform the â€œstandardâ€ C/Fortran methods, and algorithms optimized for high-precision and HPC applications. Simultaneously, it wraps the classic C/Fortran methods, making it easy to switch over to them whenever necessary. Solving differential equations with different methods from different languages and packages can be done by changing one line of code, allowing for easy benchmarking to ensure you are using the fastest method possible.\n\nDifferentialEquations.jl integrates with the Julia package sphere. Examples are:\n\nGPU acceleration through CUDAnative.jl and CuArrays.jl\nAutomated sparsity detection with Symbolics.jl\nAutomatic Jacobian coloring with SparseDiffTools.jl, allowing for fast solutions to problems with sparse or structured (Tridiagonal, Banded, BlockBanded, etc.) Jacobians\nAllowing the specification of linear solvers for maximal efficiency\nProgress meter integration with the Juno IDE for estimated time to solution\nAutomatic plotting of time series and phase plots\nBuilt-in interpolations\nWraps for common C/Fortran methods, like Sundials and Hairer's radau\nArbitrary precision with BigFloats and Arbfloats\nArbitrary array types, allowing the definition of differential equations on matrices and distributed arrays\nUnit-checked arithmetic with Unitful","category":"section"},{"location":"highlevels/equation_solvers/#Optimization.jl:-Unified-Interface-for-Optimization","page":"Equation Solvers","title":"Optimization.jl: Unified Interface for Optimization","text":"Optimization.jl is the canonical library for solving OptimizationProblems. It includes wrappers of most of the Julia nonlinear optimization ecosystem, allowing one syntax to use all packages in a uniform manner. This covers:\n\nOptimizationBBO for BlackBoxOptim.jl\nOptimizationEvolutionary for Evolutionary.jl (see also this documentation)\nOptimizationGCMAES for GCMAES.jl\nOptimizationMOI for MathOptInterface.jl (usage of algorithm via MathOptInterface API; see also the API documentation)\nOptimizationMetaheuristics for Metaheuristics.jl (see also this documentation)\nOptimizationMultistartOptimization for MultistartOptimization.jl (see also this documentation)\nOptimizationNLopt for NLopt.jl (usage via the NLopt API; see also the available algorithms)\nOptimizationNOMAD for NOMAD.jl (see also this documentation)\nOptimizationNonconvex for Nonconvex.jl (see also this documentation)\nOptimizationQuadDIRECT for QuadDIRECT.jl\nOptimizationSpeedMapping for SpeedMapping.jl (see also this documentation)","category":"section"},{"location":"highlevels/equation_solvers/#Integrals.jl:-Unified-Interface-for-Numerical-Integration","page":"Equation Solvers","title":"Integrals.jl: Unified Interface for Numerical Integration","text":"Integrals.jl is the canonical library for solving IntegralsProblems. It includes wrappers of most of the Julia quadrature ecosystem, allowing one syntax to use all packages in a uniform manner. This covers:\n\nGauss-Kronrod quadrature\nCubature methods (both h and p cubature)\nAdaptive Monte Carlo methods","category":"section"},{"location":"highlevels/equation_solvers/#JumpProcesses.jl:-Stochastic-Simulation-Algorithms-for-Jump-Processes,-Jump-ODEs,-and-Jump-Diffusions","page":"Equation Solvers","title":"JumpProcesses.jl: Stochastic Simulation Algorithms for Jump Processes, Jump-ODEs, and Jump-Diffusions","text":"JumpProcesses.jl is the library for Poisson jump processes, also known as chemical master equations or Gillespie simulations, for simulating chemical reaction networks and other applications. It allows for solving with many methods, including:\n\nDirect: the Gillespie Direct method SSA.\nRDirect: A variant of Gillespie's Direct method that uses rejection to sample the next reaction.\nDirectCR: The Composition-Rejection Direct method of Slepoy et al. For large networks and linear chain-type networks, it will often give better performance than Direct. (Requires dependency graph, see below.)\nDirectFW: the Gillespie Direct method SSA with FunctionWrappers. This aggregator uses a different internal storage format for collections of ConstantRateJumps.\nFRM: the Gillespie first reaction method SSA. Direct should generally offer better performance and be preferred to FRM.\nFRMFW: the Gillespie first reaction method SSA with FunctionWrappers.\nNRM: The Gibson-Bruck Next Reaction Method. For some reaction network structures, this may offer better performance than Direct (for example, large, linear chains of reactions). (Requires dependency graph, see below.)\nRSSA: The Rejection SSA (RSSA) method of Thanh et al. With RSSACR, for very large reaction networks, it often offers the best performance of all methods. (Requires dependency graph, see below.)\nRSSACR: The Rejection SSA (RSSA) with Composition-Rejection method of Thanh et al. With RSSA, for very large reaction networks, it often offers the best performance of all methods. (Requires dependency graph, see below.)\nSortingDirect: The Sorting Direct Method of McCollum et al. It will usually offer performance as good as Direct, and for some systems can offer substantially better performance. (Requires dependency graph, see below.)\n\nThe design of JumpProcesses.jl composes with DifferentialEquations.jl, allowing for discrete stochastic chemical reactions to be easily mixed with differential equation models, allowing for simulation of hybrid systems, jump diffusions, and differential equations driven by Levy processes.\n\nIn addition, JumpProcesses's interfaces allow for solving with regular jump methods, such as adaptive Tau-Leaping.","category":"section"},{"location":"highlevels/equation_solvers/#Third-Party-Libraries-to-Note","page":"Equation Solvers","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/equation_solvers/#JuMP.jl:-Julia-for-Mathematical-Programming","page":"Equation Solvers","title":"JuMP.jl: Julia for Mathematical Programming","text":"While Optimization.jl is the preferred library for nonlinear optimization, for all other forms of optimization Julia for Mathematical Programming (JuMP) is the star. JuMP is the leading choice in Julia for doing:\n\nLinear Programming\nQuadratic Programming\nConvex Programming\nConic Programming\nSemidefinite Programming\nMixed-Complementarity Programming\nInteger Programming\nMixed Integer (nonlinear/linear) Programming\n(Mixed Integer) Second Order Conic Programming\n\nJuMP can also be used for some nonlinear programming, though the Optimization.jl bindings to the JuMP solvers (via MathOptInterface.jl) is generally preferred.","category":"section"},{"location":"highlevels/equation_solvers/#FractionalDiffEq.jl:-Fractional-Differential-Equation-Solvers","page":"Equation Solvers","title":"FractionalDiffEq.jl: Fractional Differential Equation Solvers","text":"FractionalDiffEq.jl is a set of high-performance solvers for fractional differential equations.","category":"section"},{"location":"highlevels/equation_solvers/#ManifoldDiffEq.jl:-Solvers-for-Differential-Equations-on-Manifolds","page":"Equation Solvers","title":"ManifoldDiffEq.jl: Solvers for Differential Equations on Manifolds","text":"ManifoldDiffEq.jl is a set of high-performance solvers for differential equations on manifolds using methods such as Lie Group actions and frozen coefficients (Crouch-Grossman methods). These solvers can in many cases out-perform the OrdinaryDiffEq.jl nonautonomous operator ODE solvers by using methods specialized on manifold definitions of ManifoldsBase.","category":"section"},{"location":"highlevels/equation_solvers/#Manopt.jl:-Optimization-on-Manifolds","page":"Equation Solvers","title":"Manopt.jl: Optimization on Manifolds","text":"ManOpt.jl allows for easy and efficient solving of nonlinear optimization problems on manifolds.","category":"section"},{"location":"comparisons/matlab/#matlab","page":"Getting Started with Julia's SciML for the MATLAB User","title":"Getting Started with Julia's SciML for the MATLAB User","text":"If you're a MATLAB user who has looked into Julia for some performance improvements, you may have noticed that the standard library does not have all of the â€œbatteriesâ€ included with a base MATLAB installation. Where's the ODE solver? Where's fmincon and fsolve? Those scientific computing functionalities are the pieces provided by the Julia SciML ecosystem!","category":"section"},{"location":"comparisons/matlab/#Why-SciML?-High-Level-Workflow-Reasons","page":"Getting Started with Julia's SciML for the MATLAB User","title":"Why SciML? High-Level Workflow Reasons","text":"Performance - The key reason people are moving from MATLAB to Julia's SciML in droves is performance. Even simple ODE solvers are much faster!, demonstrating orders of magnitude performance improvements for differential equations, nonlinear solving, optimization, and more. And the performance advantages continue to grow as more complex algorithms are required.\nJulia is quick to learn from MATLAB - Most ODE codes can be translated in a few minutes. If you need help, check out the QuantEcon MATLAB-Python-Julia Cheat Sheet.\nPackage Management and Versioning - Julia's package manager takes care of dependency management, testing, and continuous delivery in order to make the installation and maintenance process smoother. For package users, this means it's easier to get packages with complex functionality in your hands.\nFree and Open Source - If you want to know how things are being computed, just look at our GitHub organization. Lots of individuals use Julia's SciML to research how the algorithms actually work because of how accessible and tweakable the ecosystem is!\nComposable Library Components - In MATLAB environments, every package feels like a silo. Functions made for one file exchange library cannot easily compose with another. SciML's generic coding with JIT compilation these connections create new optimized code on the fly and allow for a more expansive feature set than can ever be documented. Take new high-precision number types from a package and stick them into a nonlinear solver. Take a package for Intel GPU arrays and stick it into the differential equation solver to use specialized hardware acceleration.\nEasier High-Performance and Parallel Computing - With Julia's ecosystem, CUDA will automatically install of the required binaries and cu(A)*cu(B) is then all that's required to GPU-accelerate large-scale linear algebra. MPI is easy to install and use. Distributed computing through password-less SSH. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to automate the discovery of missing physical laws using neural networks embedded in differentiable simulations? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\n\nIn this plot, MATLAB in orange represents MATLAB's most commonly used solvers:\n\n(Image: )","category":"section"},{"location":"comparisons/matlab/#Need-a-case-study?","page":"Getting Started with Julia's SciML for the MATLAB User","title":"Need a case study?","text":"Check out this talk from NASA Scientists getting a 15,000x acceleration by switching from Simulink to Julia's ModelingToolkit!","category":"section"},{"location":"comparisons/matlab/#Need-Help-Translating-from-MATLAB-to-Julia?","page":"Getting Started with Julia's SciML for the MATLAB User","title":"Need Help Translating from MATLAB to Julia?","text":"The following resources can be particularly helpful when adopting Julia for SciML for the first time:\n\nQuantEcon MATLAB-Python-Julia Cheat Sheet\nThe Julia Manual's Noteworthy Differences from MATLAB page\nDouble-check your results with MATLABDiffEq.jl (automatically converts and runs ODE definitions with MATLAB's solvers)\nUse MATLAB.jl to more incrementally move code to Julia.","category":"section"},{"location":"comparisons/matlab/#MATLAB-to-Julia-SciML-Functionality-Translations","page":"Getting Started with Julia's SciML for the MATLAB User","title":"MATLAB to Julia SciML Functionality Translations","text":"The following chart will help you get quickly acquainted with Julia's SciML Tools:\n\nMATLAB Function SciML-Supported Julia packages\nplot Plots, Makie\nsparse SparseArrays\ninterp1 DataInterpolations\n\\, gmres, cg LinearSolve\nfsolve NonlinearSolve\nquad Integrals\nfmincon Optimization\nodeXX DifferentialEquations\node45 Tsit5\node113 VCABM\node23s Rosenbrock23\node15s QNDF or FBDF\node15i IDA\nbvp4c and bvp5c DifferentialEquations\nSimulink, Simscape ModelingToolkit\nfft FFTW\nchebfun ApproxFun","category":"section"},{"location":"highlevels/inverse_problems/#parameter_estimation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","text":"Parameter estimation for models and equations, also known as dynamic data analysis, solving the inverse problem, or Bayesian posterior estimation (when done probabilistically), is provided by the SciML tools for the equations in its set. In this introduction, we briefly present the relevant packages that facilitate parameter estimation, namely:\n\nSciMLSensitivity.jl\nDiffEqFlux.jl\nTuring.jl\nDataDrivenDiffEq.jl\nDiffEqParamEstim.jl\nDiffEqBayes.jl\n\nWe also provide information regarding the respective strengths of these packages so that you can easily decide which one suits your needs best.","category":"section"},{"location":"highlevels/inverse_problems/#SciMLSensitivity.jl:-Local-Sensitivity-Analysis-and-Automatic-Differentiation-Support-for-Solvers","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"SciMLSensitivity.jl: Local Sensitivity Analysis and Automatic Differentiation Support for Solvers","text":"SciMLSensitivity.jl is the system for local sensitivity, which all other inverse problem methods rely on. This package defines the interactions between the equation solvers and automatic differentiation, defining fast overloads for forward and adjoint (reverse) sensitivity analysis for fast gradient and Jacobian calculations with respect to model inputs. Its documentation covers how to use direct differentiation of equation solvers in conjunction with tools like Optimization.jl to perform model calibration of ODEs against data, PDE-constrained optimization, nonlinear optimal controls analysis, and much more. As a lower level tool, this library is very versatile, feature-rich, and high-performance, giving all the tools required but not directly providing a higher level interface.\n\nnote: Note\nSensitivity analysis is kept in a separate library from the solvers (SciMLSensitivity.jl), in order to not require all equation solvers to have a dependency on all automatic differentiation libraries. If automatic differentiation is applied to a solver library without importing SciMLSensitivity.jl, an error is thrown letting the user know to import SciMLSensitivity.jl for the functionality to exist.","category":"section"},{"location":"highlevels/inverse_problems/#DataDrivenDiffEq.jl:-Data-Driven-Modeling-and-Equation-Discovery","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"DataDrivenDiffEq.jl: Data-Driven Modeling and Equation Discovery","text":"The distinguishing feature of this package is that its ultimate goal is to identify the differential equation model that generated the input data. Depending on the user's needs, the package can provide structural identification of a given differential equation (output in a symbolic form) or structural estimation (output as a function for prediction purposes).","category":"section"},{"location":"highlevels/inverse_problems/#DiffEqParamEstim.jl:-Simplified-Parameter-Estimation-Interface","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"DiffEqParamEstim.jl: Simplified Parameter Estimation Interface","text":"This package is for simplified parameter estimation. While not as flexible of a system like DiffEqFlux.jl, it provides ready-made functions for doing standard optimization procedures like L2 fitting and MAP estimates. Among other features, it allows for the optimization of parameters in ODEs, stochastic problems, and delay differential equations.","category":"section"},{"location":"highlevels/inverse_problems/#DiffEqBayes.jl:-Simplified-Bayesian-Estimation-Interface","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"DiffEqBayes.jl: Simplified Bayesian Estimation Interface","text":"As the name suggests, this package has been designed to provide the estimation of differential equations parameters by Bayesian methods. It works in conjunction with Turing.jl, CmdStan.jl, DynamicHMC.jl, and ApproxBayes.jl. While not as flexible as direct usage of DiffEqFlux.jl or Turing.jl, DiffEqBayes.jl can be an approachable interface for those not familiar with Bayesian estimation, and provides a nice way to use Stan from pure Julia.","category":"section"},{"location":"highlevels/inverse_problems/#Third-Party-Tools-of-Note","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Third-Party Tools of Note","text":"","category":"section"},{"location":"highlevels/inverse_problems/#Turing.jl:-A-Flexible-Probabilistic-Programming-Language-for-Bayesian-Analysis","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Turing.jl: A Flexible Probabilistic Programming Language for Bayesian Analysis","text":"In the context of differential equations and parameter estimation, Turing.jl allows for a Bayesian estimation of differential equations (used in conjunction with the high-level package DiffEqBayes.jl). For more examples on combining Turing.jl with DiffEqBayes.jl, see the documentation below. It is important to note that Turing.jl can also perform Bayesian estimation without relying on DiffEqBayes.jl (for an example, consult this tutorial).","category":"section"},{"location":"highlevels/inverse_problems/#Topopt.jl:-Topology-Optimization-in-Julia","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Topopt.jl: Topology Optimization in Julia","text":"Topopt.jl solves topology optimization problems which are inverse problems on partial differential equations, solving for an optimal domain.","category":"section"},{"location":"highlevels/inverse_problems/#Recommended-Automatic-Differentiation-Libraries","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Recommended Automatic Differentiation Libraries","text":"Solving inverse problems commonly requires using automatic differentiation (AD). SciML includes extensive support for automatic differentiation throughout its solvers, though some AD libraries are more tested than others. The following libraries are the current recommendations of the SciML developers.","category":"section"},{"location":"highlevels/inverse_problems/#ForwardDiff.jl:-Operator-Overloading-Forward-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"ForwardDiff.jl: Operator-Overloading Forward Mode Automatic Differentiation","text":"ForwardDiff.jl is a library for operator-overloading based forward-mode automatic differentiation. It's commonly used as the default method for generating Jacobians throughout the SciML solver libraries.\n\nnote: Note\nBecause ForwardDiff.jl uses an operator overloading approach, uses of ForwardDiff.jl require that any caches for non-allocating mutating code allows for Dual numbers. To allow such code to be ForwardDiff.jl-compatible, see PreallocationTools.jl.","category":"section"},{"location":"highlevels/inverse_problems/#Enzyme.jl:-LLVM-Level-Forward-and-Reverse-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Enzyme.jl: LLVM-Level Forward and Reverse Mode Automatic Differentiation","text":"Enzyme.jl is an LLVM-level AD library for forward and reverse automatic differentiation. It supports many features required for high performance, such as being able to differentiate mutating and interleave compiler optimization with the AD passes. However, it does not support all of the Julia runtime, and thus some code with many dynamic behaviors and garbage collection (GC) invocations can be incompatible with Enzyme. Enzyme.jl is quickly becoming the new standard AD for SciML.","category":"section"},{"location":"highlevels/inverse_problems/#Zygote.jl:-Julia-Level-Source-to-Source-Reverse-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"Zygote.jl: Julia-Level Source-to-Source Reverse Mode Automatic Differentiation","text":"Zygote.jl is the current standard user-level reverse-mode automatic differentiation library for the SciML solvers. User-level means that many library tutorials, like in SciMLSensitivity.jl and DiffEqFlux.jl, showcase user code using Zygote.jl. This is because Zygote.jl is the AD engine associated with the Flux machine learning library. However, Zygote.jl has many limitations which limits its performance in equation solver contexts, such as an inability to handle mutation and introducing many small allocations and type-instabilities. For this reason, the SciML equation solvers define differentiation overloads using ChainRules.jl, meaning that the equation solvers tend not to use Zygote.jl internally even if the user code uses Zygote.gradient. In this manner, the speed and performance of more advanced techniques can be preserved while using the Julia standard.","category":"section"},{"location":"highlevels/inverse_problems/#FiniteDiff.jl:-Fast-Finite-Difference-Approximations","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"FiniteDiff.jl: Fast Finite Difference Approximations","text":"FiniteDiff.jl is the preferred fallback library for numerical differentiation and is commonly used by SciML solver libraries when automatic differentiation is disabled.","category":"section"},{"location":"highlevels/inverse_problems/#SparseDiffTools.jl:-Tools-for-Fast-Automatic-Differentiation-with-Sparse-Operators","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems","title":"SparseDiffTools.jl: Tools for Fast Automatic Differentiation with Sparse Operators","text":"SparseDiffTools.jl is a library for sparse automatic differentiation. It's used internally by many of the SciML equation solver libraries, which explicitly expose interfaces for colorvec color vectors generated by SparseDiffTools.jl's methods. SparseDiffTools.jl also includes many features useful to users, such as operators for matrix-free Jacobian-vector and Hessian-vector products.","category":"section"},{"location":"getting_started/find_root/#find_root","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Find the root of an equation (i.e. solve f(u)=0)","text":"A nonlinear system f(u) = 0 is specified by defining a function f(u,p), where p are the parameters of the system. Many problems can be written in such a way that solving a nonlinear rootfinding problem gives the solution. For example:\n\nDo you want to know u such that 4^u + 6^u = 7^u? Then solve f(u) = 4^u + 6^u - 7^u = 0 for u!\nIf you have an ODE u = f(u), what is the point where the solution will be completely still, i.e. u' = 0?\n\nAll of these problems are solved by using a numerical rootfinder. Let's solve our first rootfind problem!","category":"section"},{"location":"getting_started/find_root/#Required-Dependencies","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Required Dependencies","text":"The following parts of the SciML Ecosystem will be used in this tutorial:\n\nModule Description\nModelingToolkit.jl The symbolic modeling environment\nNonlinearSolve.jl The numerical solvers for nonlinear equations","category":"section"},{"location":"getting_started/find_root/#Problem-Setup","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Problem Setup","text":"For example, the following solves the vector equation:\n\nbeginaligned\n0 = Ïƒ*(y-x)\n0 = x*(Ï-z)-y\n0 = x*y - Î²*z\nendaligned\n\nWith the parameter values sigma = 100, rho = 260, beta = 83.\n\n# Import the packages\nimport ModelingToolkit as MTK\nimport NonlinearSolve as NLS\nimport ModelingToolkit: @variables, @parameters, @mtkcompile, mtkcompile\n\n# Define the nonlinear system\n@variables x=1.0 y=0.0 z=0.0\n@parameters Ïƒ=10.0 Ï=26.0 Î²=8 / 3\n\neqs = [0 ~ Ïƒ * (y - x),\n    0 ~ x * (Ï - z) - y,\n    0 ~ x * y - Î² * z]\n@mtkcompile ns = MTK.NonlinearSystem(eqs, [x, y, z], [Ïƒ, Ï, Î²])\n\n# Convert the symbolic system into a numerical system\nprob = NLS.NonlinearProblem(ns, [])\n\n# Solve the numerical problem\nsol = NLS.solve(prob, NLS.NewtonRaphson())\n\n# Analyze the solution\n@show sol[[x, y, z]], sol.resid","category":"section"},{"location":"getting_started/find_root/#Step-by-Step-Solution","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step-by-Step Solution","text":"","category":"section"},{"location":"getting_started/find_root/#Step-1:-Import-the-Packages","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step 1: Import the Packages","text":"To do this tutorial, we will need a few components:\n\nModelingToolkit.jl, our modeling environment\nNonlinearSolve.jl, the nonlinear system solvers\n\nTo start, let's add these packages as demonstrated in the installation tutorial:\n\nimport Pkg\nPkg.add([\"ModelingToolkit\", \"NonlinearSolve\"])\n\nNow we're ready. Let's load in these packages:\n\n# Import the packages\nimport ModelingToolkit as MTK\nimport NonlinearSolve as NLS\nimport ModelingToolkit: @variables, @parameters, @mtkcompile, mtkcompile","category":"section"},{"location":"getting_started/find_root/#Step-2:-Define-the-Nonlinear-System","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step 2: Define the Nonlinear System","text":"Now let's define our nonlinear system. We use the ModelingToolkit.@variabes statement to declare our 3 state variables:\n\n# Define the nonlinear system\n@variables x=1.0 y=0.0 z=0.0\n\nNotice that we are using the form state = initial condition. This is a nice shorthand for coupling an initial condition to our states. We now must similarly define our parameters, which we can associate default values via the form parameter = default value. This looks like:\n\n@parameters Ïƒ=10.0 Ï=26.0 Î²=8 / 3\n\nNow we create an array of equations to define our nonlinear system that must be satisfied. This looks as follows:\n\nnote: Note\nNote that in ModelingToolkit and Symbolics, ~ is used for equation equality. This is separate from = which is the â€œassignment operatorâ€ in the Julia programming language. For example, x = x + 1 is a valid assignment in a programming language, and it is invalid for that to represent â€œequalityâ€, which is why a separate operator is used!\n\neqs = [0 ~ Ïƒ * (y - x),\n    0 ~ x * (Ï - z) - y,\n    0 ~ x * y - Î² * z]\n\nFinally, we bring these pieces together, the equation along with its states and parameters, define our NonlinearSystem:\n\n@mtkcompile ns = MTK.NonlinearSystem(eqs, [x, y, z], [Ïƒ, Ï, Î²])","category":"section"},{"location":"getting_started/find_root/#Step-3:-Convert-the-Symbolic-Problem-to-a-Numerical-Problem","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step 3: Convert the Symbolic Problem to a Numerical Problem","text":"Now that we have created our system, let's turn it into a numerical problem to approximate. This is done with the NonlinearProblem constructor, that transforms it from a symbolic ModelingToolkit representation to a numerical NonlinearSolve representation. We need to tell it the numerical details for whether to override any of the default values for the initial conditions and parameters.\n\nIn this case, we will use the default values for all our variables, so we will pass a blank override []. This looks like:\n\n# Convert the symbolic system into a numerical system\nprob = NLS.NonlinearProblem(ns, [])\n\nIf we did want to change the initial condition of x to 2.0 and the parameter Ïƒ to 4.0, we would do [x => 2.0, Ïƒ => 4.0]. This looks like:\n\nprob2 = NLS.NonlinearProblem(ns, [x => 2.0, Ïƒ => 4.0])","category":"section"},{"location":"getting_started/find_root/#Step-4:-Solve-the-Numerical-Problem","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step 4: Solve the Numerical Problem","text":"Now we solve the nonlinear system. For this, we choose a solver from the NonlinearSolve.jl's solver options. We will choose NewtonRaphson as follows:\n\n# Solve the numerical problem\nsol = NLS.solve(prob, NLS.NewtonRaphson())","category":"section"},{"location":"getting_started/find_root/#Step-5:-Analyze-the-Solution","page":"Find the root of an equation (i.e. solve f(u)=0)","title":"Step 5: Analyze the Solution","text":"Now let's check out the solution. First of all, what kind of thing is the sol? We can see that by asking for its type:\n\ntypeof(sol)\n\nFrom this, we can see that it is an NonlinearSolution. We can see the documentation for how to use the NonlinearSolution by checking the NonlinearSolve.jl solution type page. For example, the solution is stored as .u. What is the solution to our nonlinear system, and what is the final residual value? We can check it as follows:\n\n# Analyze the solution\n@show sol[[x, y, z]], sol.resid","category":"section"},{"location":"highlevels/uncertainty_quantification/#Uncertainty-Quantification","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"There's always uncertainty in our models. Whether it's in the form of the model's equations or in the model's parameters, the uncertainty in our simulation's output often needs to be quantified. The following tools automate this process.\n\nFor Measurements.jl vs MonteCarloMeasurements.jl vs Intervals.jl, and the relation to other methods, see the Uncertainty Programming chapter of the SciML Book.","category":"section"},{"location":"highlevels/uncertainty_quantification/#PolyChaos.jl:-Intrusive-Polynomial-Chaos-Expansions-Made-Unintrusive","page":"Uncertainty Quantification","title":"PolyChaos.jl: Intrusive Polynomial Chaos Expansions Made Unintrusive","text":"PolyChaos.jl is a library for calculating intrusive polynomial chaos expansions (PCE) on arbitrary Julia functions. This allows for inputting representations of probability distributions into functions to compute the output distribution in an expansion representation. While normally this would require deriving the PCE-expanded equations by hand, PolyChaos.jl does this at the compiler level using Julia's multiple dispatch, giving a high-performance implementation to a normally complex and tedious mathematical transformation.","category":"section"},{"location":"highlevels/uncertainty_quantification/#SciMLExpectations.jl:-Fast-Calculations-of-Expectations-of-Equation-Solutions","page":"Uncertainty Quantification","title":"SciMLExpectations.jl: Fast Calculations of Expectations of Equation Solutions","text":"SciMLExpectations.jl is a library for accelerating the calculation of expectations of equation solutions with respect to input probability distributions, allowing for applications like robust optimization with respect to uncertainty. It uses Koopman operator techniques to calculate these expectations without requiring the propagation of uncertainties through a solver, effectively performing the adjoint of uncertainty quantification and being much more efficient in the process.","category":"section"},{"location":"highlevels/uncertainty_quantification/#Third-Party-Libraries-to-Note","page":"Uncertainty Quantification","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/#Measurements.jl:-Automated-Linear-Error-Propagation","page":"Uncertainty Quantification","title":"Measurements.jl: Automated Linear Error Propagation","text":"Measurements.jl is a library for automating linear error propagation. Uncertain numbers are defined as x = 3.8 Â± 0.4 and are pushed through calculations using a normal distribution approximation in order to compute an approximate uncertain output. Measurements.jl uses a dictionary-based approach to keep track of correlations to improve the accuracy over naive implementations, though note that linear error propagation theory still has some major issues handling some types of equations, as described in detail in the MonteCarloMeasurements.jl documentation.","category":"section"},{"location":"highlevels/uncertainty_quantification/#MonteCarloMeasurements.jl:-Automated-Monte-Carlo-Error-Propagation","page":"Uncertainty Quantification","title":"MonteCarloMeasurements.jl: Automated Monte Carlo Error Propagation","text":"MonteCarloMeasurements.jl is a library for automating the uncertainty quantification of equation solution using Monte Carlo methods. It defines number types which sample from an input distribution to receive a representative set of parameters that propagate through the solver to calculate a representative set of possible solutions. Note that Monte Carlo techniques can be expensive but are exact, in the sense that as the number of sample points increases to infinity it will compute a correct approximation of the output uncertainty.","category":"section"},{"location":"highlevels/uncertainty_quantification/#ProbNumDiffEq.jl:-Probabilistic-Numerics-Based-Differential-Equation-Solvers","page":"Uncertainty Quantification","title":"ProbNumDiffEq.jl: Probabilistic Numerics Based Differential Equation Solvers","text":"ProbNumDiffEq.jl is a set of probabilistic numerical ODE solvers which compute the solution of a differential equation along with a posterior distribution to estimate its numerical approximation error. Thus these specialized integrators compute an uncertainty output similar to the ProbInts technique of DiffEqUncertainty, but use specialized integration techniques in order to do it much faster for specific kinds of equations.","category":"section"},{"location":"highlevels/uncertainty_quantification/#TaylorIntegration.jl:-Taylor-Series-Integration-for-Rigorous-Numerical-Bounds","page":"Uncertainty Quantification","title":"TaylorIntegration.jl: Taylor Series Integration for Rigorous Numerical Bounds","text":"TaylorIntegration.jl is a library for Taylor series integrators, which has special functionality for computing the interval bound of possible solutions with respect to numerical approximation error.","category":"section"},{"location":"highlevels/uncertainty_quantification/#IntervalArithmetic.jl:-Rigorous-Numerical-Intervals","page":"Uncertainty Quantification","title":"IntervalArithmetic.jl: Rigorous Numerical Intervals","text":"IntervalArithmetic.jl is a library for performing interval arithmetic calculations on arbitrary Julia code. Interval arithmetic computes rigorous computations with respect to finite-precision floating-point arithmetic, i.e. its intervals are guaranteed to include the true solution. However, interval arithmetic intervals can grow at exponential rates in many problems, thus being unsuitable for analyses in many equation solver contexts.","category":"section"},{"location":"getting_started/installation/#installation","page":"Installing SciML Software","title":"Installing SciML Software","text":"","category":"section"},{"location":"getting_started/installation/#Step-1:-Install-Julia","page":"Installing SciML Software","title":"Step 1: Install Julia","text":"Download Julia using this website.\n\nnote: Note\nSome Linux distributions do weird and incorrect things with Julia installations! Please install Julia using the binaries provided by the official JuliaLang website!\n\nTo ensure that you have installed Julia correctly, open it up and type versioninfo() in the REPL. It should look like the following:\n\n(Image: )\n\n(with the CPU/OS/etc. details matching your computer!)\n\nIf you got stuck in this installation process, ask for help on the Julia Discourse or in the Julia Zulip chatrooms","category":"section"},{"location":"getting_started/installation/#Optional-Step-1.5:-Get-VS-Code-Setup-with-the-Julia-Extension","page":"Installing SciML Software","title":"Optional Step 1.5: Get VS Code Setup with the Julia Extension","text":"You can run SciML with Julia in any development environment you please, but our recommended environment is VS Code. For more information on using Julia with VS Code, check out the Julia VS Code Extension website. Let's install it!\n\nFirst download VS Code from the official website.\n\nNext, open Visual Studio Code and click Extensions.\n\n(Image: )\n\nThen, search for â€œJuliaâ€ in the search bar on the top of the extension tab, click on the â€œJuliaâ€ extension, and click the install button on the tab that opens up.\n\n(Image: )\n\nTo make sure your installation is correct, try running some code. Open a new file by either going to the top left navigation bar File |> New Text File, or hitting Ctrl+n. Name your new file test.jl (important: the Julia VS Code functionality only turns on when using a .jl file!). Next, type 1+1 and hit Ctrl+Enter. A Julia REPL should pop up and the result 2 should be displayed. Your environment should look something like this:\n\n(Image: )\n\nFor more help on using the VS Code editor with Julia, check out the VS Code in Julia documentation. Useful keyboard commands can be found here.\n\nOnce again, if you got stuck in this installation process, ask for help on the Julia Discourse or in the Julia Zulip chatrooms","category":"section"},{"location":"getting_started/installation/#Step-2:-Install-a-SciML-Package","page":"Installing SciML Software","title":"Step 2: Install a SciML Package","text":"SciML is over 130 Julia packages. That's too much stuff to give someone in a single download! Thus instead, the SciML organization divides its functionality into composable modules that can be mixed and matched as required. Installing SciML ecosystem functionality is equivalent to installation of such packages.\n\nFor example, do you need the differential equation solver? Then install DifferentialEquations via the command:\n\nusing Pkg;\nPkg.add(\"DifferentialEquations\");\n\nin the Julia REPL. Or, for a more robust REPL experience, hit the ] command to make the blue pkg> REPL environment start, and type in add DifferentialEquations. The package REPL environment will have nice extras like auto-complete that will be useful in the future. This command should run an installation sequence and precompile all of the packages (precompile = \"run a bunch of performance optimizations!\"). Don't be surprised if this installation process takes ~10 minutes on older computers. During the installation, it should look like this:\n\n(Image: )\n\nAnd that's it!","category":"section"},{"location":"getting_started/installation/#How-do-I-test-that-my-installed-correctly?","page":"Installing SciML Software","title":"How do I test that my installed correctly?","text":"The best way is to build and run your first simulation!","category":"section"},{"location":"highlevels/plots_visualization/#SciML-Supported-Plotting-and-Visualization-Libraries","page":"SciML-Supported Plotting and Visualization Libraries","title":"SciML-Supported Plotting and Visualization Libraries","text":"The following libraries are the plotting and visualization libraries which are supported and co-developed by the SciML developers. Other libraries may be used, though these are the libraries used in the tutorials and which have special hooks to ensure ergonomic usage with SciML tooling.","category":"section"},{"location":"highlevels/plots_visualization/#Plots.jl","page":"SciML-Supported Plotting and Visualization Libraries","title":"Plots.jl","text":"Plots.jl is the current standard plotting system for the SciML ecosystem. SciML types attempt to include plot recipes for as many types as possible, allowing for automatic visualization with the Plots.jl system. All current tutorials and documentation default to using Plots.jl.","category":"section"},{"location":"highlevels/plots_visualization/#Makie.jl","page":"SciML-Supported Plotting and Visualization Libraries","title":"Makie.jl","text":"Makie.jl is a high-performance interactive plotting system for the Julia programming language. It's planned to be the default plotting system used by the SciML organization in the near future.","category":"section"},{"location":"highlevels/model_libraries_and_importers/#Model-Libraries-and-Importers","page":"Model Libraries and Importers","title":"Model Libraries and Importers","text":"Models are passed on from generation to generation. Many models are not built from scratch but have a legacy of the known physics, biology, and chemistry embedded into them. Julia's SciML offers a range of pre-built modeling tools, from reusable acausal components to direct imports from common file formats.","category":"section"},{"location":"highlevels/model_libraries_and_importers/#ModelingToolkitStandardLibrary.jl:-A-Standard-Library-for-ModelingToolkit","page":"Model Libraries and Importers","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"Given the composable nature of acausal modeling systems, it's helpful to not have to define every component from scratch and instead build off a common base of standard components. ModelingToolkitStandardLibrary.jl is that library. It provides components for standard models to start building everything from circuits and engines to robots.\n\n(Image: )","category":"section"},{"location":"highlevels/model_libraries_and_importers/#DiffEqCallbacks.jl:-Pre-Made-Callbacks-for-DifferentialEquations.jl","page":"Model Libraries and Importers","title":"DiffEqCallbacks.jl: Pre-Made Callbacks for DifferentialEquations.jl","text":"DiffEqCallbacks.jl has many event handling and callback definitions which allow for quickly building up complex differential equation models. It includes:\n\nCallbacks for specialized output and saving procedures\nCallbacks for enforcing domain constraints, positivity, and manifolds\nTimed callbacks for periodic dosing, presetting of tstops, and more\nCallbacks for determining and terminating at steady state\nCallbacks for controlling stepsizes and enforcing CFL conditions\nCallbacks for quantifying uncertainty with respect to numerical errors","category":"section"},{"location":"highlevels/model_libraries_and_importers/#SBMLToolkit.jl:-SBML-Import","page":"Model Libraries and Importers","title":"SBMLToolkit.jl: SBML Import","text":"SBMLToolkit.jl is a library for reading SBML files into the standard formats for Catalyst.jl and ModelingToolkit.jl. There are well over one thousand biological models available in the BioModels Repository.","category":"section"},{"location":"highlevels/model_libraries_and_importers/#CellMLToolkit.jl:-CellML-Import","page":"Model Libraries and Importers","title":"CellMLToolkit.jl: CellML Import","text":"CellMLToolkit.jl is a library for reading CellML files into the standard formats for ModelingToolkit.jl. There are several hundred biological models available in the CellML Model Repository.","category":"section"},{"location":"highlevels/model_libraries_and_importers/#ReactionNetworkImporters.jl:-BioNetGen-Import","page":"Model Libraries and Importers","title":"ReactionNetworkImporters.jl: BioNetGen Import","text":"ReactionNetworkImporters.jl is a library for reading BioNetGen .net files and various stoichiometry matrix representations into the standard formats for Catalyst.jl and ModelingToolkit.jl.","category":"section"},{"location":"comparisons/cppfortran/#cppfortran","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"Getting Started with Julia's SciML for the C++/Fortran User","text":"You don't need help if you're a Fortran guru. I'm just kidding, you're not a Lisp developer. If you're coming from C++ or Fortran, you may be familiar with high-performance computing environments similar to SciML, such as PETSc, Trilinos, or Sundials. The following are some points to help the transition.","category":"section"},{"location":"comparisons/cppfortran/#Why-SciML?-High-Level-Workflow-Reasons","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"Why SciML? High-Level Workflow Reasons","text":"If you're coming from â€œhardcoreâ€ C++/Fortran computing environments, some things to check out with Julia's SciML are:\n\nInteractivity - use the interactive REPL to easily investigate numerical details.\nMetaprogramming performance tools - tools like LoopVectorization.jl can be used to generate faster code than even some of the most hand-optimized C++/Fortran code. Current benchmarks show this SIMD-optimized Julia code outperforming OpenBLAS and MKL BLAS implementations in many performance regimes.\nSymbolic modeling languages - writing models by hand can leave a lot of performance on the table. Using high-level modeling tools like ModelingToolkit can automate symbolic simplifications, which improve the stability and performance of numerical solvers. On complex models, even the best handwritten C++/Fortran code is orders of magnitude behind the code that symbolic tearing algorithms can achieve!\nComposable Library Components - In C++/Fortran environments, every package feels like a silo. Arrays made for PETSc cannot easily be used in Trilinos, and converting Sundials NVector outputs to DataFrames for post-simulation data processing is a process itself. The Julia SciML environment embraces interoperability. Don't wait for SciML to do it: by using generic coding with JIT compilation, these connections create new optimized code on the fly and allow for a more expansive feature set than can ever be documented. Take new high-precision number types from a package and stick them into a nonlinear solver. Take a package for Intel GPU arrays and stick it into the differential equation solver to use specialized hardware acceleration.\nWrappers to the Libraries You Know and Trust - Moving to SciML does not have to be a quick transition. SciML has extensive wrappers to many widely-used classical solver environments such as SUNDIALS and Hairer's classic Fortran ODE solvers (dopri5, dop853, etc.). Using these wrapped solvers is painless and can be swapped in for the Julia versions with one line of code. This gives you a way to incrementally adopt new features/methods while retaining the older pieces you know and trust.\nDon't Start from Scratch - SciML builds on the extensive Base library of Julia, and thus grows and improves with every update to the language. With hundreds of monthly contributors to SciML and hundreds of monthly contributors to Julia, SciML is one of the most actively developed open-source scientific computing ecosystems out there!\nEasier High-Performance and Parallel Computing - With Julia's ecosystem, CUDA will automatically install of the required binaries and cu(A)*cu(B) is then all that's required to GPU-accelerate large-scale linear algebra. MPI is easy to install and use. Distributed computing through password-less SSH. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to automate the discovery of missing physical laws using neural networks embedded in differentiable simulations? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\n\nIn this plot, Sundials/Hairer in purple/red represent C++/Fortrans most commonly used solvers:\n\n(Image: )","category":"section"},{"location":"comparisons/cppfortran/#Why-SciML?-Some-Technical-Details","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"Why SciML? Some Technical Details","text":"Let's face the facts, in the open benchmarks the pure-Julia solvers tend to outperform the classic â€œbestâ€ C++ and Fortran solvers in almost every example (with a few notable exceptions). But why?\n\nThe answer is two-fold: Julia is as fast as C++/Fortran, and the algorithms are what matter.","category":"section"},{"location":"comparisons/cppfortran/#Julia-is-as-Fast-as-C/Fortran","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"Julia is as Fast as C++/Fortran","text":"While Julia code looks high level like Python or MATLAB, its performance is on par with C++ and Fortran. At a technical level, when Julia code is type-stable, i.e. that the types that are returned from a function are deducible at compile-time from the types that go into a function, then Julia can optimize it as much as C++ or Fortran by automatically devirtualizing all dynamic behavior and compile-time optimizing the quasi-static code. This is not an empirical statement, it's a provable type-theoretic result. The resulting compiler used on the resulting quasi-static representation is LLVM, the same optimizing compiler used by clang and LFortran.\n\nFor more details on how Julia code is optimized and how to optimize your own Julia code, check out this chapter from the SciML Book.","category":"section"},{"location":"comparisons/cppfortran/#SciML's-Julia-Algorithms-Have-Performance-Advantages-in-Many-Common-Regimes","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"SciML's Julia Algorithms Have Performance Advantages in Many Common Regimes","text":"There are many ways which Julia's algorithms achieve performance advantages. Some facts to highlight include:\n\nJulia is at the forefront of numerical methods research in many domains. This is highlighted in the differential equation solver comparisons, where the Julia solvers were the first to incorporate â€œnewerâ€ optimized Runge-Kutta tableaus, around half a decade before other software. Since then, the literature has only continued to evolve, and only Julia's SciML keeps up. At this point, many of the publication's first implementation is in OrdinaryDiffEq.jl with benchmark results run on the SciML Open Benchmarking platform!\nJulia does not take low-level mathematical functions for granted. The common openlibm implementation of mathematical functions used in many open source projects is maintained by the Julia and SciML developers! However, in modern Julia, every function from   log to ^ has been reimplemented in the Julia standard library to improve numerical correctness and performance. For example, Pumas, the nonlinear mixed effects estimation system built on SciML, and used by Moderna for the vaccine trials, notes in its paper that approximations to such math libraries itself gave a 2x performance improvement in even the most simple non-stiff ODE solvers over matching Fortran implementations. Pure Julia linear algebra tooling, like RecursiveFactorization.jl for LU-factorization, outperforms common LU-factorization implementations used in open-source projects like OpenBLAS by around 5x! This should not be surprising though, given that OpenBLAS was a prior MIT Julia Lab project!\nCompilers are limited on the transformations that they can perform because they do not have high-level context-dependent mathematical knowledge. Julia's SciML makes extensive use of customized symbolic-based compiler transformations to improve performance with context-based code optimizations. Things like sparsity patterns are automatically deduced from code and optimized on. Nonlinear equations are symbolically-torn, changing large nonlinear systems into sequential solving of much smaller systems and benefiting from an O(n^3) cost reduction. These can be orders of magnitude cost reductions which come for free, and unless you know every trick in the book it will be difficult to match SciML's performance!\nPervasive automatic differentiation mixed with compiler tricks wins battles. Many high-performance libraries in C++ and Fortran cannot assume that all of its code is compatible with automatic differentiation, and thus many internal performance tricks are not applied. For example, ForwardDiff.jl's chunk seeding allows for a single call to f to generate multiple columns of a Jacobian. When mixed with sparse coloring tools, entire Jacobians can be constructed with just a few f calls. Studies in applications have shown this greatly outperforms finite differencing, especially when Julia's implicit multithreading is used.","category":"section"},{"location":"comparisons/cppfortran/#Let's-Dig-Deep-Into-One-Case:-Adjoints-of-ODEs-for-Solving-Inverse-Problems","page":"Getting Started with Julia's SciML for the C++/Fortran User","title":"Let's Dig Deep Into One Case: Adjoints of ODEs for Solving Inverse Problems","text":"To really highlight how JIT compilation and automatic differentiation integration can change algorithms, let's look at the problem of differentiating an ODE solver. As is derived and discussed in detail at a seminar with the American Statistical Association, there are many ways to implement well-known â€œadjointâ€ methods which are required for performance. Each has different stability and performance trade-offs, and Julia's SciML is the only system to systemically offer all of the trade-off options. In many cases, using analytical adjoints of a solver is not advised due to performance reasons, with the trade-off described in detail here. Likewise, even when analytical adjoints are used, it turns out that for general nonlinear equations there is a trick which uses automatic differentiation in the construction of the analytical adjoint to improve its performance. As demonstrated in this publication, this can lead to about 2-3 orders of magnitude performance improvements. These AD-enhanced adjoints are showcased as the seeding methods in this plot:\n\n(Image: )\n\nUnless one directly defines special â€œvjpâ€ functions, this is how the Julia SciML methods achieve orders of magnitude performance advantages over CVODES's adjoints and PETSC's TS-adjoint.\n\nMoral of the story, even there are many reasons to use automatic differentiation of a solver, and even if an analytical adjoint rule is used for some specific performance reason, that analytical expression can often times be accelerated by orders of magnitude itself by embedding some form of automatic differentiation into it. This is just one algorithm of many which are optimized in this fashion.","category":"section"},{"location":"getting_started/first_optimization/#first_opt","page":"Solve your first optimization problem","title":"Solve your first optimization problem","text":"Numerical optimization is the process of finding some numerical values that minimize some equation.\n\nHow much fuel should you put into an airplane to have the minimum weight that can go to its destination?\nWhat parameters should I choose for my simulation so that it minimizes the distance of its predictions from my experimental data?\n\nAll of these are examples of problems solved by numerical optimization. Let's solve our first optimization problem!","category":"section"},{"location":"getting_started/first_optimization/#Required-Dependencies","page":"Solve your first optimization problem","title":"Required Dependencies","text":"The following parts of the SciML Ecosystem will be used in this tutorial:\n\nModule Description\nOptimization.jl The numerical optimization package\nOptimizationNLopt.jl The NLopt optimizers we will use\nForwardDiff.jl The automatic differentiation library for gradients","category":"section"},{"location":"getting_started/first_optimization/#Problem-Setup","page":"Solve your first optimization problem","title":"Problem Setup","text":"First, what are we solving? Let's take a look at the Rosenbrock equation:\n\nL(up) = (p_1 - u_1)^2 + p_2 * (u_2 - u_1^2)^2\n\nWhat we want to do is find the  values of u_1 and u_2 such that L achieves its minimum value possible. We will do this under a few constraints: we want to find this optimum within some bounded domain, i.e. u_i in -11. This should be done with the parameter values p_1 = 10 and p_2 = 1000. What should u = u_1u_2 be to achieve this goal? Let's dive in!\n\nnote: Note\nThe upper and lower bounds are optional for the solver! If your problem does not need to have such bounds, just leave off the parts with lb and ub!","category":"section"},{"location":"getting_started/first_optimization/#Copy-Pastable-Code","page":"Solve your first optimization problem","title":"Copy-Pastable Code","text":"# Import the package\nimport Optimization as OPT\nimport OptimizationNLopt\nimport ForwardDiff\n\n# Define the problem to optimize\nL(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\nu0 = zeros(2)\np = [1.0, 100.0]\noptfun = OPT.OptimizationFunction(L, OPT.AutoForwardDiff())\nprob = OPT.OptimizationProblem(optfun, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\n\n# Solve the optimization problem\nsol = OPT.solve(prob, OptimizationNLopt.NLopt.LD_LBFGS())\n\n# Analyze the solution\n@show sol.u, L(sol.u, p)","category":"section"},{"location":"getting_started/first_optimization/#Step-by-Step-Solution","page":"Solve your first optimization problem","title":"Step-by-Step Solution","text":"","category":"section"},{"location":"getting_started/first_optimization/#Step-1:-Import-the-packages","page":"Solve your first optimization problem","title":"Step 1: Import the packages","text":"To do this tutorial, we will need a few components:\n\nOptimization.jl, the optimization interface.\nOptimizationNLopt.jl, the optimizers we will use.\nForwardDiff.jl, the automatic differentiation library for gradients\n\nNote that Optimization.jl is an interface for optimizers, and thus we always have to choose which optimizer we want to use. Here we choose to demonstrate OptimizationNLopt because of its efficiency and versatility. But there are many other possible choices. Check out the solver compatibility chart for a quick overview of what optimizer packages offer.\n\nTo start, let's add these packages as demonstrated in the installation tutorial:\n\nusing Pkg\nPkg.add([\"Optimization\", \"OptimizationNLopt\", \"ForwardDiff\"])\n\nNow we're ready. Let's load in these packages:\n\nimport Optimization as OPT\nimport OptimizationNLopt\nimport ForwardDiff","category":"section"},{"location":"getting_started/first_optimization/#Step-2:-Define-the-Optimization-Problem","page":"Solve your first optimization problem","title":"Step 2: Define the Optimization Problem","text":"Now let's define our problem to optimize. We start by defining our loss function. In Optimization.jl's OptimizationProblem interface, the states are given by an array u. Thus we can designate u[1] to be u_1 and u[2] to be u_2, similarly with our parameters, and write out the loss function on a vector-defined state as follows:\n\n# Define the problem to optimize\nL(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\n\nNext we need to create an OptimizationFunction where we tell Optimization.jl to use the ForwardDiff.jl package for creating the gradient and other derivatives required by the optimizer.\n\n#Create the OptimizationFunction\noptfun = OPT.OptimizationFunction(L, OPT.AutoForwardDiff())\n\nNow we need to define our OptimizationProblem. If you need help remembering how to define the OptimizationProblem, you can always refer to the Optimization.jl problem definition page.\n\nThus what we need to define is an initial condition u0 and our parameter vector p. We will make our initial condition have both values as zero, which is done by the Julia shorthand zeros(2) that creates a vector [0.0,0.0]. We manually define the parameter vector p to input our values. Then we set the lower bound and upper bound for the optimization as follows:\n\nu0 = zeros(2)\np = [1.0, 100.0]\nprob = OPT.OptimizationProblem(optfun, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])","category":"section"},{"location":"getting_started/first_optimization/#Note-about-defining-uniform-bounds","page":"Solve your first optimization problem","title":"Note about defining uniform bounds","text":"Note that we can simplify the code a bit for the lower and upper bound definition by using the Julia Base command ones, which returns a vector where each value is a one. Thus for example, ones(2) is equivalent to [1.0,1.0]. Therefore -1 * ones(2) is equivalent to [-1.0,-1.0], meaning we could have written our problem as follows:\n\nprob = OPT.OptimizationProblem(optfun, u0, p, lb = -1 * ones(2), ub = ones(2))","category":"section"},{"location":"getting_started/first_optimization/#Step-3:-Solve-the-Optimization-Problem","page":"Solve your first optimization problem","title":"Step 3: Solve the Optimization Problem","text":"Now we solve the OptimizationProblem that we have defined. This is done by passing our OptimizationProblem along with a chosen solver to the solve command. At the beginning, we explained that we will use the OptimizationNLopt set of solvers, which are documented in the OptimizationNLopt page. From here, we are choosing the NLopt.LD_LBFGS() for its mixture of robustness and performance. To perform this solve, we do the following:\n\n# Solve the optimization problem\nsol = OPT.solve(prob, OptimizationNLopt.NLopt.LD_LBFGS())","category":"section"},{"location":"getting_started/first_optimization/#Step-4:-Analyze-the-Solution","page":"Solve your first optimization problem","title":"Step 4: Analyze the Solution","text":"Now let's check out the solution. First of all, what kind of thing is the sol? We can see that by asking for its type:\n\ntypeof(sol)\n\nFrom this, we can see that it is an OptimizationSolution. We can see the documentation for how to use the OptimizationSolution by checking the Optimization.jl solution type page. For example, the solution is stored as .u. What is the solution to our optimization, and what is the final loss value? We can check it as follows:\n\n# Analyze the solution\n@show sol.u, L(sol.u, p)","category":"section"},{"location":"showcase/massively_parallel_gpu/#datagpu","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Massively Data-Parallel ODE Solving on GPUs","text":"","category":"section"},{"location":"showcase/massively_parallel_gpu/#Before-we-start:-the-two-ways-to-accelerate-ODE-solvers-with-GPUs","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Before we start: the two ways to accelerate ODE solvers with GPUs","text":"Before we dive deeper, let us remark that there are two very different ways that one can accelerate an ODE solution with GPUs. There is one case where u is very big and f is very expensive but very structured, and you use GPUs to accelerate the computation of said f. The other use case is where u is very small, but you want to solve the ODE f over many different initial conditions (u0) or parameters p. In that case, you can use GPUs to parallelize over different parameters and initial conditions. In other words:\n\nType of Problem SciML Solution\nAccelerate a big ODE Use CUDA.jl's CuArray as u0\nSolve the same ODE with many u0 and p Use DiffEqGPU.jl's EnsembleGPUArray and EnsembleGPUKernel\n\nThis showcase will focus on the latter case. For the former, see the massively parallel GPU ODE solving showcase.","category":"section"},{"location":"showcase/massively_parallel_gpu/#Supported-GPUs","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Supported GPUs","text":"SciML's GPU support extends to a wide array of hardware, including:\n\nGPU Manufacturer GPU Kernel Language Julia Support Package Backend Type\nNVIDIA CUDA CUDA.jl CUDA.CUDABackend()\nAMD ROCm AMDGPU.jl AMDGPU.ROCBackend()\nIntel OneAPI OneAPI.jl oneAPI.oneAPIBackend()\nApple (M-Series) Metal Metal.jl Metal.MetalBackend()\n\nFor this tutorial we will demonstrate the CUDA backend for NVIDIA GPUs, though any of the other GPUs can be used by simply swapping out the backend choice.","category":"section"},{"location":"showcase/massively_parallel_gpu/#Problem-Setup","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Problem Setup","text":"Let's say we wanted to quantify the uncertainty in the solution of a differential equation. One simple way to do this would be to a Monte Carlo simulation of the same ODE, randomly jiggling around some parameters according to an uncertainty distribution. We could do that on a CPU, but that's not hip. What's hip are GPUs! GPUs have thousands of cores, so could we make each core of our GPU solve the same ODE, but with different parameters? The ensembling tools of DiffEqGPU.jl solve exactly this issue, and today you will learn how to master the GPUniverse.\n\nLet's dive right in.","category":"section"},{"location":"showcase/massively_parallel_gpu/#Defining-the-Ensemble-Problem-for-CPU","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Defining the Ensemble Problem for CPU","text":"DifferentialEquations.jl has an ensemble interface for solving many ODEs. DiffEqGPU conveniently uses exactly the same interface, so just a change of a few characters is all that's required to change a CPU-parallelized code into a GPU-parallelized code. Given that, let's start with the CPU-parallelized code.\n\nLet's implement the Lorenz equation out-of-place. If you don't know what that means, see the getting started with DifferentialEquations.jl\n\nimport DiffEqGPU\nimport OrdinaryDiffEq as ODE\nimport StaticArrays\nimport CUDA\nfunction lorenz(u, p, t)\n    Ïƒ = p[1]\n    Ï = p[2]\n    Î² = p[3]\n    du1 = Ïƒ * (u[2] - u[1])\n    du2 = u[1] * (Ï - u[3]) - u[2]\n    du3 = u[1] * u[2] - Î² * u[3]\n    return StaticArrays.SVector{3}(du1, du2, du3)\nend\n\nu0 = StaticArrays.@SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = StaticArrays.@SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODE.ODEProblem{false}(lorenz, u0, tspan, p)\n\nNotice we use SVectors, i.e. StaticArrays, in order to define our arrays. This is important for later, since the GPUs will want a fully non-allocating code to build a kernel on.\n\nNow, from this problem, we build an EnsembleProblem as per the DifferentialEquations.jl specification. A prob_func jiggles the parameters and we solve 10_000 trajectories:\n\nprob_func = (prob, i, repeat) -> ODE.remake(prob, p = (StaticArrays.@SVector rand(Float32, 3)) .* p)\nmonteprob = DiffEqGPU.EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)\nsol = ODE.solve(monteprob, ODE.Tsit5(), DiffEqGPU.EnsembleThreads(), trajectories = 10_000, saveat = 1.0f0)","category":"section"},{"location":"showcase/massively_parallel_gpu/#Taking-the-Ensemble-to-the-GPU","page":"Massively Data-Parallel ODE Solving on GPUs","title":"Taking the Ensemble to the GPU","text":"Now uhh, we just change EnsembleThreads() to EnsembleGPUArray()\n\nsol = ODE.solve(monteprob, ODE.Tsit5(), DiffEqGPU.EnsembleGPUArray(CUDA.CUDABackend()),\n    trajectories = 10_000, saveat = 1.0f0)\n\nOr for a more efficient version, EnsembleGPUKernel(). But that requires special solvers, so we also change to GPUTsit5().\n\nsol = ODE.solve(\n    monteprob, DiffEqGPU.GPUTsit5(), DiffEqGPU.EnsembleGPUKernel(CUDA.CUDABackend()), trajectories = 10_000)\n\nOkay, so that was anticlimactic, but that's the point: if it were harder than that, it wouldn't be automatic! Now go check out DiffEqGPU.jl's documentation for more details, that's the end of our show.","category":"section"},{"location":"showcase/ode_types/#ode_types","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","text":"One of the nice things about DifferentialEquations.jl is that it is designed with Julia's type system in mind. What this means is, if you have properly defined a Number type, you can use this number type in DifferentialEquations.jl's algorithms! There's more than a few useful/interesting types that can be used:\n\nJulia Type Name Julia Package Use case\nBigFloat Base Julia Higher precision solutions\nArbFloat ArbNumerics.jl More efficient higher precision solutions\nMeasurement Measurements.jl Uncertainty propagation\nParticles MonteCarloMeasurements.jl Uncertainty propagation\nUnitful Unitful.jl Unit-checked arithmetic\nQuaternion Quaternions.jl Quaternions, duh.\nFun ApproxFun.jl Representing PDEs as ODEs in function spaces\nAbstractOrthoPoly PolyChaos.jl Polynomial Chaos Expansion (PCE) for uncertainty quantification\nNum Symbolics.jl Build symbolic expressions of ODE solution approximations\nTaylor TaylorSeries.jl Build a Taylor series around a solution point\nDual ForwardDiff.jl Perform forward-mode automatic differentiation\nTrackedArray\\TrackedReal ReverseDiff.jl Perform reverse-mode automatic differentiation\n\nand on and on. That's only a subset of types people have effectively used on the SciML tools.\n\nWe will look into the BigFloat, Measurement, and Unitful cases to demonstrate the utility of alternative numerical types.","category":"section"},{"location":"showcase/ode_types/#How-Type-Support-Works-in-DifferentialEquations.jl-/-SciML","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"How Type Support Works in DifferentialEquations.jl / SciML","text":"DifferentialEquations.jl determines the numbers to use in its solvers via the types that are designated by tspan and the initial condition u0 of the problem. It will keep the time values in the same type as tspan, and the solution values in the same type as the initial condition.\n\nnote: Note\nSupport for this feature is restricted to the native algorithms of OrdinaryDiffEq.jl. The other solvers such as Sundials.jl, and ODEInterface.jl are incompatible with some number systems.\n\nwarn: Warn\nAdaptive timestepping requires that the time type is compatible with sqrt and ^ functions. Thus for example, tspan cannot be Int if adaptive timestepping is chosen.\n\nLet's use this feature in some cool ways!","category":"section"},{"location":"showcase/ode_types/#Arbitrary-Precision:-Rationals-and-BigFloats","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Arbitrary Precision: Rationals and BigFloats","text":"Let's solve the linear ODE. First, define an easy way to get ODEProblems for the linear ODE:\n\nimport DifferentialEquations as DE\nf(u, p, t) = p * u\nprob_ode_linear = DE.ODEProblem(f, 1 / 2, (0.0, 1.0), 1.01);\n\nNext, let's solve it using Float64s. To do so, we just need to set u0 to a Float64 (which is done by the default) and dt should be a float as well.\n\nsol = DE.solve(prob_ode_linear, DE.Tsit5())\n\nNotice that both the times and the solutions were saved as Float64. Let's change the state to use BigFloat values. We do this by changing the u0 to use BigFloats like:\n\nprob_ode_linear_bigu = DE.ODEProblem(f, big(1 / 2), (0.0, 1.0), 1.01);\nsol = DE.solve(prob_ode_linear_bigu, DE.Tsit5())\n\nNow we see that u is in arbitrary precision BigFloats, while t is in Float64. We can then change t to be arbitrary precision BigFloats by changing the types of the tspan like:\n\nprob_ode_linear_big = DE.ODEProblem(f, big(1 / 2), (big(0.0), big(1.0)), 1.01);\nsol = DE.solve(prob_ode_linear_big, DE.Tsit5())\n\nNow let's send it into the bizarre territory. Let's use rational values for everything. Let's start by making the time type Rational. Rationals are incompatible with adaptive time stepping since they do not have an L2 norm (this can be worked around by defining internalnorm, but we will skip that in this tutorial). To account for this, let's turn off adaptivity as well. Thus the following is a valid use of rational time (and parameter):\n\nprob = DE.ODEProblem(f, 1 / 2, (0 // 1, 1 // 1), 101 // 100);\nsol = DE.solve(prob, DE.RK4(), dt = 1 // 2^(6), adaptive = false)\n\nNow let's change the state to use Rational{BigInt}. You will see that we will need to use the arbitrary-sized integers because... well... there's a reason people use floating-point numbers with ODE solvers:\n\nprob = DE.ODEProblem(f, BigInt(1) // BigInt(2), (0 // 1, 1 // 1), 101 // 100);\nsol = DE.solve(prob, DE.RK4(), dt = 1 // 2^(6), adaptive = false)\n\nYeah...\n\nsol[end]\n\nThat's one huge fraction! 0 floating-point error ODE solve achieved.","category":"section"},{"location":"showcase/ode_types/#Unit-Checked-Arithmetic-via-Unitful.jl","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Unit Checked Arithmetic via Unitful.jl","text":"Units and dimensional analysis are standard tools across the sciences for checking the correctness of your equation. However, most ODE solvers only allow for the equation to be in dimensionless form, leaving it up to the user to both convert the equation to a dimensionless form, punch in the equations, and hopefully not make an error along the way.\n\nDifferentialEquations.jl allows for one to use Unitful.jl to have unit-checked arithmetic natively in the solvers. Given the dispatch implementation of the Unitful, this has little overhead because the unit checks occur at compile-time and not at runtime, and thus it does not have a runtime effect unless conversions are required (i.e. converting cm to m), which automatically adds a floating-point operation for the multiplication.\n\nLet's see this in action.","category":"section"},{"location":"showcase/ode_types/#Using-Unitful","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Using Unitful","text":"To use Unitful, you need to have the package installed. Then you can add units to your variables. For example:\n\nimport Unitful\nt = 1.0Unitful.u\"s\"\n\nNotice that t is a variable with units in seconds. If we make another value with seconds, they can add:\n\nt2 = 1.02Unitful.u\"s\"\nt + t2\n\nand they can multiply:\n\nt * t2\n\nYou can even do rational roots:\n\nsqrt(t)\n\nMany operations work. These operations will check to make sure units are correct, and will throw an error for incorrect operations:\n\nt + sqrt(t)","category":"section"},{"location":"showcase/ode_types/#Using-Unitful-with-DifferentialEquations.jl","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Using Unitful with DifferentialEquations.jl","text":"Just like with other number systems, you can choose the units for your numbers by simply specifying the units of the initial condition and the timespan. For example, to solve the linear ODE where the variable has units of Newton's and t is in seconds, we would use:\n\nimport DifferentialEquations as DE\nf(u, p, t) = 0.5 * u\nu0 = 1.5Unitful.u\"N\"\nprob = DE.ODEProblem(f, u0, (0.0Unitful.u\"s\", 1.0Unitful.u\"s\"))\n#sol = DE.solve(prob,DE.Tsit5())\n\nNotice that we received a unit mismatch error. This is correctly so! Remember that for an ODE:\n\nfracdydt = f(ty)\n\nwe must have that f is a rate, i.e. f is a change in y per unit time. So, we need to fix the units of f in our example to be N/s. Notice that we then do not receive an error if we do the following:\n\nf(y, p, t) = 0.5 * y / 3.0Unitful.u\"s\"\nprob = DE.ODEProblem(f, u0, (0.0Unitful.u\"s\", 1.0Unitful.u\"s\"))\nsol = DE.solve(prob, DE.Tsit5())\n\nThis gives a normal solution object. Notice that the values are all with the correct units:\n\nprint(sol[:])\n\nAnd when we plot the solution, it automatically adds the units:\n\nimport Plots\nPlots.gr()\nPlots.Plots.plot(sol, lw = 3)","category":"section"},{"location":"showcase/ode_types/#Measurements.jl:-Numbers-with-Linear-Uncertainty-Propagation","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Measurements.jl: Numbers with Linear Uncertainty Propagation","text":"The result of a measurement should be given as a number with an attached uncertainty, besides the physical unit, and all operations performed involving the result of the measurement should propagate the uncertainty, taking care of correlation between quantities.\n\nThere is a Julia package for dealing with numbers with uncertainties: Measurements.jl. Thanks to Julia's features, DifferentialEquations.jl easily works together with Measurements.jl out-of-the-box.\n\nLet's try to automate uncertainty propagation through number types on some classical physics examples!","category":"section"},{"location":"showcase/ode_types/#Warning-about-Measurement-type","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Warning about Measurement type","text":"Before going on with the tutorial, we must point up a subtlety of Measurements.jl that you should be aware of:\n\nusing Measurements: Â±\n5.23 Â± 0.14 === 5.23 Â± 0.14\n\n(5.23 Â± 0.14) - (5.23 Â± 0.14)\n\n(5.23 Â± 0.14) / (5.23 Â± 0.14)\n\nThe two numbers above, even though have the same nominal value and the same uncertainties, are actually two different measurements that only by chance share the same figures and their difference and their ratio have a non-zero uncertainty.  It is common in physics to get very similar, or even equal, results for a repeated measurement, but the two measurements are not the same thing.\n\nInstead, if you have one measurement and want to perform some operations involving it, you have to assign it to a variable:\n\nx = 5.23 Â± 0.14\nx === x\n\nx - x\n\nx / x\n\nWith that in mind, let's start by importing Measurements.jl for realsies.","category":"section"},{"location":"showcase/ode_types/#Automated-UQ-on-an-ODE:-Radioactive-Decay-of-Carbon-14","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Automated UQ on an ODE: Radioactive Decay of Carbon-14","text":"The rate of decay of carbon-14 is governed by a first order linear ordinary differential equation:\n\nfracmathrmdu(t)mathrmdt = -fracu(t)tau\n\nwhere tau is the mean lifetime of carbon-14, which is related to the half-life t_12 = (5730 pm 40) years by the relation tau = t_12ln(2). Writing this in DifferentialEquations.jl syntax, this looks like:\n\n# Half-life and mean lifetime of radiocarbon, in years\nt_12 = 5730 Â± 40\nÏ„ = t_12 / log(2)\n\n#Setup\nuâ‚€ = 1 Â± 0\ntspan = (0.0, 10000.0)\n\n#Define the problem\nradioactivedecay(u, p, t) = -u / Ï„\n\n#Pass to solver\nprob = DE.ODEProblem(radioactivedecay, uâ‚€, tspan)\nsol = DE.solve(prob, DE.Tsit5(), reltol = 1e-8)\n\nAnd bingo: numbers with uncertainty went in, so numbers with uncertainty came out. But can we trust those values for the uncertainty?\n\nWe can check the uncertainty quantification by evaluating an analytical solution to the ODE. Since it's a linear ODE, the analytical solution is simply given by the exponential:\n\nu = exp.(-sol.t / Ï„)\n\nHow do the two solutions compare?\n\nPlots.plot(sol.t, sol.u, label = \"Numerical\", xlabel = \"Years\", ylabel = \"Fraction of Carbon-14\")\nPlots.plot!(sol.t, u, label = \"Analytic\")\n\nThe two curves are perfectly superimposed, indicating that the numerical solution matches the analytic one.  We can check that also the uncertainties are correctly propagated in the numerical solution:\n\nprintln(\"Quantity of carbon-14 after \", sol.t[11], \" years:\")\nprintln(\"Numerical: \", sol[11])\nprintln(\"Analytic:  \", u[11])\n\nBullseye. Both the value of the numerical solution and its uncertainty match the analytic solution within the requested tolerance.  We can also note that close to 5730 years after the beginning of the decay (half-life of the radioisotope), the fraction of carbon-14 that survived is about 0.5.","category":"section"},{"location":"showcase/ode_types/#Simple-pendulum:-Small-angles-approximation","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Simple pendulum: Small angles approximation","text":"The next problem we are going to study is the simple pendulum in the approximation of small angles.  We address this simplified case because there exists an easy analytic solution to compare.\n\nThe differential equation we want to solve is:\n\nddottheta + fracgL theta = 0\n\nwhere g = (979 pm 002)mathrmmmathrms^2 is the gravitational acceleration measured where the experiment is carried out, and L = (100 pm 001)mathrmm is the length of the pendulum.\n\nWhen you set up the problem for DifferentialEquations.jl remember to define the measurements as variables, as seen above.\n\nimport DifferentialEquations as DE, Plots\nusing Measurements: Â±\n\ng = 9.79 Â± 0.02; # Gravitational constants\nL = 1.00 Â± 0.01; # Length of the pendulum\n\n#Initial Conditions\nuâ‚€ = [0 Â± 0, Ï€ / 60 Â± 0.01] # Initial speed and initial angle\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du, u, p, t)\n    Î¸ = u[1]\n    dÎ¸ = u[2]\n    du[1] = dÎ¸\n    du[2] = -(g / L) * Î¸\nend\n\n#Pass to solvers\nprob = DE.ODEProblem(simplependulum, uâ‚€, tspan)\nsol = DE.solve(prob, DE.Tsit5(), reltol = 1e-6)\n\nAnd that's it! What about comparing it this time to the analytical solution?\n\nu = uâ‚€[2] .* cos.(sqrt(g / L) .* sol.t)\n\nPlots.plot(sol.t, getindex.(sol.u, 2), label = \"Numerical\")\nPlots.plot!(sol.t, u, label = \"Analytic\")\n\nBingo. Also in this case there is a perfect superimposition between the two curves, including their uncertainties.\n\nWe can also have a look at the difference between the two solutions:\n\nPlots.plot(sol.t, getindex.(sol.u, 2) .- u, label = \"\")\n\nTiny difference on the order of the chosen 1e-6 tolerance.","category":"section"},{"location":"showcase/ode_types/#Simple-pendulum:-Arbitrary-amplitude","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Simple pendulum: Arbitrary amplitude","text":"Now that we know how to solve differential equations involving numbers with uncertainties, we can solve the simple pendulum problem without any approximation. This time, the differential equation to solve is the following:\n\nddottheta + fracgL sin(theta) = 0\n\nThat would be done via:\n\ng = 9.79 Â± 0.02; # Gravitational constants\nL = 1.00 Â± 0.01; # Length of the pendulum\n\n#Initial Conditions\nuâ‚€ = [0 Â± 0, Ï€ / 3 Â± 0.02] # Initial speed and initial angle\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du, u, p, t)\n    Î¸ = u[1]\n    dÎ¸ = u[2]\n    du[1] = dÎ¸\n    du[2] = -(g / L) * sin(Î¸)\nend\n\n#Pass to solvers\nprob = DE.ODEProblem(simplependulum, uâ‚€, tspan)\nsol = DE.solve(prob, DE.Tsit5(), reltol = 1e-6)\n\nPlots.plot(sol.t, getindex.(sol.u, 2), label = \"Numerical\")","category":"section"},{"location":"showcase/ode_types/#Warning-about-Linear-Uncertainty-Propagation","page":"Automatic Uncertainty Quantification, Arbitrary Precision, and Unit Checking in ODE Solutions using Julia's Type System","title":"Warning about Linear Uncertainty Propagation","text":"Measurements.jl uses linear uncertainty propagation, which has an error associated with it. MonteCarloMeasurements.jl has a page which showcases where this method can lead to incorrect uncertainty measurements. Thus for more nonlinear use cases, it's suggested that one uses one of the more powerful UQ methods, such as:\n\nMonteCarloMeasurements.jl\nPolyChaos.jl\nSciMLExpectations.jl\nThe ProbInts Uncertainty Quantification callbacks\n\nBasically, types can make the algorithm you want to run exceedingly simple to do, but make sure it's the correct algorithm!","category":"section"},{"location":"showcase/pinngpu/#pinngpu","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","text":"Machine learning is all the rage. Everybody thinks physics is cool.\n\nTherefore, using machine learning to solve physics equations? ðŸ§ ðŸ’¥\n\nSo let's be cool and use a physics-informed neural network (PINN) to solve the Heat Equation. Let's be even cooler by using GPUs (ironically, creating even more heat, but it's the heat equation so that's cool).","category":"section"},{"location":"showcase/pinngpu/#Step-1:-Import-Libraries","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 1: Import Libraries","text":"To solve PDEs using neural networks, we will use the NeuralPDE.jl package. This package uses ModelingToolkit's symbolic PDESystem as an input, and it generates an Optimization.jl OptimizationProblem which, when solved, gives the weights of the neural network that solve the PDE. In the end, our neural network NN satisfies the PDE equations and is thus the solution to the PDE! Thus our packages look like:\n\n# High Level Interface\nimport NeuralPDE\nimport ModelingToolkit as MTK\nusing ModelingToolkit: @parameters, @variables, Differential, PDESystem\nusing DomainSets: Interval\n\n# Optimization Libraries\nimport Optimization as OPT\nimport OptimizationOptimisers\n\n# Machine Learning Libraries and Helpers\nimport Lux\nimport LuxCUDA\nimport ComponentArrays\nimport MLDataDevices\nconst gpud = MLDataDevices.gpu_device() # allocate a GPU device\n\n# Standard Libraries\nimport Printf\nimport Random\n\n# Plotting\nimport Plots","category":"section"},{"location":"showcase/pinngpu/#Problem-Setup","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Problem Setup","text":"Let's solve the 2+1-dimensional Heat Equation. This is the PDE:\n\n_t u(x y t) = ^2_x u(x y t) + ^2_y u(x y t)  \n\nwith the initial and boundary conditions:\n\nbeginalign*\nu(x y 0) = e^x+y cos(x + y)       \nu(0 y t) = e^y   cos(y + 4t)      \nu(2 y t) = e^2+y cos(2 + y + 4t)  \nu(x 0 t) = e^x   cos(x + 4t)      \nu(x 2 t) = e^x+2 cos(x + 2 + 4t)  \nendalign*\n\non the space and time domain:\n\nx in 0 2   y in 0 2    t in 0 2  \n\nwith physics-informed neural networks.","category":"section"},{"location":"showcase/pinngpu/#Step-2:-Define-the-PDESystem","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 2: Define the PDESystem","text":"First, let's use ModelingToolkit's PDESystem to represent the PDE. To do this, basically just copy-paste the PDE definition into Julia code. This looks like:\n\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min = 0.0\nt_max = 2.0\nx_min = 0.0\nx_max = 2.0\ny_min = 0.0\ny_max = 2.0\n\n# 2D PDE\neq = Dt(u(t, x, y)) ~ Dxx(u(t, x, y)) + Dyy(u(t, x, y))\n\nanalytic_sol_func(t, x, y) = exp(x + y) * cos(x + y + 4t)\n# Initial and boundary conditions\nbcs = [u(t_min, x, y) ~ analytic_sol_func(t_min, x, y),\n    u(t, x_min, y) ~ analytic_sol_func(t, x_min, y),\n    u(t, x_max, y) ~ analytic_sol_func(t, x_max, y),\n    u(t, x, y_min) ~ analytic_sol_func(t, x, y_min),\n    u(t, x, y_max) ~ analytic_sol_func(t, x, y_max)]\n\n# Space and time domains\ndomains = [t âˆˆ Interval(t_min, t_max),\n    x âˆˆ Interval(x_min, x_max),\n    y âˆˆ Interval(y_min, y_max)]\n\nMTK.@named pde_system = PDESystem(eq, bcs, domains, [t, x, y], [u(t, x, y)])\n\nnote: Note\nWe used the wildcard form of the variable definition @variables u(..) which then requires that we always specify what the dependent variables of u are. This is because in the boundary conditions we change from using u(t,x,y) to more specific points and lines, like u(t,x_max,y).","category":"section"},{"location":"showcase/pinngpu/#Step-3:-Define-the-Lux-Neural-Network","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 3: Define the Lux Neural Network","text":"Now let's define the neural network that will act as our solution. We will use a simple multi-layer perceptron, like:\n\ninner = 25\nchain = Lux.Chain(Lux.Dense(3, inner, Lux.Ïƒ),\n    Lux.Dense(inner, inner, Lux.Ïƒ),\n    Lux.Dense(inner, inner, Lux.Ïƒ),\n    Lux.Dense(inner, inner, Lux.Ïƒ),\n    Lux.Dense(inner, 1))\nps = Lux.setup(Random.default_rng(), chain)[1]","category":"section"},{"location":"showcase/pinngpu/#Step-4:-Place-it-on-the-GPU.","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 4: Place it on the GPU.","text":"Just plop it on that sucker. We must ensure that our initial parameters for the neural network are on the GPU. If that is done, then the internal computations will all take place on the GPU. This is done by using the gpud function (i.e. the GPU device we created at the start) on the initial parameters, like:\n\nps = ps |> ComponentArrays.ComponentArray |> gpud .|> Float64","category":"section"},{"location":"showcase/pinngpu/#Step-5:-Discretize-the-PDE-via-a-PINN-Training-Strategy","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 5: Discretize the PDE via a PINN Training Strategy","text":"strategy = NeuralPDE.GridTraining(0.05)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,\n    strategy,\n    init_params = ps)\nprob = NeuralPDE.discretize(pde_system, discretization)","category":"section"},{"location":"showcase/pinngpu/#Step-6:-Solve-the-Optimization-Problem","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 6: Solve the Optimization Problem","text":"callback = function (state, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = OPT.solve(prob, OptimizationOptimisers.Adam(0.01); callback = callback, maxiters = 2500);\n\nWe then use the remake function to rebuild the PDE problem to start a new optimization at the optimized parameters, and continue with a lower learning rate:\n\nprob = OPT.remake(prob, u0 = res.u)\nres = OPT.solve(prob, OptimizationOptimisers.Adam(0.001); callback = callback, maxiters = 2500);","category":"section"},{"location":"showcase/pinngpu/#Step-7:-Inspect-the-PINN's-Solution","page":"GPU-Accelerated Physics-Informed Neural Network (PINN) PDE Solvers","title":"Step 7: Inspect the PINN's Solution","text":"Finally, we inspect the solution:\n\nphi = discretization.phi\nts, xs, ys = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]\nu_real = [analytic_sol_func(t, x, y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi([t, x, y], res.u))) for t in ts for x in xs for y in ys]\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t, x, y) for x in xs for y in ys],\n            (length(xs), length(ys)))\n        u_predict = reshape([Array(phi([t, x, y], res.u))[1] for x in xs for y in ys],\n            length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict, t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st = :contourf, label = \"\", title = title)\n        plot(p1, p2, p3)\n    end\n    gif(anim, \"3pde.gif\", fps = 10)\nend\n\nplot_(res)\n\n(Image: 3pde)","category":"section"},{"location":"showcase/brusselator/#brusselator","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Automated Efficient Solution of Nonlinear Partial Differential Equations","text":"Solving nonlinear partial differential equations (PDEs) is hard. Solving nonlinear PDEs fast and accurately is even harder. Doing it all in an automated method from just a symbolic description is just plain fun. That's what we'd demonstrate here: how to solve a nonlinear PDE from a purely symbolic definition using the combination of ModelingToolkit, MethodOfLines, and DifferentialEquations.jl.\n\nnote: Note\nThis example is a combination of the Brusselator tutorial from MethodOfLines.jl and the Solving Large Stiff Equations tutorial from DifferentialEquations.jl.","category":"section"},{"location":"showcase/brusselator/#Required-Dependencies","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Required Dependencies","text":"The following parts of the SciML Ecosystem will be used in this tutorial:\n\nModule Description\nModelingToolkit.jl The symbolic modeling environment\nMethodOfLines.jl The symbolic PDE discretization tooling\nDifferentialEquations.jl The numerical differential equation solvers\nLinearSolve.jl The numerical linear solvers","category":"section"},{"location":"showcase/brusselator/#Problem-Setup","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Problem Setup","text":"The Brusselator PDE is defined as follows:\n\nbeginalign\nfracpartial upartial t = 1 + u^2v - 44u + alpha left(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2right) + f(x y t)\nfracpartial vpartial t = 34u - u^2v + alpha left(fracpartial^2 vpartial x^2 + fracpartial^2 vpartial y^2right)\nendalign\n\nwhere\n\nf(x y t) = begincases\n5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11 \n0  quad textelse\nendcases\n\nand the initial conditions are\n\nbeginalign\nu(x y 0) = 22cdot (y(1-y))^32 \nv(x y 0) = 27cdot (x(1-x))^32\nendalign\n\nwith the periodic boundary condition\n\nbeginalign\nu(x+1yt) = u(xyt) \nu(xy+1t) = u(xyt)\nendalign\n\nWe wish to obtain the solution to this PDE on a timespan of t in 0115.","category":"section"},{"location":"showcase/brusselator/#Defining-the-symbolic-PDEsystem-with-ModelingToolkit.jl","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Defining the symbolic PDEsystem with ModelingToolkit.jl","text":"With ModelingToolkit.jl, we first symbolically define the system, see also the docs for PDESystem:\n\nimport ModelingToolkit as MTK\nimport MethodOfLines\nimport OrdinaryDiffEq as ODE\nimport LinearSolve as LS\nimport DomainSets\nusing ModelingToolkit: @named, @parameters, @variables, Differential, Interval, PDESystem\n\n@parameters x y t\n@variables u(..) v(..)\nDt = Differential(t)\nDx = Differential(x)\nDy = Differential(y)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\nâˆ‡Â²(u) = Dxx(u) + Dyy(u)\n\nbrusselator_f(x, y, t) = (((x - 0.3)^2 + (y - 0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.0\n\nx_min = y_min = t_min = 0.0\nx_max = y_max = 1.0\nt_max = 11.5\n\nÎ± = 10.0\n\nu0(x, y, t) = 22(y * (1 - y))^(3 / 2)\nv0(x, y, t) = 27(x * (1 - x))^(3 / 2)\n\neq = [\n    Dt(u(x, y, t)) ~ 1.0 + v(x, y, t) * u(x, y, t)^2 - 4.4 * u(x, y, t) +\n                     Î± * âˆ‡Â²(u(x, y, t)) + brusselator_f(x, y, t),\n    Dt(v(x, y, t)) ~ 3.4 * u(x, y, t) - v(x, y, t) * u(x, y, t)^2 + Î± * âˆ‡Â²(v(x, y, t))]\n\ndomains = [x âˆˆ DomainSets.Interval(x_min, x_max),\n    y âˆˆ DomainSets.Interval(y_min, y_max),\n    t âˆˆ DomainSets.Interval(t_min, t_max)]\n\n# Periodic BCs\nbcs = [u(x, y, 0) ~ u0(x, y, 0),\n    u(0, y, t) ~ u(1, y, t),\n    u(x, 0, t) ~ u(x, 1, t), v(x, y, 0) ~ v0(x, y, 0),\n    v(0, y, t) ~ v(1, y, t),\n    v(x, 0, t) ~ v(x, 1, t)]\n\n@named pdesys = PDESystem(eq, bcs, domains, [x, y, t], [u(x, y, t), v(x, y, t)])\n\nLooks just like the LaTeX description, right? Now let's solve it.","category":"section"},{"location":"showcase/brusselator/#Automated-symbolic-discretization-with-MethodOfLines.jl","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Automated symbolic discretization with MethodOfLines.jl","text":"Next we create the discretization. Here we will use the finite difference method via method of lines. Method of lines is a method of recognizing that a discretization of a partial differential equation transforms it into a new numerical problem. For example:\n\nDiscretization Form Numerical Problem Type\nFinite Difference, Finite Volume, Finite Element, discretizing all variables NonlinearProblem\nFinite Difference, Finite Volume, Finite Element, discretizing all variables except time ODEProblem/DAEProblem\nPhysics-Informed Neural Network OptimizationProblem\nFeynman-Kac Formula SDEProblem\nUniversal Stochastic Differential Equation (High dimensional PDEs) OptimizationProblem inverse problem over SDEProblem\n\nThus the process of solving a PDE is fundamentally about transforming its symbolic form to a standard numerical problem and solving the standard numerical problem using one of the solvers in the SciML ecosystem! Here we will demonstrate one of the most classic methods: the finite difference method. Since the Brusselator is a time-dependent PDE with heavy stiffness in the time-domain, we will leave time undiscretized, which means that we will use the finite difference method in the x and y domains to obtain a representation of the equation at `u_i = u(x_i,y_i)grid point values, obtaining an ODEu_i' = \\ldots that defines how the values at the grid points evolve over time.\n\nTo do this, we use the MOLFiniteDifference construct of MethodOfLines.jl as follows:\n\nN = 32\n\ndx = (x_max - x_min) / N\ndy = (y_max - y_min) / N\n\norder = 2\n\ndiscretization = MethodOfLines.MOLFiniteDifference([x => dx, y => dy], t, approx_order = order,\n    grid_align = MethodOfLines.center_align)\n\nNext, we discretize the system, converting the PDESystem in to an ODEProblem:\n\nprob = MethodOfLines.discretize(pdesys, discretization);","category":"section"},{"location":"showcase/brusselator/#Solving-the-PDE","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Solving the PDE","text":"Now your problem can be solved with an appropriate ODE solver. This is just your standard DifferentialEquations.jl usage, though we'll return to this point in a bit to talk about efficiency:\n\nsol = ODE.solve(prob, ODE.TRBDF2(), saveat = 0.1);","category":"section"},{"location":"showcase/brusselator/#Examining-Results-via-the-Symbolic-Solution-Interface","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Examining Results via the Symbolic Solution Interface","text":"Now that we have solved the ODE representation of the PDE, we have an PDETimeSeriesSolution that wraps an ODESolution, which we can get with sol.original_sol. If we look at the original sol, it represents u_i = ldots at each of the grid points. If you check sol.original_sol.u inside the solution, that's those values... but that's not very helpful. How do you interpret original_sol[1]? How do you interpret original_sol[1,:]?\n\nTo make the handling of such cases a lot simpler, MethodOfLines.jl implements a symbolic interface for the solution object that allows for interpreting the computation through its original representation. For example, if we want to know how to interpret the values of the grid corresponding to the independent variables, we can just index using symbolic variables:\n\ndiscrete_x = sol[x];\ndiscrete_y = sol[y];\ndiscrete_t = sol[t];\n\nWhat this tells us is that, for a solution at a given time point, say original_sol[1] for the solution at the initial time (the initial condition), the value original_sol[1][1] is the solution at the grid point (discrete_x[1], discrete_y[1]). For values that are not the initial time point, original_sol[i] corresponds to the solution at discrete_t[i].\n\nBut we also have two dependent variables, u and v. How do we interpret which of the results correspond to the different dependent variables? This is done by indexing the solution by the dependent variables! For example:\n\nsolu = sol[u(x, y, t)];\nsolv = sol[v(x, y, t)];\n\nThis then gives an array of results for the u and v separately, each dimension corresponding to the discrete form of the independent variables.\n\nUsing this high-level indexing, we can create an animation of the solution of the Brusselator as follows. For u we receive:\n\nimport Plots\nanim = Plots.@animate for k in 1:length(discrete_t)\n    Plots.heatmap(solu[2:end, 2:end, k], title = \"$(discrete_t[k])\") # 2:end since end = 1, periodic condition\nend\nPlots.gif(anim, \"plots/Brusselator2Dsol_u.gif\", fps = 8)\n\n(Image: Brusselator2Dsol_u)\n\nand for v:\n\nanim = Plots.@animate for k in 1:length(discrete_t)\n    Plots.heatmap(solv[2:end, 2:end, k], title = \"$(discrete_t[k])\")\nend\nPlots.gif(anim, \"plots/Brusselator2Dsol_v.gif\", fps = 8)\n\n(Image: Brusselator2Dsol_v)","category":"section"},{"location":"showcase/brusselator/#Improving-the-Solution-Process","page":"Automated Efficient Solution of Nonlinear Partial Differential Equations","title":"Improving the Solution Process","text":"warn: Warn\nThis section was disabled temporarily and will be re-enabled after major improvements with SymbolicUtils v4.\n\nNow, if all we needed was a single solution, then we're done. Budda bing budda boom, we got a solution, we're outta here. But if for example we're solving an inverse problem on a PDE, or we need to bump it up to higher accuracy, then we will need to make sure we solve this puppy more efficiently. So let's dive into how this can be done.\n\nFirst of all, large PDEs generally are stiff and thus require an implicit solver. However, their stiffness is generally governed by a nonlinear system which as a sparse Jacobian. Handling that implicit system with sparsity is key to solving the system efficiently, so let's do that!\n\nIn order to enable such options, we simply need to pass the ModelingToolkit.jl problem construction options to the discretize call. This looks like:\n\n# Analytical Jacobian expression and sparse Jacobian\nprob_sparse = MethodOfLines.discretize(pdesys, discretization; jac = true, sparse = true)\n\nNow when we solve the problem it will be a lot faster. We can use BenchmarkTools.jl to assess this performance difference:\n\nimport BenchmarkTools as BT\nBT.@btime sol = ODE.solve(prob, ODE.TRBDF2(), saveat = 0.1);\n\nBT.@btime sol = ODE.solve(prob_sparse, ODE.TRBDF2(), saveat = 0.1);\n\nBut we can further improve this as well. Instead of just using the default linear solver, we can change this to a Newton-Krylov method by passing in the GMRES method:\n\nBT.@btime sol = ODE.solve(prob_sparse, ODE.TRBDF2(linsolve = LS.KrylovJL_GMRES()), saveat = 0.1);\n\nBut to further improve performance, we can use an iLU preconditioner. This looks like as follows:\n\nimport IncompleteLU\nfunction incompletelu(W, du, u, p, t, newW, Plprev, Prprev, solverdata)\n    if newW === nothing || newW\n        Pl = IncompleteLU.ilu(convert(AbstractMatrix, W), Ï„ = 50.0)\n    else\n        Pl = Plprev\n    end\n    Pl, nothing\nend\n\nBT.@btime ODE.solve(prob_sparse,\n    ODE.TRBDF2(linsolve = LS.KrylovJL_GMRES(), precs = incompletelu, concrete_jac = true),\n    save_everystep = false);\n\nAnd now we're zooming! For more information on these performance improvements, check out the deeper dive in the DifferentialEquations.jl tutorials.\n\nIf you're interested in figuring out what's the fastest current solver for this kind of PDE, check out the Brusselator benchmark in SciMLBenchmarks.jl","category":"section"},{"location":"comparisons/python/#python","page":"Getting Started with Julia's SciML for the Python User","title":"Getting Started with Julia's SciML for the Python User","text":"If you're a Python user who has looked into Julia, you're probably wondering what is the equivalent to SciPy is. And you found it: it's the SciML ecosystem! To a Python developer, SciML is SciPy, but with the high-performance GPU, capabilities of PyTorch, and neural network capabilities, all baked right in. With SciML, there is no â€œseparate worldâ€ of machine learning sublanguages: there is just one cohesive package ecosystem.","category":"section"},{"location":"comparisons/python/#Why-SciML?-High-Level-Workflow-Reasons","page":"Getting Started with Julia's SciML for the Python User","title":"Why SciML? High-Level Workflow Reasons","text":"Performance - The key reason people are moving from SciPy to Julia's SciML in droves is performance. Even simple ODE solvers are much faster!, demonstrating orders of magnitude performance improvements for differential equations, nonlinear solving, optimization, and more. And the performance advantages continue to grow as more complex algorithms are required.\nPackage Management and Versioning - Julia's package manager takes care of dependency management, testing, and continuous delivery in order to make the installation and maintenance process smoother. For package users, this means it's easier to get packages with complex functionality in your hands.\nComposable Library Components - In Python environments, every package feels like a silo. Functions made for one file exchange library cannot easily compose with another. SciML's generic coding with JIT compilation these connections create new optimized code on the fly and allow for a more expansive feature set than can ever be documented. Take new high-precision number types from a package and stick them into a nonlinear solver. Take a package for Intel GPU arrays and stick it into the differential equation solver to use specialized hardware acceleration.\nEasier High-Performance and Parallel Computing - With Julia's ecosystem, CUDA will automatically install of the required binaries and cu(A)*cu(B) is then all that's required to GPU-accelerate large-scale linear algebra. MPI is easy to install and use. Distributed computing through password-less SSH. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to automate the discovery of missing physical laws using neural networks embedded in differentiable simulations? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\n\nIn this plot, SciPy in yellow represents Python's most commonly used solvers:\n\n(Image: )","category":"section"},{"location":"comparisons/python/#Need-Help-Translating-from-Python-to-Julia?","page":"Getting Started with Julia's SciML for the Python User","title":"Need Help Translating from Python to Julia?","text":"The following resources can be particularly helpful when adopting Julia for SciML for the first time:\n\nThe Julia Manual's Noteworthy Differences from Python page\nDouble-check your results with SciPyDiffEq.jl (automatically converts and runs ODE definitions with SciPy's solvers)\nUse PyCall.jl to more incrementally move code to Julia.","category":"section"},{"location":"comparisons/python/#Python-to-Julia-SciML-Functionality-Translations","page":"Getting Started with Julia's SciML for the Python User","title":"Python to Julia SciML Functionality Translations","text":"The following chart will help you get quickly acquainted with Julia's SciML Tools:\n\nWorkflow Element SciML-Supported Julia packages\nMatplotlib Plots, Makie\nscipy.special SpecialFunctions\nscipy.linalg.solve LinearSolve\nscipy.integrate Integrals\nscipy.optimize Optimization\nscipy.optimize.fsolve NonlinearSolve\nscipy.interpolate DataInterpolations\nscipy.fft FFTW\nscipy.linalg Julia's Built-In Linear Algebra\nscipy.sparse SparseArrays, ARPACK\nodeint/solve_ivp DifferentialEquations\nscipy.integrate.solve_bvp Boundary-value problem\nPyTorch Flux, Lux\ngillespy2 Catalyst, JumpProcesses\nscipy.optimize.approx_fprime FiniteDiff\nautograd ForwardDiff*, Enzyme*, DiffEqSensitivity\nStan Turing\nsympy Symbolics","category":"section"},{"location":"comparisons/python/#Why-is-Differentiable-Programming-Important-for-Scientific-Computing?","page":"Getting Started with Julia's SciML for the Python User","title":"Why is Differentiable Programming Important for Scientific Computing?","text":"Check out this blog post that goes into detail on how training neural networks in tandem with simulation improves performance by orders of magnitude. But can't you use analytical adjoint definitions? You can, but there are tricks to mix automatic differentiation into the adjoint definitions for a few orders of magnitude improvement too, as explained in this blog post.\n\nThese facts, along with many others, compose to algorithmic improvements with the implementation improvements, which leads to orders of magnitude improvements!","category":"section"},{"location":"getting_started/first_simulation/#first_sim","page":"Build and run your first simulation with Julia's SciML","title":"Build and run your first simulation with Julia's SciML","text":"In this tutorial, we will build and run our first simulation with SciML!\n\nnote: Note\nThis tutorial assumes that you have already installed Julia on your system. If you have not done so already, please follow the installation tutorial first.\n\nTo build our simulation, we will use the ModelingToolkit system for modeling and simulation. ModelingToolkit is a bit higher level than directly defining code for a differential equation system: it's a symbolic system that will automatically simplify our models, optimize our code, and generate compelling visualizations. Sounds neat? Let's dig in.","category":"section"},{"location":"getting_started/first_simulation/#Required-Dependencies","page":"Build and run your first simulation with Julia's SciML","title":"Required Dependencies","text":"The following parts of the SciML Ecosystem will be used in this tutorial:\n\nModule Description\nModelingToolkit.jl The symbolic modeling environment\nDifferentialEquations.jl The differential equation solvers\nPlots.jl The plotting and visualization package","category":"section"},{"location":"getting_started/first_simulation/#Our-Problem:-Simulate-the-Lotka-Volterra-Predator-Prey-Dynamics","page":"Build and run your first simulation with Julia's SciML","title":"Our Problem: Simulate the Lotka-Volterra Predator-Prey Dynamics","text":"The dynamics of our system are given by the Lotka-Volterra dynamical system: Let x(t) be the number of rabbits in the environment and y(t) be the number of wolves. The equation that defines the evolution of the species is given as follows:\n\nbeginalign\nfracdxdt = alpha x - beta x y\nfracdydt = -gamma y + delta x y\nendalign\n\nwhere alpha beta gamma delta are parameters. Starting from equal numbers of rabbits and wolves, x(0) = 1 and y(0) = 1, we want to simulate this system from time t_0 = 0 to t_f = 10. Luckily, a local guide provided us with some parameters that seem to match the system! These are alpha = 15, beta = 10, gamma = 30, delta = 10. How many rabbits and wolves will there be 10 months from now? And if z = x + y, i.e. the total number of animals at a given time, can we visualize this total number of animals at each time?","category":"section"},{"location":"getting_started/first_simulation/#Solution-as-Copy-Pastable-Code","page":"Build and run your first simulation with Julia's SciML","title":"Solution as Copy-Pastable Code","text":"import DifferentialEquations as DE\nimport ModelingToolkit as MTK\nimport Plots\nimport ModelingToolkit: t_nounits as t, D_nounits as D,\n                        @variables, @parameters, @named, @mtkcompile, mtkcompile\n\n# Define our state variables: state(t) = initial condition\n@variables x(t)=1 y(t)=1 z(t)\n\n# Define our parameters\n@parameters Î±=1.5 Î²=1.0 Î³=3.0 Î´=1.0\n\n# Define the differential equations\neqs = [D(x) ~ Î± * x - Î² * x * y\n       D(y) ~ -Î³ * y + Î´ * x * y\n       z ~ x + y]\n\n# Bring these pieces together into an ODESystem with independent variable t\n@mtkcompile sys = MTK.ODESystem(eqs, t)\n\n# Convert from a symbolic to a numerical problem to simulate\ntspan = (0.0, 10.0)\nprob = DE.ODEProblem(sys, [], tspan)\n\n# Solve the ODE\nsol = DE.solve(prob)\n\n# Plot the solution\np1 = Plots.plot(sol, title = \"Rabbits vs Wolves\")\np2 = Plots.plot(sol, idxs = z, title = \"Total Animals\")\n\nPlots.plot(p1, p2, layout = (2, 1))","category":"section"},{"location":"getting_started/first_simulation/#Step-by-Step-Solution","page":"Build and run your first simulation with Julia's SciML","title":"Step-by-Step Solution","text":"","category":"section"},{"location":"getting_started/first_simulation/#Step-1:-Install-and-Import-the-Required-Packages","page":"Build and run your first simulation with Julia's SciML","title":"Step 1: Install and Import the Required Packages","text":"To do this tutorial, we will need a few components:\n\nModelingToolkit.jl, our modeling environment\nDifferentialEquations.jl, the differential equation solvers\nPlots.jl, our visualization tool\n\nTo start, let's add these packages as demonstrated in the installation tutorial:\n\nusing Pkg\nPkg.add([\"ModelingToolkit\", \"DifferentialEquations\", \"Plots\"])\n\nNow we're ready. Let's load in these packages:\n\nimport DifferentialEquations as DE\nimport ModelingToolkit as MTK\nimport Plots\nimport ModelingToolkit: t_nounits as t, D_nounits as D, @variables, @parameters, @named, @mtkcompile, mtkcompile","category":"section"},{"location":"getting_started/first_simulation/#Step-2:-Define-our-ODE-Equations","page":"Build and run your first simulation with Julia's SciML","title":"Step 2: Define our ODE Equations","text":"Now let's define our ODEs. We use the ModelingToolkit.@variabes statement to declare our variables. We have the independent variable time t, and then define our 3 state variables:\n\n# Define our state variables: state(t) = initial condition\n@variables x(t)=1 y(t)=1 z(t)\n\nNotice here that we use the form state = default, where on the right-hand side the default value of a state is interpreted to be its initial condition. Note that since z will be given by an algebraic equation, we do not need to specify its initial condition.\n\nThis is then done similarly for parameters, where the default value is now the parameter value:\n\n# Define our parameters\n@parameters Î±=1.5 Î²=1.0 Î³=3.0 Î´=1.0\n\nnote: Note\nJulia's text editors like VS Code are compatible with Unicode defined in a LaTeX form. Thus if you write \\alpha into your REPL and then press Tab, it will auto-complete that into the Î± symbol. That can make your code look a lot more like the mathematical expressions!\n\nNext, we define our set of differential equations.\n\nnote: Note\nNote that in ModelingToolkit and Symbolics, ~ is used for equation equality. This is separate from = which is the â€œassignment operatorâ€ in the Julia programming language. For example, x = x + 1 is a valid assignment in a programming language, and it is invalid for that to represent â€œequalityâ€, which is why a separate operator is used!\n\n# Define the differential equations\neqs = [D(x) ~ Î± * x - Î² * x * y\n       D(y) ~ -Î³ * y + Î´ * x * y\n       z ~ x + y]\n\nNotice that in the display, it will automatically generate LaTeX. If one is interested in generating this LaTeX locally, one can simply do:\n\nusing Latexify # add the package first\nlatexify(eqs)","category":"section"},{"location":"getting_started/first_simulation/#Step-3:-Define-the-ODEProblem","page":"Build and run your first simulation with Julia's SciML","title":"Step 3: Define the ODEProblem","text":"Now we bring these pieces together. In ModelingToolkit, we can bring these pieces together to represent an ODESystem with the following:\n\n# Bring these pieces together into an ODESystem with independent variable t\n@mtkcompile sys = MTK.ODESystem(eqs, t)\n\nNotice that in our equations we have an algebraic equation z ~ x + y. This is not a differential equation but an algebraic equation, and thus we call this set of equations a Differential-Algebraic Equation (DAE). The symbolic system of ModelingToolkit can eliminate such equations to return simpler forms to numerically approximate.\n\nNotice that what is returned is an ODESystem, but now with the simplified set of equations. z has been turned into an â€œobservableâ€, i.e. a state that is not computed but can be constructed on-demand. This is one of the ways that SciML reaches its speed: you can have 100,000 equations, but solve only 1,000 to then automatically reconstruct the full set. Here, it's just 3 equations to 2, but as models get more complex, the symbolic system will find ever more clever interactions!\n\nNow that we have simplified our system, let's turn it into a numerical problem to approximate. This is done with the ODEProblem constructor, that transforms it from a symbolic ModelingToolkit representation to a numerical DifferentialEquations representation. We need to tell it the numerical details now:\n\nWhether to override any of the default values for the initial conditions and parameters.\nWhat is the initial time point.\nHow long to integrate it for.\n\nIn this case, we will use the default values for all our variables, so we will pass a blank override []. If for example we did want to change the initial condition of x to 2.0 and Î± to 4.0, we would do [x => 2.0, Î± => 4.0]. Then secondly, we pass a tuple for the time span, (0.0,10.0) meaning start at 0.0 and end at 10.0. This looks like:\n\n# Convert from a symbolic to a numerical problem to simulate\ntspan = (0.0, 10.0)\nprob = DE.ODEProblem(sys, [], tspan)","category":"section"},{"location":"getting_started/first_simulation/#Step-4:-Solve-the-ODE-System","page":"Build and run your first simulation with Julia's SciML","title":"Step 4: Solve the ODE System","text":"Now we solve the ODE system. Julia's SciML solvers have a defaulting system that can automatically determine an appropriate solver for a given system, so we can just tell it to solve:\n\n# Solve the ODE\nsol = DE.solve(prob)","category":"section"},{"location":"getting_started/first_simulation/#Step-5:-Visualize-the-Solution","page":"Build and run your first simulation with Julia's SciML","title":"Step 5: Visualize the Solution","text":"Now let's visualize the solution! Notice that our solution only has two states. If we recall, the simplified system only has two states: z was symbolically eliminated. We can access any of the values, even the eliminated values, using the symbolic variable as the index. For example:\n\nsol[z]\n\nreturns the time series of the observable z at time points corresponding to sol.t. We can use this with the automated plotting functionality. First let's create a plot of x and y over time using plot(sol) which will plot all of the states. Then next, we will explicitly tell it to make a plot with the index being z, i.e. idxs=z.\n\nnote: Note\nNote that one can pass an array of indices as well, so idxs=[x,y,z] would make a plot with all three lines together!\n\n# Plot the solution\np1 = Plots.plot(sol, title = \"Rabbits vs Wolves\")\n\np2 = Plots.plot(sol, idxs = z, title = \"Total Animals\")\n\nFinally, let's make a plot where we merge these two plot elements. To do so, we can take our two plot objects, p1 and p2, and make a plot with both of them. Then we tell Plots to do a layout of (2,1), or 2 rows and 1 columns. Let's see what happens when we bring these together:\n\nPlots.plot(p1, p2, layout = (2, 1))\n\nAnd tada, we have a full analysis of our ecosystem!","category":"section"},{"location":"getting_started/first_simulation/#Bonus-Step:-Emoji-Variables","page":"Build and run your first simulation with Julia's SciML","title":"Bonus Step: Emoji Variables","text":"If you made it this far, then congrats, you get to learn a fun fact! Since Julia code can use Unicode, emojis work for variable names. Here's the simulation using emojis of rabbits and wolves to define the system:\n\nimport DifferentialEquations as DE\nimport ModelingToolkit as MTK\nimport ModelingToolkit: t_nounits as t, D_nounits as D, @variables, @parameters, @named\n@parameters Î±=1.5 Î²=1.0 Î³=3.0 Î´=1.0\n@variables ðŸ°(t)=1.0 ðŸº(t)=1.0\neqs = [D(ðŸ°) ~ Î± * ðŸ° - Î² * ðŸ° * ðŸº,\n    D(ðŸº) ~ -Î³ * ðŸº + Î´ * ðŸ° * ðŸº]\n\n@mtkcompile sys = MTK.ODESystem(eqs, t)\nprob = DE.ODEProblem(sys, [], (0.0, 10.0))\nsol = DE.solve(prob)\n\nNow go make your professor mad that they have to grade a fully emojified code. I'll vouch for you: the documentation told you to do this.","category":"section"},{"location":"highlevels/numerical_utilities/#SciML-Numerical-Utility-Libraries","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#ExponentialUtilities.jl:-Faster-Matrix-Exponentials","page":"SciML Numerical Utility Libraries","title":"ExponentialUtilities.jl: Faster Matrix Exponentials","text":"ExponentialUtilities.jl is a library for efficient computation of matrix exponentials. While Julia has a built-in exp(A) method, ExponentialUtilities.jl offers many features around this to improve performance in scientific contexts, including:\n\nFaster methods for (non-allocating) matrix exponentials via exponential!\nMethods for computing matrix exponential that are generic to number types and arrays (i.e. GPUs)\nMethods for computing Arnoldi iterations on Krylov subspaces\nDirect computation of exp(t*A)*v, i.e. exponentiation of a matrix times a vector, without computing the matrix exponential\nDirect computation of Ï•_m(t*A)*v operations, where Ï•_0(z) = exp(z) and Ï•_(k+1)(z) = (Ï•_k(z) - 1) / z\n\nExponentialUtilities.jl includes complex adaptive time stepping techniques such as KIOPS in order to perform these calculations in a fast and numerically-stable way.","category":"section"},{"location":"highlevels/numerical_utilities/#QuasiMonteCarlo.jl:-Fast-Quasi-Random-Number-Generation","page":"SciML Numerical Utility Libraries","title":"QuasiMonteCarlo.jl: Fast Quasi-Random Number Generation","text":"QuasiMonteCarlo.jl is a library for fast generation of low discrepancy Quasi-Monte Carlo samples, using methods like:\n\nGridSample(dx) where the grid is given by lb:dx[i]:ub in the ith direction.\nUniformSample for uniformly distributed random numbers.\nSobolSample for the Sobol sequence.\nLatinHypercubeSample for a Latin Hypercube.\nLatticeRuleSample for a randomly-shifted rank-1 lattice rule.\nLowDiscrepancySample(base) where base[i] is the base in the ith direction.\nGoldenSample for a Golden Ratio sequence.\nKroneckerSample(alpha, s0) for a Kronecker sequence, where alpha is a length-d vector of irrational numbers (often sqrt(d)) and s0 is a length-d seed vector (often 0).\nSectionSample(x0, sampler) where sampler is any sampler above and x0 is a vector of either NaN for a free dimension or some scalar for a constrained dimension.","category":"section"},{"location":"highlevels/numerical_utilities/#DataInterpolations.jl:-One-Dimensional-Interpolations","page":"SciML Numerical Utility Libraries","title":"DataInterpolations.jl: One-Dimensional Interpolations","text":"DataInterpolations.jl is a library of one-dimensional interpolation schemes which are composable with automatic differentiation and the SciML ecosystem. It includes direct interpolation methods and regression techniques for handling noisy data. Its methods include:\n\nConstantInterpolation(u,t) - A piecewise constant interpolation.\nLinearInterpolation(u,t) - A linear interpolation.\nQuadraticInterpolation(u,t) - A quadratic interpolation.\nLagrangeInterpolation(u,t,n) - A Lagrange interpolation of order n.\nQuadraticSpline(u,t) - A quadratic spline interpolation.\nCubicSpline(u,t) - A cubic spline interpolation.\nBSplineInterpolation(u,t,d,pVec,knotVec) - An interpolation B-spline. This is a B-spline which hits each of the data points. The argument choices are:\nd - degree of B-spline\npVec - Symbol to Parameters Vector, pVec = :Uniform for uniform spaced parameters and pVec = :ArcLen for parameters generated by chord length method.\nknotVec - Symbol to Knot Vector, knotVec = :Uniform for uniform knot vector, knotVec = :Average for average spaced knot vector.\nBSplineApprox(u,t,d,h,pVec,knotVec) - A regression B-spline which smooths the fitting curve. The argument choices are the same as the BSplineInterpolation, with the additional parameter h<length(t) which is the number of control points to use, with smaller h indicating more smoothing.\nCurvefit(u,t,m,p,alg) - An interpolation which is done by fitting a user-given functional form m(t,p) where p is the vector of parameters. The user's input p is an initial value for a least-square fitting, alg is the algorithm choice used to optimize the cost function (sum of squared deviations) via Optim.jl and optimal ps are used in the interpolation.\n\nThese interpolations match the SciML interfaces and have direct support for packages like ModelingToolkit.jl.","category":"section"},{"location":"highlevels/numerical_utilities/#PoissonRandom.jl:-Fast-Poisson-Random-Number-Generation","page":"SciML Numerical Utility Libraries","title":"PoissonRandom.jl: Fast Poisson Random Number Generation","text":"PoissonRandom.jl is just fast Poisson random number generation for Poisson processes, like chemical master equations.","category":"section"},{"location":"highlevels/numerical_utilities/#PreallocationTools.jl:-Write-Non-Allocating-Code-Easier","page":"SciML Numerical Utility Libraries","title":"PreallocationTools.jl: Write Non-Allocating Code Easier","text":"PreallocationTools.jl is a library of tools for writing non-allocating code that interacts well with advanced features like automatic differentiation and symbolics.","category":"section"},{"location":"highlevels/numerical_utilities/#RuntimeGeneratedFunctions.jl:-Efficient-Staged-Programming-in-Julia","page":"SciML Numerical Utility Libraries","title":"RuntimeGeneratedFunctions.jl: Efficient Staged Programming in Julia","text":"RuntimeGeneratedFunctions.jl allows for staged programming in Julia, compiling functions at runtime with full optimizations. This is used by many libraries such as ModelingToolkit.jl to allow for runtime code generation for improved performance.","category":"section"},{"location":"highlevels/numerical_utilities/#EllipsisNotation.jl:-Implementation-of-Ellipsis-Array-Slicing","page":"SciML Numerical Utility Libraries","title":"EllipsisNotation.jl: Implementation of Ellipsis Array Slicing","text":"EllipsisNotation.jl defines the ellipsis array slicing notation for Julia. It uses .. as a catch-all for â€œall dimensionsâ€, allowing for indexing like [..,1] to mean [:,:,:,1] on four dimensional arrays, in a way that is generic to the number of dimensions in the underlying array.","category":"section"},{"location":"highlevels/numerical_utilities/#Third-Party-Libraries-to-Note","page":"SciML Numerical Utility Libraries","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#Distributions.jl:-Representations-of-Probability-Distributions","page":"SciML Numerical Utility Libraries","title":"Distributions.jl: Representations of Probability Distributions","text":"Distributions.jl is a library for defining distributions in Julia. It's used all throughout the SciML libraries for specifications of probability distributions.\n\nnote: Note\nFor full compatibility with automatic differentiation, see DistributionsAD.jl","category":"section"},{"location":"highlevels/numerical_utilities/#FFTW.jl:-Fastest-Fourier-Transformation-in-the-West","page":"SciML Numerical Utility Libraries","title":"FFTW.jl: Fastest Fourier Transformation in the West","text":"FFTW.jl is the preferred library for fast Fourier Transformations on the CPU.","category":"section"},{"location":"highlevels/numerical_utilities/#SpecialFunctions.jl:-Implementations-of-Mathematical-Special-Functions","page":"SciML Numerical Utility Libraries","title":"SpecialFunctions.jl: Implementations of Mathematical Special Functions","text":"SpecialFunctions.jl is a library of implementations of special functions, like Bessel functions and error functions (erf). This library is compatible with automatic differentiation.","category":"section"},{"location":"highlevels/numerical_utilities/#LoopVectorization.jl:-Automated-Loop-Accelerator","page":"SciML Numerical Utility Libraries","title":"LoopVectorization.jl: Automated Loop Accelerator","text":"LoopVectorization.jl is a library which provides the @turbo and @tturbo macros for accelerating the computation of loops. This can be used to accelerating the model functions sent to the equation solvers, for example, accelerating handwritten PDE discretizations.","category":"section"},{"location":"highlevels/numerical_utilities/#Polyester.jl:-Cheap-Threads","page":"SciML Numerical Utility Libraries","title":"Polyester.jl: Cheap Threads","text":"Polyester.jl is a cheaper version of threads for Julia, which use a set pool of threads for lower overhead. Note that Polyester does not compose with the standard Julia composable threading infrastructure, and thus one must take care not to compose two levels of Polyester, as this will oversubscribe the computation and lead to performance degradation. Many SciML solvers have options to use Polyester for threading to achieve the top performance.","category":"section"},{"location":"highlevels/numerical_utilities/#Tullio.jl:-Fast-Tensor-Calculations-and-Einstein-Notation","page":"SciML Numerical Utility Libraries","title":"Tullio.jl: Fast Tensor Calculations and Einstein Notation","text":"Tullio.jl is a library for fast tensor calculations with Einstein notation. It allows for defining operations which are compatible with automatic differentiation, GPUs, and more.","category":"section"},{"location":"highlevels/numerical_utilities/#ParallelStencil.jl:-High-Level-Code-for-Parallelized-Stencil-Computations","page":"SciML Numerical Utility Libraries","title":"ParallelStencil.jl: High-Level Code for Parallelized Stencil Computations","text":"ParallelStencil.jl is a library for writing high-level code for parallelized stencil computations. It is compatible with SciML equation solvers and is thus a good way to generate GPU and distributed parallel model code.","category":"section"},{"location":"highlevels/numerical_utilities/#Julia-Utilities","page":"SciML Numerical Utility Libraries","title":"Julia Utilities","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#StaticCompiler.jl","page":"SciML Numerical Utility Libraries","title":"StaticCompiler.jl","text":"StaticCompiler.jl is a package for generating static binaries from Julia code. It only supports a subset of Julia, so not all equation solver algorithms are compatible with StaticCompiler.jl.","category":"section"},{"location":"highlevels/numerical_utilities/#PackageCompiler.jl","page":"SciML Numerical Utility Libraries","title":"PackageCompiler.jl","text":"PackageCompiler.jl is a package for generating shared libraries from Julia code. It builds the entirety of Julia by bundling a system image with the Julia runtime. It thus builds complete binaries that can hold all the functionality of SciML. Furthermore, it can also be used to generate new system images to decrease startup times and remove JIT-compilation from SciML usage.","category":"section"},{"location":"highlevels/developer_documentation/#Developer-Documentation","page":"Developer Documentation","title":"Developer Documentation","text":"For uniformity and clarity, the SciML Open-Source Software Organization has many well-defined rules and practices for its development. However, we stress one important principle:\n\nDo not be deterred from contributing if you think you do not know everything. No one knows everything. These rules and styles are designed for iterative contributions. Open pull requests and contribute what you can with what you know, and the maintainers will help you learn and do the rest!\n\nIf you need any help contributing, please feel welcome joining our community channels.\n\nThe diffeq-bridged and sciml-bridged channels in the Julia Zulip Chat\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nOn the Julia Discourse forums\nSee also SciML Community page\n\nWe welcome everybody.","category":"section"},{"location":"highlevels/developer_documentation/#Getting-Started-With-Contributing-to-SciML","page":"Developer Documentation","title":"Getting Started With Contributing to SciML","text":"To get started contributing to SciML, check out the following resources:\n\nDeveloping Julia Packages\nGetting Started with Julia (for Experienced Programmers)","category":"section"},{"location":"highlevels/developer_documentation/#SciMLStyle:-The-SciML-Style-Guide-for-Julia","page":"Developer Documentation","title":"SciMLStyle: The SciML Style Guide for Julia","text":"(Image: SciML Code Style)\n\nThis is a style guide for how to program in Julia for SciML contributions. It describes everything one needs to know, from preferred naming schemes of functions to fundamental dogmas for designing traits. We stress that this style guide is meant to be comprehensive for the sake of designing automatic formatters and teaching desired rules, but complete knowledge and adherence to the style guide is not required for contributions!","category":"section"},{"location":"highlevels/developer_documentation/#COLPRAC:-Contributor's-Guide-on-Collaborative-Practices-for-Community-Packages","page":"Developer Documentation","title":"COLPRAC: Contributor's Guide on Collaborative Practices for Community Packages","text":"(Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages)\n\nWhat are the rules for when PRs should be merged? What are the rules for whether to tag a major, minor, or patch release? All of these development rules are defined in COLPRAC.","category":"section"},{"location":"highlevels/developer_documentation/#DiffEq-Developer-Documentation","page":"Developer Documentation","title":"DiffEq Developer Documentation","text":"There are many solver libraries which share similar internals, such as OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl. This section of the documentation describes the internal systems of these packages and how they are used to quickly write efficient solvers.","category":"section"},{"location":"highlevels/developer_documentation/#Third-Party-Libraries-to-Note","page":"Developer Documentation","title":"Third-Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/developer_documentation/#Documenter.jl","page":"Developer Documentation","title":"Documenter.jl","text":"Documenter.jl is the documentation generation library that the SciML organization uses, and thus its documentation is the documentation of the documentation.","category":"section"},{"location":"highlevels/developer_documentation/#JuliaFormatter.jl","page":"Developer Documentation","title":"JuliaFormatter.jl","text":"JuliaFormatter.jl is the formatter used by the SciML organization to enforce the SciML Style. Setting style = \"sciml\" in a .JuliaFormatter.toml file of a repo and using the standard FormatCheck.yml as part of continuous integration makes JuliaFormatter check for SciML Style compliance on pull requests.\n\nTo run JuliaFormatter in a SciML repository, do:\n\nimport JuliaFormatter, DevedPackage\nJuliaFormatter.format(pkgdir(DevedPackage))\n\nwhich will reformat the code according to the SciML Style.","category":"section"},{"location":"highlevels/developer_documentation/#GitHub-Actions-Continuous-Integrations","page":"Developer Documentation","title":"GitHub Actions Continuous Integrations","text":"The SciML Organization uses continuous integration testing to always ensure tests are passing when merging pull requests. The organization uses the GitHub Actions supplied by Julia Actions to accomplish this. Common continuous integration scripts are:\n\nCI.yml, the standard CI script\nDownstream.yml, used to specify packages for downstream testing. This will make packages which depend on the current package also be tested to ensure that â€œnon-breaking changesâ€ do not actually break other packages.\nDocumentation.yml, used to run the documentation automatic generation with Documenter.jl\nFormatCheck.yml, used to check JuliaFormatter SciML Style compliance","category":"section"},{"location":"highlevels/developer_documentation/#CompatHelper","page":"Developer Documentation","title":"CompatHelper","text":"CompatHelper is used to automatically create pull requests whenever a dependent package is upper bounded. The results of CompatHelper PRs should be checked to ensure that the latest version of the dependencies are grabbed for the test process. After successful CompatHelper PRs, i.e. if the increase of the upper bound did not cause a break to the tests, a new version tag should follow. It is set up by adding the CompatHelper.yml GitHub action.","category":"section"},{"location":"highlevels/developer_documentation/#TagBot","page":"Developer Documentation","title":"TagBot","text":"TagBot automatically creates tags in the GitHub repository whenever a package is registered to the Julia General repository. It is set up by adding the TagBot.yml GitHub action.","category":"section"},{"location":"highlevels/modeling_languages/#Modeling-Languages","page":"Modeling Languages","title":"Modeling Languages","text":"While in theory one can build perfect code for all models from scratch, in practice many scientists and engineers need or want some help! The SciML modeling tools provide a higher level interface over the equation solver, which helps the translation from good models to good simulations in a way that abstracts away the mathematical and computational details without giving up performance.","category":"section"},{"location":"highlevels/modeling_languages/#ModelingToolkit.jl:-Acausal-Symbolic-Modeling","page":"Modeling Languages","title":"ModelingToolkit.jl: Acausal Symbolic Modeling","text":"Acausal modeling is an extension of causal modeling that is more composable and allows for more code reuse. Build a model of an electric engine, then build a model of a battery, and now declare connections by stating \"the voltage at the engine equals the voltage at the connector of the battery\", and generate the composed model. The tool for this is ModelingToolkit.jl. ModelingToolkit.jl is a sophisticated symbolic modeling library which allows for specifying these types of large-scale differential equation models in a simple way, abstracting away the computational details. However, its symbolic analysis allows for generating much more performant code for differential-algebraic equations than most users could ever write by hand, with its structural_simplify automatically correcting the model to improve parallelism, numerical stability, and automatically remove variables which it can show are redundant.\n\nModelingToolkit.jl is the base of the SciML symbolic modeling ecosystem, defining the AbstractSystem types, such as ODESystem, SDESystem, OptimizationSystem, PDESystem, and more, which are then used by all the other modeling tools. As such, when using other modeling tools like Catalyst.jl, the reference for all the things that can be done with the symbolic representation is simply ModelingToolkit.jl.","category":"section"},{"location":"highlevels/modeling_languages/#Catalyst.jl:-Chemical-Reaction-Networks-(CRN),-Systems-Biology,-and-Quantitative-Systems-Pharmacology-(QSP)-Modeling","page":"Modeling Languages","title":"Catalyst.jl: Chemical Reaction Networks (CRN), Systems Biology, and Quantitative Systems Pharmacology (QSP) Modeling","text":"Catalyst.jl is a modeling interface for efficient simulation of mass action ODE, chemical Langevin SDE, and stochastic chemical kinetics jump process (i.e. chemical master equation) models for chemical reaction networks and population processes. It uses a highly intuitive chemical reaction syntax interface, which generates all the extra functionality necessary for the fastest use with JumpProcesses.jl, DifferentialEquations.jl, and higher level SciML libraries. Its ReactionSystem type is a programmable extension of the ModelingToolkit AbstractSystem interface, meaning that complex reaction systems are represented symbolically, and then compiled to optimized representations automatically when converting ReactionSystems to concrete ODE/SDE/jump process representations. Catalyst also provides functionality to support chemical reaction network and steady-state analysis.\n\nFor an overview of the library, see Modeling Biochemical Systems with Catalyst.jl - Samuel Isaacson","category":"section"},{"location":"highlevels/modeling_languages/#NBodySimulator.jl:-A-differentiable-simulator-for-N-body-problems,-including-astrophysical-and-molecular-dynamics","page":"Modeling Languages","title":"NBodySimulator.jl: A differentiable simulator for N-body problems, including astrophysical and molecular dynamics","text":"NBodySimulator.jl is a differentiable simulator for N-body problems, including astrophysical and molecular dynamics. It uses the DifferentialEquations.jl solvers, allowing for one to choose between a large variety of symplectic integration schemes. It implements many of the thermostats required for doing standard molecular dynamics approximations.","category":"section"},{"location":"highlevels/modeling_languages/#DiffEqFinancial.jl:-Financial-models-for-use-in-the-DifferentialEquations-ecosystem","page":"Modeling Languages","title":"DiffEqFinancial.jl: Financial models for use in the DifferentialEquations ecosystem","text":"The goal of DiffEqFinancial.jl is to be a feature-complete set of solvers for the types of problems found in libraries like QuantLib, such as the Heston process or the Black-Scholes model.","category":"section"},{"location":"highlevels/modeling_languages/#ParameterizedFunctions.jl:-Simple-Differential-Equation-Definitions-Made-Easy","page":"Modeling Languages","title":"ParameterizedFunctions.jl: Simple Differential Equation Definitions Made Easy","text":"(Image: )\n\nThis image that went viral is actually runnable code from ParameterizedFunctions.jl. Define equations and models using a very simple high-level syntax and let the code generation tools build symbolic fast Jacobian, gradient, etc. functions for you.","category":"section"},{"location":"highlevels/modeling_languages/#Third-Party-Tools-of-Note","page":"Modeling Languages","title":"Third-Party Tools of Note","text":"","category":"section"},{"location":"highlevels/modeling_languages/#MomentClosure.jl:-Automated-Generation-of-Moment-Closure-Equations","page":"Modeling Languages","title":"MomentClosure.jl: Automated Generation of Moment Closure Equations","text":"MomentClosure.jl is a library for generating the moment closure equations for a given chemical master equation or stochastic differential equation. Thus instead of solving a stochastic model thousands of times to find the mean and variance, this library can generate the deterministic equations for how the mean and variance evolve in order to be solved in a single run. MomentClosure.jl uses Catalyst ReactionSystem and ModelingToolkit SDESystem types as the input for its symbolic generation processes.","category":"section"},{"location":"highlevels/modeling_languages/#Agents.jl:-Agent-Based-Modeling-Framework-in-Julia","page":"Modeling Languages","title":"Agents.jl: Agent-Based Modeling Framework in Julia","text":"If one wants to do agent-based modeling in Julia, Agents.jl is the go-to library. It's fast and flexible, making it a solid foundation for any agent-based model.","category":"section"},{"location":"highlevels/modeling_languages/#Unitful.jl:-A-Julia-package-for-physical-units","page":"Modeling Languages","title":"Unitful.jl: A Julia package for physical units","text":"Supports not only SI units, but also any other unit system. Unitful.jl has minimal run-time penalty of units. Includes facilities for dimensional analysis, and integrates easily with the usual mathematical operations and collections that are defined in Julia.","category":"section"},{"location":"highlevels/modeling_languages/#ReactionMechanismSimulator.jl:-Simulation-and-Analysis-of-Large-Chemical-Reaction-Systems","page":"Modeling Languages","title":"ReactionMechanismSimulator.jl: Simulation and Analysis of Large Chemical Reaction Systems","text":"ReactionMechanismSimulator.jl is a tool for simulating and analyzing large chemical reaction mechanisms. It interfaces with the ReactionMechanismGenerator suite for automatically constructing reaction pathways from chemical components to quickly build realistic models of chemical systems.","category":"section"},{"location":"highlevels/modeling_languages/#FiniteStateProjection.jl:-Direct-Solution-of-Chemical-Master-Equations","page":"Modeling Languages","title":"FiniteStateProjection.jl: Direct Solution of Chemical Master Equations","text":"FiniteStateProjection.jl is a library for finite state projection direct solving of the chemical master equation. It automatically converts the Catalyst ReactionSystem definitions into ModelingToolkit ODESystem representations for the evolution of probability distributions to allow for directly solving the weak form of the stochastic model.","category":"section"},{"location":"highlevels/modeling_languages/#AlgebraicPetri.jl:-Applied-Category-Theory-of-Modeling","page":"Modeling Languages","title":"AlgebraicPetri.jl: Applied Category Theory of Modeling","text":"AlgebraicPetri.jl is a library for automating the intuitive generation of dynamical models using a Category theory-based approach.","category":"section"},{"location":"highlevels/modeling_languages/#QuantumOptics.jl:-Simulating-quantum-systems.","page":"Modeling Languages","title":"QuantumOptics.jl: Simulating quantum systems.","text":"QuantumOptics.jl makes it easy to simulate various kinds of quantum systems. It is inspired by the Quantum Optics Toolbox for MATLAB and the Python framework QuTiP.","category":"section"},{"location":"highlevels/learning_resources/#Curated-Learning,-Teaching,-and-Training-Resources","page":"Curated Learning, Teaching, and Training Resources","title":"Curated Learning, Teaching, and Training Resources","text":"While the SciML documentation is made to be comprehensive, there will always be good alternative resources. The purpose of this section of the documentation is to highlight the alternative resources which can be helpful for learning how to use the SciML Open-Source Software libraries.","category":"section"},{"location":"highlevels/learning_resources/#JuliaCon-and-SciMLCon-Videos","page":"Curated Learning, Teaching, and Training Resources","title":"JuliaCon and SciMLCon Videos","text":"Many tutorials and introductions to packages have been taught through previous JuliaCon/SciMLCon workshops and talks. The following is a curated list of such training videos:\n\nIntro to solving differential equations in Julia\nJuliaCon 2020 | Doing Scientific Machine Learning (SciML) With Julia\nSimulating Big Models in Julia with ModelingToolkit | Workshop | JuliaCon 2021\nStructural Identifiability Tools in Julia: A Tutorial | Ilia Ilmer | SciMLCon 2022\nJuliaCon 2018 | Solving Partial Differential Equations with Julia | Chris Rackauckas","category":"section"},{"location":"highlevels/learning_resources/#SciML-Book:-Parallel-Computing-and-Scientific-Machine-Learning-(SciML):-Methods-and-Applications","page":"Curated Learning, Teaching, and Training Resources","title":"SciML Book: Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications","text":"The book Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications is a compilation of the lecture notes from the MIT Course 18.337J/6.338J: Parallel Computing and Scientific Machine Learning. It contains a walkthrough of many of the methods implemented in the SciML libraries, as well as how to understand much of the functionality at a deeper level. This course was intended for MIT graduate students in engineering, computer science, and mathematics and thus may have a high prerequisite requirement than many other resources.","category":"section"},{"location":"highlevels/learning_resources/#sir-julia:-Various-implementations-of-the-classical-SIR-model-in-Julia","page":"Curated Learning, Teaching, and Training Resources","title":"sir-julia: Various implementations of the classical SIR model in Julia","text":"For those who like to learn by example, the repository sir-julia is a great resource! It showcases how to use the SciML libraries in many different ways to simulate different variations of the classic SIR epidemic model.","category":"section"},{"location":"highlevels/learning_resources/#Other-Books-Featuring-SciML","page":"Curated Learning, Teaching, and Training Resources","title":"Other Books Featuring SciML","text":"Nonlinear Dynamics: A Concise Introduction Interlaced with Code\nNumerical Methods for Scientific Computing: The Definitive Manual for Math Geeks\nFundamentals of Numerical Computation\nStatistics with Julia\nStatistical Rethinking with Julia\nThe Koopman Operator in Systems and Control\nâ€œAll simulations have been performed in Julia, with additional Julia packages: LinearAlgebra.jl, Random.jl, Plots.jl, Lasso.jl, DifferentialEquations.jlâ€","category":"section"},{"location":"showcase/blackhole/#blackhole","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","text":"In this showcase we will demonstrate using Newtonian mechanics as prior known information in a universal differential equation and learning relativistic corrections to the physics via the gravitational waveform.\n\nThis showcase is minimally adapted from Keith et al. 2021.","category":"section"},{"location":"showcase/blackhole/#Starting-Point:-The-Packages-To-Use","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Starting Point: The Packages To Use","text":"There are many packages which are used as part of this showcase. Let's detail what they are and how they are used. For the neural network training:\n\nModule Description\nOrdinaryDiffEq.jl (DifferentialEquations.jl) The numerical differential equation solvers\nSciMLSensitivity.jl The adjoint methods, defines gradients of ODE solvers\nOptimization.jl The optimization library\nOptimizationOptimisers.jl The optimization solver package with Adam\nOptimizationOptimJL.jl The optimization solver package with BFGS\nComponentArrays.jl For the ComponentArray type to match Lux to SciML\n\nFor the symbolic model discovery:\n\nModule Description\nModelingToolkit.jl The symbolic modeling environment\nDataDrivenDiffEq.jl The symbolic regression interface\nDataDrivenSparse.jl The sparse regression symbolic regression solvers\nZygote.jl The automatic differentiation library for fast gradients\n\nJulia standard libraries:\n\nModule Description\nLinearAlgebra Required for the norm function\nStatistics Required for the mean function\n\nAnd external libraries:\n\nModule Description\nLux.jl The deep learning (neural network) framework\nLineSearches.jl Allows for setting a line search for optimization\nDataFrames.jl A nice and easy data handling format\nCSV.jl Import and export of CSV files\nPlots.jl The plotting and visualization library\nStableRNGs.jl Stable random seeding\n\n# SciML Tools\nimport OrdinaryDiffEq as ODE\nimport ModelingToolkit as MTK\nimport DataDrivenDiffEq\nimport SciMLSensitivity as SMS\nimport DataDrivenSparse\nimport Optimization as OPT\nimport OptimizationOptimisers\nimport OptimizationOptimJL\n\n# Standard Libraries\nimport LinearAlgebra\nimport Statistics\n\n# External Libraries\nimport ComponentArrays\nimport Lux\nimport Zygote\nimport Plots\nimport StableRNGs\nimport DataFrames\nimport CSV\nimport LineSearches\nPlots.gr()\n\n# Set a random seed for reproducible behaviour\nrng = StableRNGs.StableRNG(1111)","category":"section"},{"location":"showcase/blackhole/#Problem-Setup","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Problem Setup","text":"For this example we will use the known relativistic relations to generate data matching the expected LIGO gravitational waveforms. Details can be found in  Keith et al. 2021.","category":"section"},{"location":"showcase/blackhole/#Feel-free-to-skip-reading-this-setup-code!","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Feel free to skip reading this setup code!","text":"<details><summary>The setup of the data and the ODEs for the Newtonian ODE model</summary>\n\nimport DelimitedFiles\n\n#=\n    ODE models for orbital mechanics\n=#\n\nfunction NewtonianOrbitModel(u, model_params, t)\n    #=\n        Defines system of odes which describes motion of\n        point like particle with Newtonian physics, uses\n\n        u[1] = Ï‡\n        u[2] = Ï•\n\n        where, p, M, and e are constants\n    =#\n    Ï‡, Ï• = u\n    p, M, e = model_params\n\n    numer = (1 + e * cos(Ï‡))^2\n    denom = M * (p^(3 / 2))\n\n    Ï‡Ì‡ = numer / denom\n    Ï•Ì‡ = numer / denom\n\n    return [Ï‡Ì‡, Ï•Ì‡]\nend\n\nfunction RelativisticOrbitModel(u, model_params, t)\n    #=\n        Defines system of odes which describes motion of\n        point like particle in schwarzschild background, uses\n\n        u[1] = Ï‡\n        u[2] = Ï•\n\n        where, p, M, and e are constants\n    =#\n    Ï‡, Ï• = u\n    p, M, e = model_params\n\n    numer = (p - 2 - 2 * e * cos(Ï‡)) * (1 + e * cos(Ï‡))^2\n    denom = sqrt((p - 2)^2 - 4 * e^2)\n\n    Ï‡Ì‡ = numer * sqrt(p - 6 - 2 * e * cos(Ï‡)) / (M * (p^2) * denom)\n    Ï•Ì‡ = numer / (M * (p^(3 / 2)) * denom)\n\n    return [Ï‡Ì‡, Ï•Ì‡]\nend\n\nfunction AbstractNNOrbitModel(u, model_params, t; NN = nothing, NN_params = nothing)\n    #=\n        Defines system of odes which describes motion of\n        point like particle with Newtonian physics, uses\n\n        u[1] = Ï‡\n        u[2] = Ï•\n\n        where, p, M, and e are constants\n    =#\n    Ï‡, Ï• = u\n    p, M, e = model_params\n\n    if isnothing(NN)\n        nn = [1, 1]\n    else\n        nn = 1 .+ NN([u[1]], NN_params, st)[1]\n    end\n\n    numer = (1 + e * cos(Ï‡))^2\n    denom = M * (p^(3 / 2))\n\n    Ï‡Ì‡ = (numer / denom) * nn[1]\n    Ï•Ì‡ = (numer / denom) * nn[2]\n\n    return [Ï‡Ì‡, Ï•Ì‡]\nend\n\nfunction AbstractNROrbitModel(u, model_params, t;\n        NN_chiphi = nothing, NN_chiphi_params = nothing,\n        NN_pe = nothing, NN_pe_params = nothing)\n    #=\n        Defines system of odes which describes motion of\n        point like particle with Newtonian physics, uses\n\n        u[1] = Ï‡\n        u[2] = Ï•\n        u[3] = p\n        u[4] = e\n\n        q is the mass ratio\n    =#\n    Ï‡, Ï•, p, e = u\n    q = model_params[1]\n    M = 1.0\n\n    if p <= 0\n        println(\"p = \", p)\n    end\n\n    if isnothing(NN_chiphi)\n        nn_chiphi = [1, 1]\n    else\n        nn_chiphi = 1 .+ NN_chiphi(u, NN_chiphi_params, st)\n    end\n\n    if isnothing(NN_pe)\n        nn_pe = [0, 0]\n    else\n        nn_pe = NN_pe(u, NN_pe_params, st)\n    end\n\n    numer = (1 + e * cos(Ï‡))^2\n    denom = M * (abs(p)^(3 / 2))\n\n    Ï‡Ì‡ = (numer / denom) * nn_chiphi[1]\n    Ï•Ì‡ = (numer / denom) * nn_chiphi[2]\n    pÌ‡ = nn_pe[1]\n    eÌ‡ = nn_pe[2]\n\n    return [Ï‡Ì‡, Ï•Ì‡, pÌ‡, eÌ‡]\nend\n\n#=\n    Axiliary functions for orbital mechanics\n=#\n\nfunction soln2orbit(soln, model_params = nothing)\n    #=\n        Performs change of variables:\n        (Ï‡(t),Ï•(t)) â†¦ (x(t),y(t))\n    =#\n    if size(soln, 1) == 2\n        Ï‡ = soln[1, :]\n        Ï• = soln[2, :]\n        if length(model_params) == 3\n            p, M, e = model_params\n        else\n            error(\"model_params must have length 3 when size(soln,2) = 2\")\n        end\n    elseif size(soln, 1) == 4\n        Ï‡ = soln[1, :]\n        Ï• = soln[2, :]\n        p = soln[3, :]\n        e = soln[4, :]\n    else\n        error(\"size(soln,2) must be either 2 or 4\")\n    end\n\n    r = p ./ (1 .+ e .* cos.(Ï‡))\n    x = r .* cos.(Ï•)\n    y = r .* sin.(Ï•)\n\n    orbit = vcat(x', y')\n    return orbit\nend\n\nfunction orbit2tensor(orbit, component, mass = 1.0)\n    #=\n        Construct trace-free moment tensor Î™(t) for orbit from BH orbit (x(t),y(t))\n\n        component defines the Cartesion indices in x,y. For example,\n        I_{22} is the yy component of the moment tensor.\n    =#\n    x = orbit[1, :]\n    y = orbit[2, :]\n\n    Ixx = x .^ 2\n    Iyy = y .^ 2\n    Ixy = x .* y\n    trace = Ixx .+ Iyy\n\n    if component[1] == 1 && component[2] == 1\n        tmp = Ixx .- (1.0 ./ 3.0) .* trace\n    elseif component[1] == 2 && component[2] == 2\n        tmp = Iyy .- (1.0 ./ 3.0) .* trace\n    else\n        tmp = Ixy\n    end\n\n    return mass .* tmp\nend\n\nfunction d_dt(v::AbstractVector, dt)\n    # uses second-order one-sided difference stencils at the endpoints; see https://doi.org/10.1090/S0025-5718-1988-0935077-0\n    a = -3 / 2 * v[1] + 2 * v[2] - 1 / 2 * v[3]\n    b = (v[3:end] .- v[1:(end - 2)]) / 2\n    c = 3 / 2 * v[end] - 2 * v[end - 1] + 1 / 2 * v[end - 2]\n    return [a; b; c] / dt\nend\n\nfunction d2_dt2(v::AbstractVector, dt)\n    # uses second-order one-sided difference stencils at the endpoints; see https://doi.org/10.1090/S0025-5718-1988-0935077-0\n    a = 2 * v[1] - 5 * v[2] + 4 * v[3] - v[4]\n    b = v[1:(end - 2)] .- 2 * v[2:(end - 1)] .+ v[3:end]\n    c = 2 * v[end] - 5 * v[end - 1] + 4 * v[end - 2] - v[end - 3]\n    return [a; b; c] / (dt^2)\nend\n\nfunction h_22_quadrupole_components(dt, orbit, component, mass = 1.0)\n    #=\n        x(t) and y(t) inputs are the trajectory of the orbiting BH.\n\n       WARNING: assuming x and y are on a uniform grid of spacing dt\n        x_index and y_index are 1,2,3 for x, y, and z indices.\n    =#\n\n    mtensor = orbit2tensor(orbit, component, mass)\n    mtensor_ddot = d2_dt2(mtensor, dt)\n\n    # return mtensor\n    return 2 * mtensor_ddot\nend\n\nfunction h_22_quadrupole(dt, orbit, mass = 1.0)\n    h11 = h_22_quadrupole_components(dt, orbit, (1, 1), mass)\n    h22 = h_22_quadrupole_components(dt, orbit, (2, 2), mass)\n    h12 = h_22_quadrupole_components(dt, orbit, (1, 2), mass)\n    return h11, h12, h22\nend\n\nfunction h_22_strain_one_body(dt, orbit)\n    h11, h12, h22 = h_22_quadrupole(dt, orbit)\n\n    hâ‚Š = h11 - h22\n    hâ‚“ = 2.0 * h12\n\n    scaling_const = sqrt(pi / 5)\n    return scaling_const * hâ‚Š, -scaling_const * hâ‚“\nend\n\nfunction h_22_quadrupole_two_body(dt, orbit1, mass1, orbit2, mass2)\n    h11_1, h12_1, h22_1 = h_22_quadrupole(dt, orbit1, mass1)\n    h11_2, h12_2, h22_2 = h_22_quadrupole(dt, orbit2, mass2)\n    h11 = h11_1 + h11_2\n    h12 = h12_1 + h12_2\n    h22 = h22_1 + h22_2\n    return h11, h12, h22\nend\n\nfunction h_22_strain_two_body(dt, orbit1, mass1, orbit2, mass2)\n    # compute (2,2) mode strain from orbits of BH 1 of mass1 and BH2 of mass 2\n\n    @assert abs(mass1 + mass2 - 1.0)<1e-12 \"Masses do not sum to unity\"\n\n    h11, h12, h22 = h_22_quadrupole_two_body(dt, orbit1, mass1, orbit2, mass2)\n\n    hâ‚Š = h11 - h22\n    hâ‚“ = 2.0 * h12\n\n    scaling_const = sqrt(pi / 5)\n    return scaling_const * hâ‚Š, -scaling_const * hâ‚“\nend\n\nfunction one2two(path, m1, m2)\n    #=\n        We need a very crude 2-body path\n\n        Assume the 1-body motion is a newtonian 2-body position vector r = r1 - r2\n        and use Newtonian formulas to get r1, r2\n        (e.g. Theoretical Mechanics of Particles and Continua 4.3)\n    =#\n\n    M = m1 + m2\n    r1 = m2 / M .* path\n    r2 = -m1 / M .* path\n\n    return r1, r2\nend\n\nfunction compute_waveform(dt, soln, mass_ratio, model_params = nothing)\n    @assert mass_ratio<=1.0 \"mass_ratio must be <= 1\"\n    @assert mass_ratio>=0.0 \"mass_ratio must be non-negative\"\n\n    orbit = soln2orbit(soln, model_params)\n    if mass_ratio > 0\n        mass1 = mass_ratio / (1.0 + mass_ratio)\n        mass2 = 1.0 / (1.0 + mass_ratio)\n\n        orbit1, orbit2 = one2two(orbit, mass1, mass2)\n        waveform = h_22_strain_two_body(dt, orbit1, mass1, orbit2, mass2)\n    else\n        waveform = h_22_strain_one_body(dt, orbit)\n    end\n    return waveform\nend\n\nfunction interpolate_time_series(tsteps, tdata, fdata)\n    @assert length(tdata)==length(fdata) \"lengths of tdata and fdata must match\"\n\n    interp_fdata = zeros(length(tsteps))\n    for j in 1:length(tsteps)\n        for i in 1:(length(tdata) - 1)\n            if tdata[i] <= tsteps[j] < tdata[i + 1]\n                weight = (tsteps[j] - tdata[i]) / (tdata[i + 1] - tdata[i])\n                interp_fdata[j] = (1 - weight) * fdata[i] + weight * fdata[i + 1]\n                break\n            end\n        end\n    end\n\n    return interp_fdata\nend\n\nfunction file2waveform(tsteps, filename = \"waveform.txt\")\n\n    # read in file\n    f = open(filename, \"r\")\n    data = readdlm(f)\n    tdata = data[:, 1]\n    wdata = data[:, 2]\n\n    # interpolate data to tsteps\n    waveform = interpolate_time_series(tsteps, tdata, wdata)\n\n    return waveform\nend\n\nfunction file2trajectory(tsteps, filename = \"trajectoryA.txt\")\n\n    # read in file\n    f = open(filename, \"r\")\n    data = readdlm(f)\n    tdata = data[:, 1]\n    xdata = data[:, 2]\n    ydata = data[:, 3]\n\n    # interpolate data to tsteps\n    x = interpolate_time_series(tsteps, tdata, xdata)\n    y = interpolate_time_series(tsteps, tdata, ydata)\n\n    return x, y\nend\n\n</details>","category":"section"},{"location":"showcase/blackhole/#Testing-the-Model-Setup","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Testing the Model Setup","text":"Now let's test the relativistic orbital model. Let's choose a few parameters of interest:\n\nmass_ratio = 0.0         # test particle\nu0 = Float64[pi, 0.0]    # initial conditions\ndatasize = 250\ntspan = (0.0f0, 6.0f4)   # timespace for GW waveform\ntsteps = range(tspan[1], tspan[2], length = datasize)  # time at each timestep\ndt_data = tsteps[2] - tsteps[1]\ndt = 100.0\nmodel_params = [100.0, 1.0, 0.5]; # p, M, e\n\nand demonstrate the gravitational waveform:\n\nprob = ODE.ODEProblem(RelativisticOrbitModel, u0, tspan, model_params)\nsoln = Array(ODE.solve(prob, ODE.RK4(), saveat = tsteps, dt = dt, adaptive = false))\nwaveform = compute_waveform(dt_data, soln, mass_ratio, model_params)[1]\nplt = Plots.plot(tsteps, waveform,\n    markershape = :circle, markeralpha = 0.25,\n    linewidth = 2, alpha = 0.5,\n    label = \"waveform data\", xlabel = \"Time\", ylabel = \"Waveform\")\n\nLooks great!","category":"section"},{"location":"showcase/blackhole/#Automating-the-Discovery-of-Relativistic-Equations-from-Newtonian-Physics","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Automating the Discovery of Relativistic Equations from Newtonian Physics","text":"Now let's learn the relativistic corrections directly from the data. To define the UDE, we will define a Lux neural network and pass it into our Newtonian Physics + Neural Network ODE definition from above:\n\nNN = Lux.Chain((x) -> cos.(x),\n    Lux.Dense(1, 32, cos),\n    Lux.Dense(32, 32, cos),\n    Lux.Dense(32, 2))\np, st = Lux.setup(rng, NN)\nNN_params = ComponentArrays.ComponentArray{Float64}(p)\n\nfunction ODE_model(u, NN_params, t)\n    du = AbstractNNOrbitModel(u, model_params, t, NN = NN, NN_params = NN_params)\n    return du\nend\n\nNext, we can compute the orbital trajectory and gravitational waveform using the neural network with its initial weights.\n\nprob_nn = ODE.ODEProblem(ODE_model, u0, tspan, NN_params)\nsoln_nn = Array(ODE.solve(\n    prob_nn, ODE.RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive = false))\nwaveform_nn = compute_waveform(dt_data, soln_nn, mass_ratio, model_params)[1]\nPlots.plot!(plt, tsteps, waveform_nn,\n    markershape = :circle, markeralpha = 0.25,\n    linewidth = 2, alpha = 0.5,\n    label = \"waveform NN\")\ndisplay(plt)\n\nThis is the model before training.\n\nNext, we define the objective (loss) function to be minimized when training the neural differential equations.\n\nfunction loss(NN_params)\n    first_obs_to_use_for_training = 1\n    last_obs_to_use_for_training = length(waveform)\n    obs_to_use_for_training = first_obs_to_use_for_training:last_obs_to_use_for_training\n\n    pred = Array(ODE.solve(\n        prob_nn, ODE.RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive = false))\n    pred_waveform = compute_waveform(dt_data, pred, mass_ratio, model_params)[1]\n\n    loss = ( sum(abs2, view(waveform,obs_to_use_for_training) .- view(pred_waveform,obs_to_use_for_training) ) )\n    return loss\nend\n\nWe can test the loss function and see that it returns a pair, a scalar loss and an array with the predicted waveform.\n\nloss(NN_params)\n\nWe'll use the following callback to save the history of the loss values.\n\nlosses = []\n\ncallback(state, l; doplot = true) = begin\n    push!(losses, l)\n    #=  Disable plotting as it trains since in docs\n    display(l)\n    waveform = compute_waveform(dt_data, soln, mass_ratio, model_params)[1]\n    # plot current prediction against data\n    plt = plot(tsteps, waveform,\n        markershape=:circle, markeralpha = 0.25,\n        linewidth = 2, alpha = 0.5,\n        label=\"wform data (h22)\", legend=:topleft)\n    plot!(plt, tsteps, pred_waveform,\n        markershape=:circle, markeralpha = 0.25,\n        linewidth = 2, alpha = 0.5,\n        label = \"wform NN\")\n    if doplot\n        display(plot(plt))\n    end\n    # Tell sciml_train to not halt the optimization. If return true, then\n    # optimization stops.\n    =#\n    return false\nend","category":"section"},{"location":"showcase/blackhole/#Running-the-Training","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Running the Training","text":"The next cell initializes the weights of the neural network and then trains the neural network. Training uses the BFGS optimizers.  This seems to give good results because the Newtonian model seems to give a very good initial guess.\n\nNN_params = NN_params .* 0 +\n            Float64(1e-4) * randn(StableRNGs.StableRNG(2031), eltype(NN_params), size(NN_params))\n\nadtype = OPT.AutoZygote()\noptf = OPT.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = OPT.OptimizationProblem(optf, ComponentArrays.ComponentVector{Float64}(NN_params))\nres1 = OPT.solve(\n    optprob, OptimizationOptimisers.Adam(0.001f0), callback = callback, maxiters = 100)\noptprob = OPT.OptimizationProblem(optf, res1.u)\nres2 = OPT.solve(\n    optprob, OptimizationOptimJL.BFGS(initial_stepnorm = 0.01, linesearch = LineSearches.BackTracking()),\n    callback = callback, maxiters = 20)","category":"section"},{"location":"showcase/blackhole/#Result-Analysis","page":"Discovering the Relativistic Corrections to Binary Black Hole Dynamics","title":"Result Analysis","text":"Now, we'll plot the learned solutions of the neural ODE and compare them to our full physical model and the Newtonian model.\n\nreference_solution = ODE.solve(ODE.remake(prob, p = model_params, saveat = tsteps, tspan = tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\n\noptimized_solution = ODE.solve(\n    ODE.remake(prob_nn, p = res2.minimizer, saveat = tsteps, tspan = tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\nNewtonian_prob = ODE.ODEProblem(NewtonianOrbitModel, u0, tspan, model_params)\n\nNewtonian_solution = ODE.solve(\n    ODE.remake(Newtonian_prob, p = model_params, saveat = tsteps, tspan = tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\n\ntrue_orbit = soln2orbit(reference_solution, model_params)\npred_orbit = soln2orbit(optimized_solution, model_params)\nNewt_orbit = soln2orbit(Newtonian_solution, model_params)\n\ntrue_waveform = compute_waveform(dt_data, reference_solution, mass_ratio, model_params)[1]\npred_waveform = compute_waveform(dt_data, optimized_solution, mass_ratio, model_params)[1]\nNewt_waveform = compute_waveform(dt_data, Newtonian_solution, mass_ratio, model_params)[1]\n\ntrue_orbit = soln2orbit(reference_solution, model_params)\npred_orbit = soln2orbit(optimized_solution, model_params)\nNewt_orbit = soln2orbit(Newtonian_solution, model_params)\nplt = Plots.plot(true_orbit[1, :], true_orbit[2, :], linewidth = 2, label = \"truth\")\nPlots.plot!(plt, pred_orbit[1, :], pred_orbit[2, :],\n    linestyle = :dash, linewidth = 2, label = \"prediction\")\nPlots.plot!(plt, Newt_orbit[1, :], Newt_orbit[2, :], linewidth = 2, label = \"Newtonian\")\n\nplt = Plots.plot(tsteps, true_waveform, linewidth = 2, label = \"truth\",\n    xlabel = \"Time\", ylabel = \"Waveform\")\nPlots.plot!(plt, tsteps, pred_waveform, linestyle = :dash, linewidth = 2, label = \"prediction\")\nPlots.plot!(plt, tsteps, Newt_waveform, linewidth = 2, label = \"Newtonian\")\n\nNow we'll do the same, but extrapolating the model out in time.\n\nfactor = 5\n\nextended_tspan = (tspan[1], factor * tspan[2])\nextended_tsteps = range(tspan[1], factor * tspan[2], length = factor * datasize)\nreference_solution = ODE.solve(\n    ODE.remake(prob, p = model_params, saveat = extended_tsteps, tspan = extended_tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\noptimized_solution = ODE.solve(\n    ODE.remake(prob_nn, p = res2.minimizer, saveat = extended_tsteps, tspan = extended_tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\nNewtonian_prob = ODE.ODEProblem(NewtonianOrbitModel, u0, tspan, model_params)\nNewtonian_solution = ODE.solve(\n    ODE.remake(\n        Newtonian_prob, p = model_params, saveat = extended_tsteps, tspan = extended_tspan),\n    ODE.RK4(), dt = dt, adaptive = false)\ntrue_orbit = soln2orbit(reference_solution, model_params)\npred_orbit = soln2orbit(optimized_solution, model_params)\nNewt_orbit = soln2orbit(Newtonian_solution, model_params)\nplt = Plots.plot(true_orbit[1, :], true_orbit[2, :], linewidth = 2, label = \"truth\")\nPlots.plot!(plt, pred_orbit[1, :], pred_orbit[2, :],\n    linestyle = :dash, linewidth = 2, label = \"prediction\")\nPlots.plot!(plt, Newt_orbit[1, :], Newt_orbit[2, :], linewidth = 2, label = \"Newtonian\")\n\ntrue_waveform = compute_waveform(dt_data, reference_solution, mass_ratio, model_params)[1]\npred_waveform = compute_waveform(dt_data, optimized_solution, mass_ratio, model_params)[1]\nNewt_waveform = compute_waveform(dt_data, Newtonian_solution, mass_ratio, model_params)[1]\nplt = Plots.plot(extended_tsteps, true_waveform, linewidth = 2,\n    label = \"truth\", xlabel = \"Time\", ylabel = \"Waveform\")\nPlots.plot!(plt, extended_tsteps, pred_waveform, linestyle = :dash,\n    linewidth = 2, label = \"prediction\")\nPlots.plot!(plt, extended_tsteps, Newt_waveform, linewidth = 2, label = \"Newtonian\")","category":"section"},{"location":"showcase/optimal_data_gathering_for_missing_physics/#srmf","page":"Optimal Data Gathering for Missing Physics","title":"Optimal Data Gathering for Missing Physics","text":"The missing physics showcase teaches how to discover the missing parts of a dynamic model, using universal differential equations (UDE) and symbolic regression (SR).\n\nHigh quality data is needed to ensure the true dynamics are recovered. In this tutorial, we look at an efficient data gathering technique for SciML models, using a bioreactor example. To this end, we will rely on the following packages:\n\nusing Random; Random.seed!(984519674645)\nusing StableRNGs; rng = StableRNG(845652695)\nimport ModelingToolkit as MTK\nimport ModelingToolkit: t_nounits as t, D_nounits as D, @mtkmodel, @mtkcompile, mtkcompile\nusing ModelingToolkit\nimport ModelingToolkitNeuralNets\nimport OrdinaryDiffEqRosenbrock as ODE\nimport SymbolicIndexingInterface\nusing Plots\nimport Optimization as OPT\nimport OptimizationOptimisers as OptOptim\nimport OptimizationBBO as OptBBO\nimport OptimizationNLopt as OptNL\nimport SciMLStructures\nimport SciMLStructures: Tunable\nimport SciMLSensitivity as SMS\nusing Statistics\nusing SymbolicRegression\nusing LuxCore\nusing LuxCore: stateless_apply\nusing Lux\nusing Statistics\nusing DataFrames\nnothing # hide\n\nThe bioreactor consists of 3 states: substrate concentration C_s(t), biomass concentration C_x(t) and volume V(t).\n\nbeginaligned\nfracdC_sdt = -left(fracmu(C_s)y_xs + mright) C_x + fracQ_in(t)V(C_Sin - C_s)\nfracdC_xdt = mu(C_s) C_x - fracQ_in(t)VC_x\nfracdVdt = Q_in(t)\nendaligned\n\nThe substrate is eaten by the biomass, causing the biomass to grow. The rate by which the biomass grows Î¼(t) is an unknown function (missing physics), which must be estimated from experimental data. The rate by which the substrate is consumed Ïƒ(t) is dependent on Î¼(t), trough a yield factor y_xs and a maintenance term m, where are assumed to be known parameters. More substrate can be pumped into the reactor  with pumping speed Q_in(t). This pumped substrate has known concentration C_s_in. The goal is to optimize the control action Q_in(t), such that Î¼(t) can be estimated as precisely as possible. We restrict Q_in(t) to piecewise constant functions. This can be implemented in MTK as:\n\n@mtkmodel Bioreactor begin\n    @constants begin\n        C_s_in = 50.0\n        y_x_s = 0.777\n        m = 0.0\n    end\n    @parameters begin\n        controls[1:length(optimization_state)-1] = optimization_state[2:end], [tunable = false] # optimization_state is defined further below\n        Q_in = optimization_initial, [tunable = false] # similar for optimization state\n    end\n    @variables begin\n        C_s(t) = 1.0\n        C_x(t) = 1.0\n        V(t) = 7.0\n        Î¼(t)\n        Ïƒ(t)\n    end\n    @equations begin\n        Ïƒ ~ Î¼ / y_x_s + m\n        D(C_s) ~ -Ïƒ * C_x + Q_in / V * (C_s_in - C_s)\n        D(C_x) ~ Î¼ * C_x - Q_in / V * C_x\n        D(V) ~ Q_in\n    end\n    @discrete_events begin\n        (t == 1.0) => [Q_in ~ controls[1]]\n        (t == 2.0) => [Q_in ~ controls[2]]\n        (t == 3.0) => [Q_in ~ controls[3]]\n        (t == 4.0) => [Q_in ~ controls[4]]\n        (t == 5.0) => [Q_in ~ controls[5]]\n        (t == 6.0) => [Q_in ~ controls[6]]\n        (t == 7.0) => [Q_in ~ controls[7]]\n        (t == 8.0) => [Q_in ~ controls[8]]\n        (t == 9.0) => [Q_in ~ controls[9]]\n        (t == 10.0) => [Q_in ~ controls[10]]\n        (t == 11.0) => [Q_in ~ controls[11]]\n        (t == 12.0) => [Q_in ~ controls[12]]\n        (t == 13.0) => [Q_in ~ controls[13]]\n        (t == 14.0) => [Q_in ~ controls[14]]\n        (t == 15.0) => [Q_in ~ optimization_initial] # HACK TO GET Q_IN BACK TO ITS ORIGINAL VALUE\n    end\nend\nnothing # hide\n\nThe true value of Î¼(t), which must be recovered is the Monod equation.\n\nbeginequation*\nmu(C_s) = fracmu_maxC_sK_s + C_s\nendequation*\n\nWe thus extend the bioreactor MTK model with this equation:\n\n@mtkmodel TrueBioreactor begin\n    @extend Bioreactor()\n    @parameters begin\n        Î¼_max = 0.421\n        K_s = 0.439*10\n    end\n    @equations begin\n        Î¼ ~ Î¼_max * C_s / (K_s + C_s) \n    end\nend\nnothing # hide\n\nSimilarly, we can extend the bioreactor with a neural network to represent this missing physics.\n\n@mtkmodel UDEBioreactor begin\n    @extend Bioreactor()\n    @structural_parameters begin\n        chain = Lux.Chain(Lux.Dense(1, 5, tanh),\n                          Lux.Dense(5, 5, tanh),\n                          Lux.Dense(5, 1, x->1*sigmoid(x)))\n    end\n    @components begin\n        nn = ModelingToolkitNeuralNets.NeuralNetworkBlock(; n_input=1, n_output=1, chain, rng)\n    end\n    @equations begin\n        nn.outputs[1] ~ Î¼\n        nn.inputs[1] ~ C_s\n    end\nend\nnothing # hide\n\nWe start by gathering some initial data. Because we don't yet know anything about the missing physics, we arbitrarily pick the zero control action. The only state we measure is C_s We also add some noise to the simulated data, to make it more realistic:\n\noptimization_state =  zeros(15)\noptimization_initial = optimization_state[1] # HACK CAN'T GET THIS TO WORK WITHOUT SEPARATE SCALAR\n@mtkcompile true_bioreactor = TrueBioreactor()\nprob = ODE.ODEProblem(true_bioreactor, [], (0.0, 15.0), [], tstops = 0:15, save_everystep=false)\nsol = ODE.solve(prob, ODE.Rodas5P())\n\n@mtkcompile  ude_bioreactor = UDEBioreactor()\nude_prob = ODE.ODEProblem(ude_bioreactor, [], (0.0, 15.0), [], tstops = 0:15, save_everystep=false)\nude_sol = ODE.solve(ude_prob, ODE.Rodas5P())\n\ndata = DataFrame(sol)\ndata = data[1:2:end, :] # HACK TO GET ONLY THE MEASUREMENTS WE NEED; MTK ALWAYS SAVES BEFORE AND AFTER CALLBACK; WITH NO OPTION TO DISABLE\n\nsd_cs = 0.1\ndata[!, \"C_s(t)\"] += sd_cs * randn(size(data, 1))\n\nplts = plot(), plot(), plot(), plot()\nplot!(plts[1], sol, idxs=:C_s, lw=3,c=1)\nplot!(plts[1], ylabel=\"Câ‚›(g/L)\", xlabel=\"t(h)\")\nscatter!(plts[1], data[!, \"timestamp\"], data[!, \"C_s(t)\"]; ms=3,c=1)\nplot!(plts[2], sol, idxs=:C_x, lw=3,c=1)\nplot!(plts[2], ylabel=\"Câ‚“(g/L)\", xlabel=\"t(h)\")\nplot!(plts[3], sol, idxs=:V, ylabel=\"V(L)\", xlabel=\"t(h)\", lw=3, color=:black, ylims=(6.0,8.0))\nC_s_range_plot = 0.0:0.01:50.0\nÎ¼_max = 0.421; K_s = 0.439*10 # TODO extract the  values from the model.\nplot!(plts[4], C_s_range_plot, Î¼_max .* C_s_range_plot ./ (K_s .+ C_s_range_plot), lw=3, c=1)\nplot!(plts[4], ylabel=\"Î¼(1/h)\", xlabel=\"Câ‚›(g/L)\",ylims=(0,0.5))\nplot(plts..., layout = 4, tickfontsize=10, guidefontsize=12, legendfontsize=14, grid=false, legend=false)\n\nNow we can train the neural network to match this data:\n\nfunction loss(x, (probs, get_varss, datas))\n    loss = zero(eltype(x))\n    for i in eachindex(probs)\n        prob = probs[i]\n        get_vars = get_varss[i]\n        data = datas[i]\n        new_p = SciMLStructures.replace(Tunable(), prob.p, x)\n        new_prob = remake(prob, p=new_p, u0=eltype(x).(prob.u0))\n        new_sol = ODE.solve(new_prob, ODE.Rodas5P())\n        for (i, j) in enumerate(1:2:length(new_sol.t)) # HACK TO DEAL WITH DOUBLE SAVE\n            loss += sum(abs2.(get_vars(new_sol, j) .- data[!, \"C_s(t)\"][i]))\n        end\n        if !(SciMLBase.successful_retcode(new_sol))\n            println(\"failed\")\n            return Inf\n        end\n    end\n    loss\nend\nof = OPT.OptimizationFunction{true}(loss, SMS.AutoZygote())\nx0 = reduce(vcat, getindex.((MTK.default_values(ude_bioreactor),), MTK.tunable_parameters(ude_bioreactor)))\nget_vars = SymbolicIndexingInterface.getu(ude_bioreactor, [ude_bioreactor.C_s])\nps = ([ude_prob], [get_vars], [data]);\nop = OPT.OptimizationProblem(of, x0, ps)\nres = OPT.solve(op, OptOptim.LBFGS(), maxiters=1000)\n\nnew_p = SciMLStructures.replace(Tunable(), ude_prob.p, res.u)\nres_prob = remake(ude_prob, p=new_p)\nres_sol = ODE.solve(res_prob, ODE.Rodas5P())\n\nextracted_chain = arguments(equations(ude_bioreactor.nn)[1].rhs)[1]\nT = defaults(ude_bioreactor)[ude_bioreactor.nn.T]\nÎ¼_predicted_plot = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in C_s_range_plot]\nÎ¼_predicted_data = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data[!, \"C_s(t)\"]]\n\nplts = plot(), plot(), plot(), plot()\nplot!(plts[1], sol, idxs=:C_s, lw=3,c=1)\nplot!(plts[1], res_sol, idxs=:C_s, lw=3,c=2)\nplot!(plts[1], ylabel=\"Câ‚›(g/L)\", xlabel=\"t(h)\")\nscatter!(plts[1], data[!, \"timestamp\"], data[!, \"C_s(t)\"]; ms=3,c=1)\nplot!(plts[2], sol, idxs=:C_x, lw=3,c=1)\nplot!(plts[2], res_sol, idxs=:C_x, lw=3,c=2)\nplot!(plts[2], ylabel=\"Câ‚“(g/L)\", xlabel=\"t(h)\")\nplot!(plts[3], sol, idxs=:V, ylabel=\"V(L)\", xlabel=\"t(h)\", lw=3, color=:black, ylims=(6.0,8.0))\nplot!(plts[4], C_s_range_plot, Î¼_max .* C_s_range_plot ./ (K_s .+ C_s_range_plot), lw=3, c=1)\nplot!(plts[4], C_s_range_plot, Î¼_predicted_plot, lw=3, c=2)\nscatter!(plts[4], data[!, \"C_s(t)\"], Î¼_predicted_data, ms=3, c=2)\nplot!(plts[4], ylabel=\"Î¼(1/h)\", xlabel=\"Câ‚›(g/L)\",ylims=(0,0.5))\nplot(plts..., layout = 4, tickfontsize=10, guidefontsize=12, legendfontsize=14, grid=false, legend=false)\n\nOn the above figure we see that the neural network predicts C_s well, except during the final hours of the experiment, where we have multiple positive realizations of the noise in a row. The neural network also predicts Âµ well in the low substrate concentration region, where we have data available. However, the fit is poor at higher substrate concentrations, where we do not have data.\n\nWe continue by making the neural network interpretable using symbolic regression.\n\noptions = SymbolicRegression.Options(\n    unary_operators=(exp, sin, cos),\n    binary_operators=(+, *, /, -),\n    seed=123,\n    deterministic=true,\n    save_to_file=false,\n    defaults=v\"0.24.5\"\n)\nhall_of_fame = equation_search(collect(data[!, \"C_s(t)\"])', Î¼_predicted_data; options, niterations=1000, runtests=false, parallelism=:serial)\n\nNext, we extract the 10 model structures which symbolic regression thinks are best, and predict the system with them.\n\nn_best = 10\nfunction get_model_structures(hall_of_fame, options, n_best)\n    best_models = []\n    best_models_scores = []\n    i = 1\n    round(hall_of_fame.members[i].loss,sigdigits=5)\n    while length(best_models) <= n_best\n        member = hall_of_fame.members[i]\n        rounded_score = round(member.loss, sigdigits=5)\n        if !(rounded_score in best_models_scores)\n            push!(best_models,member)\n            push!(best_models_scores, rounded_score)\n        end\n        i += 1\n    end\n    model_structures = []\n    @syms x\n    for i = 1:n_best\n        eqn = node_to_symbolic(best_models[i].tree, options, varMap=[\"x\"])\n        fi = build_function(eqn, x, expression=Val{false})\n        push!(model_structures, fi)\n    end\n    return model_structures\nend\n\nfunction get_probs_and_caches(model_structures)\n    probs_plausible = Array{Any}(undef, length(model_structures))\n    syms_cache = Array{Any}(undef, length(model_structures))\n    i = 1\n    for i in 1:length(model_structures)\n        @mtkmodel PlausibleBioreactor begin\n            @extend Bioreactor()\n            @equations begin\n                Î¼ ~ model_structures[i](C_s)\n            end\n        end\n        @mtkcompile plausible_bioreactor = PlausibleBioreactor()\n        plausible_prob = ODE.ODEProblem(plausible_bioreactor, [], (0.0, 15.0), [], tstops=0:15, saveat=0:15)\n        probs_plausible[i] = plausible_prob\n\n        callback_controls = plausible_bioreactor.controls\n        initial_control = plausible_bioreactor.Q_in\n\n        syms_cache[i] = (callback_controls, initial_control, plausible_bioreactor.C_s)\n    end\n    probs_plausible, syms_cache\nend\nmodel_structures = get_model_structures(hall_of_fame, options, n_best)\nprobs_plausible, syms_cache = get_probs_and_caches(model_structures)\n\nplts = plot(), plot(), plot(), plot()\nfor i in 1:length(model_structures)\n    plot!(plts[4],  C_s_range_plot, model_structures[i].( C_s_range_plot);c=i+2,lw=1,ls=:dash)\n    plausible_prob = probs_plausible[i]\n    sol_plausible = ODE.solve(plausible_prob, ODE.Rodas5P())\n    # plot!(sol_plausible; label=[\"Câ‚›(g/L)\" \"Câ‚“(g/L)\" \"V(L)\"], xlabel=\"t(h)\", lw=3)\n    plot!(plts[1], sol_plausible, idxs=:C_s, lw=1,ls=:dash,c=i+2)\n    plot!(plts[2], sol_plausible, idxs=:C_x, lw=1,ls=:dash,c=i+2)\nend\nplot!(plts[1], sol, idxs=:C_s, lw=3,c=1)\nplot!(plts[1], res_sol, idxs=:C_s, lw=3,c=2)\nplot!(plts[1], ylabel=\"Câ‚›(g/L)\", xlabel=\"t(h)\")\nscatter!(plts[1], data[!, \"timestamp\"], data[!, \"C_s(t)\"]; ms=3,c=1)\nplot!(plts[2], sol, idxs=:C_x, lw=3,c=1)\nplot!(plts[2], res_sol, idxs=:C_x, lw=3,c=2)\nplot!(plts[2], ylabel=\"Câ‚“(g/L)\", xlabel=\"t(h)\")\nplot!(plts[3], sol, idxs=:V, ylabel=\"V(L)\", xlabel=\"t(h)\", lw=3, color=:black, ylims=(6.0,8.0))\nÎ¼_max = 0.421; K_s = 0.439*10 # TODO extract the  values from the model.\nplot!(plts[4], C_s_range_plot, Î¼_max .* C_s_range_plot ./ (K_s .+ C_s_range_plot), lw=3, c=1)\nplot!(plts[4], C_s_range_plot, Î¼_predicted_plot, lw=3, c=2)\nscatter!(plts[4], data[!, \"C_s(t)\"], Î¼_predicted_data, ms=3, c=2)\nplot!(plts[4], ylabel=\"Î¼(1/h)\", xlabel=\"Câ‚›(g/L)\",ylims=(0,0.5))\nplot(plts..., layout = 4, tickfontsize=10, guidefontsize=12, legendfontsize=14, grid=false, legend=false)\n\nOn the figure, we see that most plausible model structures predict the states C_s and C_x well, similar to the neural network. The plausible model structures also fit mu well in the low C_s region, but not outside this region. One group of the structures predicts that mu keeps increasing as C_s becomes large while another group predicts that mu stays below 01 1mathrmh.\n\nWe now design a second experiment to start discriminating between these plausible model structures, using the following criterion:\n\nbeginequation*\nargmax_bm Q_in frac2(10-2)10sum_i=1^10 sum_j=i+1^10 max_t_k (bm C_s^i(t_k) - bm C_s^j(t_k))^2\nendequation*\n\nIn this equation, C_s^i denotes the predicted substrate concentration for the i'th plausible model structure. The distance between two model structures is scored by the maximal squared difference between the two structures at the measurement times. The criterion then calculates the average distance between all model structures. Collecting measurements where the plausible model structures differ greatly in predictions, will cause at least some of the model structures to become unlikely, and thus cause new model structures to enter the top 10 plausible model structures.\n\nfunction S_criterion(optimization_state, (probs_plausible, syms_cache))\n    n_structures = length(probs_plausible)\n    sols = Array{Any}(undef, n_structures)\n    for i in 1:n_structures\n        plausible_prob = probs_plausible[i]\n        callback_controls, initial_control, C_s = syms_cache[i]\n        plausible_prob.ps[callback_controls] = optimization_state[2:end]\n        plausible_prob.ps[initial_control] = optimization_state[1]\n        sol_plausible = ODE.solve(plausible_prob, ODE.Rodas5P())\n        if !(SciMLBase.successful_retcode(sol_plausible))\n            return 0.0\n        end\n    loss\n        sols[i] = sol_plausible\n    end\n    squared_differences = Float64[]\n    for i in 1:n_structures\n        callback_controls, initial_control, C_s = syms_cache[i]\n        for j in i+1:n_structures\n            push!(squared_differences, maximum((sols[i][C_s] .- sols[j][C_s]) .^ 2))\n        end\n    end\n    ret = -mean(squared_differences)\n    println(ret)\n    return ret\nend\nlb = zeros(15)\nub = 10 * ones(15)\n\ndesign_prob = OPT.OptimizationProblem(S_criterion, optimization_state, (probs_plausible, syms_cache), lb=lb, ub=ub)\ncontrol_pars_opt = OPT.solve(design_prob, OptBBO.BBO_adaptive_de_rand_1_bin_radiuslimited(), maxtime=100.0)\n\noptimization_state = control_pars_opt.u\noptimization_initial = optimization_initial2 = optimization_state[1]\n\nplts = plot(), plot()\nt_pwc = []\npwc = []\nfor i in 0:14\n    push!(t_pwc,i)\n    push!(t_pwc,i+1)\n    push!(pwc,optimization_state[i+1])\n    push!(pwc,optimization_state[i+1])\nend\nplot!(plts[1], t_pwc, pwc, lw=3, color=:black,xlabel=\"t(h)\",ylabel=\"Qin(L/h)\")\nfor i in 1:length(model_structures)\n    plausible_prob = probs_plausible[i]\n    callback_controls, initial_control, C_s = syms_cache[i]\n    plausible_prob.ps[callback_controls] = control_pars_opt[2:end]\n    plausible_prob.ps[initial_control] = control_pars_opt[1]\n    sol_plausible = ODE.solve(plausible_prob, ODE.Rodas5P())\n    plot!(plts[2], sol_plausible, idxs=:C_s, lw=3,ls=:dash,c=i+2)\nend\nplot!(plts[2],xlabel=\"t(h)\",ylabel=\"Câ‚›(g/L)\")\nplot(plts..., layout = (2, 1), tickfontsize=12, guidefontsize=14, legendfontsize=14, grid=false, legend=false)\n\nThe above figure shows that a maximal control action is generally preferred. This causes the two aforementioned groups in the model structures to be easily discriminated from one another.\n\nWe now gather a second dataset and perform the same exercise.\n\n@mtkcompile true_bioreactor2 = TrueBioreactor()\nprob2 = ODE.ODEProblem(true_bioreactor2, [], (0.0, 15.0), [], tstops=0:15, save_everystep=false)\nsol2 = ODE.solve(prob2, ODE.Rodas5P())\n@mtkcompile ude_bioreactor2 = UDEBioreactor()\nude_prob2 = ODE.ODEProblem(ude_bioreactor2, [], (0.0, 15.0), [ude_bioreactor2.Q_in => optimization_initial], tstops=0:15, save_everystep=false)\nude_sol2 = ODE.solve(ude_prob2, ODE.Rodas5P())\nplot(ude_sol2[3,:])\nude_prob_remake = remake(ude_prob, p=ude_prob2.p)\nsol_remake = ODE.solve(ude_prob_remake, ODE.Rodas5P())\nplot(sol_remake[3,:])\nx0 = reduce(vcat, getindex.((MTK.default_values(ude_bioreactor),), MTK.tunable_parameters(ude_bioreactor)))\n\nget_vars2 = SymbolicIndexingInterface.getu(ude_bioreactor2, [ude_bioreactor2.C_s])\n\ndata2 = DataFrame(sol2)\ndata2 = data2[1:2:end, :]\ndata2[!, \"C_s(t)\"] += sd_cs * randn(size(data2, 1))\n\nps = ([ude_prob, ude_prob2], [get_vars, get_vars2], [data, data2]);\nop = OPT.OptimizationProblem(of, x0, ps)\nres = OPT.solve(op, OptNL.NLopt.LN_BOBYQA, maxiters=5_000)\n\nnew_p = SciMLStructures.replace(Tunable(), ude_prob2.p, res.u)\nres_prob = remake(ude_prob2, p=new_p)\ncallback_controls, initial_control, C_s = syms_cache[1]\nres_prob.ps[initial_control] = optimization_initial2\nres_sol = ODE.solve(res_prob, ODE.Rodas5P())\nextracted_chain = arguments(equations(ude_bioreactor2.nn)[1].rhs)[1]\nT = defaults(ude_bioreactor2)[ude_bioreactor2.nn.T]\nÎ¼_predicted_plot2 = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in C_s_range_plot]\n\nÎ¼_predicted_data = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data[!, \"C_s(t)\"]]\nÎ¼_predicted_data2 = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data2[!, \"C_s(t)\"]]\n\ntotal_data = hcat(collect(data[!, \"C_s(t)\"]'), collect(data2[!, \"C_s(t)\"]'))\ntotal_predicted_data =  vcat(Î¼_predicted_data, Î¼_predicted_data2)\nhall_of_fame = equation_search(total_data, total_predicted_data; options, niterations=1000, runtests=false, parallelism=:serial)\nmodel_structures = get_model_structures(hall_of_fame, options, n_best)\nprobs_plausible, syms_cache = get_probs_and_caches(model_structures);\n\nplts = plot(), plot(), plot(), plot()\nfor i in 1:length(model_structures)\n    plot!(plts[4],  C_s_range_plot, model_structures[i].( C_s_range_plot);c=i+2,lw=1,ls=:dash)\n    plausible_prob = probs_plausible[i]\n    sol_plausible = ODE.solve(plausible_prob, ODE.Rodas5P())\n    # plot!(sol_plausible; label=[\"Câ‚›(g/L)\" \"Câ‚“(g/L)\" \"V(L)\"], xlabel=\"t(h)\", lw=3)\n    plot!(plts[1], sol_plausible, idxs=:C_s, lw=1,ls=:dash,c=i+2)\n    plot!(plts[2], sol_plausible, idxs=:C_x, lw=1,ls=:dash,c=i+2)\nend\nplot!(plts[1], sol2, idxs=:C_s, lw=3,c=1)\nplot!(plts[1], res_sol, idxs=:C_s, lw=3,c=2)\nplot!(plts[1], ylabel=\"Câ‚›(g/L)\", xlabel=\"t(h)\")\nscatter!(plts[1], data2[!, \"timestamp\"], data2[!, \"C_s(t)\"]; ms=3,c=1)\nplot!(plts[2], sol2, idxs=:C_x, lw=3,c=1)\nplot!(plts[2], res_sol, idxs=:C_x, lw=3,c=2)\nplot!(plts[2], ylabel=\"Câ‚“(g/L)\", xlabel=\"t(h)\")\nplot!(plts[3], sol2, idxs=:V, ylabel=\"V(L)\", xlabel=\"t(h)\", lw=3, color=:black)\nplot!(plts[4], C_s_range_plot, Î¼_max .* C_s_range_plot ./ (K_s .+ C_s_range_plot), lw=3, c=1)\nplot!(plts[4], C_s_range_plot, Î¼_predicted_plot2, lw=3, c=2)\nscatter!(plts[4], data[!, \"C_s(t)\"], Î¼_predicted_data, ms=3, c=2)\nscatter!(plts[4], data2[!, \"C_s(t)\"], Î¼_predicted_data2, ms=3, c=2)\nplot!(plts[4], ylabel=\"Î¼(1/h)\", xlabel=\"Câ‚›(g/L)\",ylims=(0,0.5))\nplot(plts..., layout = 4, tickfontsize=10, guidefontsize=12, legendfontsize=14, grid=false, legend=false)\n\nThe above shows the data analysis corresponding to this second experiment. Both the UDE and most of the plausible model structures predict the states well,\n\nThe UDE and the plausible model structures also approximate the missing physics mu well in the region where we have gathered data. This means in the regions of low substrate concentration, with data coming primarily from the first experiment, and high substrate concentration, coming from the second experiment. However, we do not have any measurements at substrate concentrations between these two groups. This causes there to be substantial disagreement between the plausible model structures in the medium substrate concentration range.\n\nWe now optimize the controls for a third experiment:\n\nprob = OPT.OptimizationProblem(S_criterion, zeros(15), (probs_plausible, syms_cache), lb=lb, ub=ub)\ncontrol_pars_opt = OPT.solve(prob, OptBBO.BBO_adaptive_de_rand_1_bin_radiuslimited(), maxtime=60.0)\n\noptimization_state = control_pars_opt.u\noptimization_initial = optimization_state[1]\n\nplts = plot(), plot()\nt_pwc = []\npwc = []\nfor i in 0:14\n    push!(t_pwc,i)\n    push!(t_pwc,i+1)\n    push!(pwc,optimization_state[i+1])\n    push!(pwc,optimization_state[i+1])\nend\nplot!(plts[1], t_pwc, pwc, lw=3, color=:black,xlabel=\"t(h)\",ylabel=\"Qin(L/h)\")\nfor i in 1:length(model_structures)\n    plausible_prob = probs_plausible[i]\n    callback_controls, initial_control, C_s = syms_cache[i]\n    plausible_prob.ps[callback_controls] = control_pars_opt[2:end]\n    plausible_prob.ps[initial_control] = control_pars_opt[1]\n    sol_plausible = ODE.solve(plausible_prob, ODE.Rodas5P())\n    plot!(plts[2], sol_plausible, idxs=:C_s, lw=3,ls=:dash,c=i+2)\nend\nplot!(plts[2],xlabel=\"t(h)\",ylabel=\"Câ‚›(g/L)\")\nplot(plts..., layout = (2, 1), tickfontsize=12, guidefontsize=14, legendfontsize=14, grid=false, legend=false)\n\nThe optimal design algorithm is also aware of this uncertainty at the medium concentration range, and aims to remedy this in the next experiment, as can be seen on the above figure. Using the first control action, the bioreactor substrate concentration gets pumped from a low substrate concentration level to a medium level. At this level, there is substantial disagreement between the plausible model structures, leading to substantial disagreement in predicted substrate concentrations. To keep the reactor at the medium substrate concentration range, while the biomass concentration increases rapidly, an increasing amount of substrate has to be pumped into the reactor every hour. This explains the staircase with increasing step heights form of the control function. After the staircase reaches the maximal control value, a zero control is used. Some model structures decrease more rapidly in substrate concentration than others.\n\n@mtkcompile true_bioreactor3 = TrueBioreactor()\nprob3 = ODE.ODEProblem(true_bioreactor3, [], (0.0, 15.0), [], tstops=0:15, save_everystep=false)\nsol3 = ODE.solve(prob3, ODE.Rodas5P())\n@mtkcompile ude_bioreactor3 = UDEBioreactor()\nude_prob3 = ODE.ODEProblem(ude_bioreactor3, [], (0.0, 15.0), tstops=0:15, save_everystep=false)\n\nx0 = reduce(vcat, getindex.((MTK.default_values(ude_bioreactor3),), MTK.tunable_parameters(ude_bioreactor3)))\n\nget_vars3 = SymbolicIndexingInterface.getu(ude_bioreactor3, [ude_bioreactor3.C_s])\n\ndata3 = DataFrame(sol3)\ndata3 = data3[1:2:end, :]\ndata3[!, \"C_s(t)\"] += sd_cs * randn(size(data3, 1))\n\nps = ([ude_prob, ude_prob2, ude_prob3], [get_vars, get_vars2, get_vars3], [data, data2, data3]);\nop = OPT.OptimizationProblem(of, x0, ps)\nres = OPT.solve(op, OptOptim.LBFGS(), maxiters=10_000)\nextracted_chain = arguments(equations(ude_bioreactor3.nn)[1].rhs)[1]\nT = defaults(ude_bioreactor3)[ude_bioreactor3.nn.T]\n\nÎ¼_predicted_data = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data[!, \"C_s(t)\"]]\nÎ¼_predicted_data2 = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data2[!, \"C_s(t)\"]]\nÎ¼_predicted_data3 = [only(stateless_apply(extracted_chain, [C_s], convert(T,res.u))) for C_s in data3[!, \"C_s(t)\"]]\n\ntotal_data = hcat(collect(data[!, \"C_s(t)\"]'), collect(data2[!, \"C_s(t)\"]'), collect(data3[!, \"C_s(t)\"]'))\ntotal_predicted_data =  vcat(Î¼_predicted_data, Î¼_predicted_data2, Î¼_predicted_data3)\nhall_of_fame = equation_search(total_data, total_predicted_data; options, niterations=1000, runtests=false, parallelism=:serial)\nbar(i->hall_of_fame.members[i].loss, 1:10, ylabel=\"loss\", xlabel=\"hall of fame member\", xticks=1:10)\nplot!(tickfontsize=10, guidefontsize=12, legendfontsize=14, grid=false, legend=false)\n\nThe Monod equation (0419  ((x1 + 4300)  x1)) is member 7 of the hall of fame. All hall of fame members before it have visually higher loss, while all the members after it are indiscernible from it. This indicates that it is a good candidate for the true model structure.\n\nSymbolic regression sometimes finds the true model structure in a somewhat unusual form, like with a double division. This is because symbolic regression considers multiplication and division to have the same complexity.\n\nIn this tutorial, we have shown that experimental design can be used to explore the state space of a dynamic system in a thoughtful way, such that missing physics can be recovered in an efficient manner.","category":"section"},{"location":"highlevels/implicit_layers/#Implicit-Layer-Deep-Learning","page":"Implicit Layer Deep Learning","title":"Implicit Layer Deep Learning","text":"Implicit layer deep learning is a field which uses implicit rules, such as differential equations and nonlinear solvers, to define the layers of neural networks. This field has brought the potential to automatically optimize network depth and improve training performance. SciML's differentiable solver ecosystem is specifically designed to accommodate implicit layer methodologies, and provides libraries with pre-built layers for common methods.","category":"section"},{"location":"highlevels/implicit_layers/#DiffEqFlux.jl:-High-Level-Pre-Built-Architectures-for-Implicit-Deep-Learning","page":"Implicit Layer Deep Learning","title":"DiffEqFlux.jl: High Level Pre-Built Architectures for Implicit Deep Learning","text":"DiffEqFlux.jl is a library of pre-built architectures for implicit deep learning, including layer definitions for methods like:\n\nNeural Ordinary Differential Equations (Neural ODEs)\nCollocation-Based Neural ODEs (Neural ODEs without a solver, by far the fastest way!)\nMultiple Shooting Neural Ordinary Differential Equations\nNeural Stochastic Differential Equations (Neural SDEs)\nNeural Differential-Algebraic Equations (Neural DAEs)\nNeural Delay Differential Equations (Neural DDEs)\nAugmented Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nContinuous Normalizing Flows (CNF) and FFJORD","category":"section"},{"location":"highlevels/implicit_layers/#DeepEquilibriumNetworks.jl:-Deep-Equilibrium-Models-Made-Fast","page":"Implicit Layer Deep Learning","title":"DeepEquilibriumNetworks.jl: Deep Equilibrium Models Made Fast","text":"DeepEquilibriumNetworks.jl is a library of optimized layer implementations for Deep Equilibrium Models (DEQs). It uses special training techniques such as implicit-explicit regularization in order to accelerate the convergence over traditional implementations, all while using the optimized and flexible SciML libraries under the hood.","category":"section"},{"location":"getting_started/fit_simulation/#fit_simulation","page":"Fit a simulation to a dataset","title":"Fit a simulation to a dataset","text":"Running simulations is only half of the battle. Many times, in order to make the simulation realistic, you need to fit the simulation to data. The SciML ecosystem has integration with automatic differentiation and adjoint methods to automatically make the fitting process stable and efficient. Let's see this in action.","category":"section"},{"location":"getting_started/fit_simulation/#Required-Dependencies","page":"Fit a simulation to a dataset","title":"Required Dependencies","text":"The following parts of the SciML Ecosystem will be used in this tutorial:\n\nModule Description\nDifferentialEquations.jl The differential equation solvers\nOptimization.jl The numerical optimization package\nOptimizationPolyalgorithms.jl The optimizers we will use\nSciMLSensitivity.jl The connection of the SciML ecosystems to differentiation\n\nAlong with the following general ecosystem packages:\n\nModule Description\nPlots.jl The plotting and visualization package\nForwardDiff.jl The automatic differentiation package","category":"section"},{"location":"getting_started/fit_simulation/#Problem-Setup:-Fitting-Lotka-Volterra-Data","page":"Fit a simulation to a dataset","title":"Problem Setup: Fitting Lotka-Volterra Data","text":"Assume that we know that the dynamics of our system are given by the Lotka-Volterra dynamical system: Let x(t) be the number of rabbits in the environment and y(t) be the number of wolves. This is the same dynamical system as the first tutorial! The equation that defines the evolution of the species is given as follows:\n\nbeginalign\nfracdxdt = alpha x - beta x y\nfracdydt = -gamma y + delta x y\nendalign\n\nwhere alpha beta gamma delta are parameters. Starting from equal numbers of rabbits and wolves, x(0) = 1 and y(0) = 1.\n\nNow, in the first tutorial, we assumed:\n\nLuckily, a local guide provided us with some parameters that seem to match the system!\n\nSadly, magical nymphs do not always show up and give us parameters. Thus in this case, we will need to use Optimization.jl to optimize the model parameters to best fit some experimental data. We are given experimentally observed data of both rabbit and wolf populations over a time span of t_0 = 0 to t_f = 10 at every Delta t = 1. Can we figure out what the parameter values should be directly from the data?","category":"section"},{"location":"getting_started/fit_simulation/#Solution-as-Copy-Pastable-Code","page":"Fit a simulation to a dataset","title":"Solution as Copy-Pastable Code","text":"import DifferentialEquations as DE\nimport Optimization as OPT\nimport OptimizationPolyalgorithms\nimport SciMLSensitivity\nimport ForwardDiff\nimport Plots\n\n# Define experimental data\nt_data = 0:10\nx_data = [1.000 2.773 6.773 0.971 1.886 6.101 1.398 1.335 4.353 3.247 1.034]\ny_data = [1.000 0.259 2.015 1.908 0.323 0.629 3.458 0.508 0.314 4.547 0.906]\nxy_data = vcat(x_data, y_data)\n\n# Plot the provided data\nPlots.scatter(t_data, xy_data', label = [\"x Data\" \"y Data\"])\n\n# Setup the ODE function\nfunction lotka_volterra!(du, u, p, t)\n    x, y = u\n    Î±, Î², Î´, Î³ = p\n    du[1] = dx = Î± * x - Î² * x * y\n    du[2] = dy = -Î´ * y + Î³ * x * y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval\ntspan = (0.0, 10.0)\n\n# LV equation parameter. p = [Î±, Î², Î´, Î³]\npguess = [1.0, 1.2, 2.5, 1.2]\n\n# Set up the ODE problem with our guessed parameter values\nprob = DE.ODEProblem(lotka_volterra!, u0, tspan, pguess)\n\n# Solve the ODE problem with our guessed parameter values\ninitial_sol = DE.solve(prob, saveat = 1)\n\n# View the guessed model solution\nplt = Plots.plot(initial_sol, label = [\"x Prediction\" \"y Prediction\"])\nPlots.scatter!(plt, t_data, xy_data', label = [\"x Data\" \"y Data\"])\n\n# Define a loss metric function to be minimized\nfunction loss(newp)\n    newprob = DE.remake(prob, p = newp)\n    sol = DE.solve(newprob, saveat = 1)\n    loss = sum(abs2, sol .- xy_data)\n    return loss\nend\n\n# Define a callback function to monitor optimization progress\nfunction callback(state, l)\n    display(l)\n    newprob = DE.remake(prob, p = state.u)\n    sol = DE.solve(newprob, saveat = 1)\n    plt = Plots.plot(sol, ylim = (0, 6), label = [\"Current x Prediction\" \"Current y Prediction\"])\n    Plots.scatter!(plt, t_data, xy_data', label = [\"x Data\" \"y Data\"])\n    display(plt)\n    return false\nend\n\n# Set up the optimization problem with our loss function and initial guess\nadtype = OPT.AutoForwardDiff()\npguess = [1.0, 1.2, 2.5, 1.2]\noptf = OPT.OptimizationFunction((x, _) -> loss(x), adtype)\noptprob = OPT.OptimizationProblem(optf, pguess)\n\n# Optimize the ODE parameters for best fit to our data\npfinal = OPT.solve(optprob, OptimizationPolyalgorithms.PolyOpt(),\n    callback = callback,\n    maxiters = 200)\nÎ±, Î², Î³, Î´ = round.(pfinal, digits = 1)","category":"section"},{"location":"getting_started/fit_simulation/#Step-by-Step-Solution","page":"Fit a simulation to a dataset","title":"Step-by-Step Solution","text":"","category":"section"},{"location":"getting_started/fit_simulation/#Step-1:-Install-and-Import-the-Required-Packages","page":"Fit a simulation to a dataset","title":"Step 1: Install and Import the Required Packages","text":"To do this tutorial, we will need a few components. This is done using the Julia Pkg REPL:\n\nusing Pkg\nPkg.add([\n    \"DifferentialEquations\",\n    \"Optimization\",\n    \"OptimizationPolyalgorithms\",\n    \"SciMLSensitivity\",\n    \"ForwardDiff\",\n    \"Plots\"\n])\n\nNow we're ready. Let's load in these packages:\n\nimport DifferentialEquations as DE\nimport Optimization as OPT\nimport OptimizationPolyalgorithms\nimport SciMLSensitivity\nimport ForwardDiff\nimport Plots","category":"section"},{"location":"getting_started/fit_simulation/#Step-2:-View-the-Training-Data","page":"Fit a simulation to a dataset","title":"Step 2: View the Training Data","text":"In our example, we are given observed values for x and y populations at eleven instances in time. Let's make that the training data for our Lotka-Volterra dynamical system model.\n\n# Define experimental data\nt_data = 0:10\nx_data = [1.000 2.773 6.773 0.971 1.886 6.101 1.398 1.335 4.353 3.247 1.034]\ny_data = [1.000 0.259 2.015 1.908 0.323 0.629 3.458 0.508 0.314 4.547 0.906]\nxy_data = vcat(x_data, y_data)\n\n# Plot the provided data\nPlots.scatter(t_data, xy_data', label = [\"x Data\" \"y Data\"])\n\nnote: Note\nThe Array xy_data above has been oriented with time instances as columns so that it can be directly compared with an ODESolution object. (See Solution Handling for more information on accessing DifferentialEquation.jl solution data.) However, plotting an Array with Plots.jl requires the variables to be columns and the time instances to be rows. Thus, whenever the experimental data is plotted, the transpose xy_data' will be used.","category":"section"},{"location":"getting_started/fit_simulation/#Step-3:-Set-Up-the-ODE-Model","page":"Fit a simulation to a dataset","title":"Step 3: Set Up the ODE Model","text":"We know that our system will behave according to the Lotka-Volterra ODE model, so let's set up that model with an initial guess at the parameter values: \\alpha, \\beta, \\gamma, and \\delta. Unlike the first tutorial, which used ModelingToolkit, let's demonstrate using DifferentialEquations.jl to directly define the ODE for the numerical solvers.\n\nTo do this, we define a vector-based mutating function that calculates the derivatives for our system. We will define our system as a vector u = [x,y], and thus u[1] = x and u[2] = y. This means that we need to calculate the derivative as du = [dx,dy]. Our parameters will simply be the vector p = [Î±, Î², Î´, Î³]. Writing down the Lotka-Volterra equations in the DifferentialEquations.jl direct form thus looks like the following:\n\nfunction lotka_volterra!(du, u, p, t)\n    x, y = u\n    Î±, Î², Î´, Î³ = p\n    du[1] = dx = Î± * x - Î² * x * y\n    du[2] = dy = -Î´ * y + Î³ * x * y\nend\n\nNow we need to define the initial condition, time span, and parameter vector in order to solve this differential equation. We do not currently know the parameter values, but we will guess some values to start with and optimize them later. Following the problem setup, this looks like:\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval\ntspan = (0.0, 10.0)\n\n# LV equation parameter. p = [Î±, Î², Î´, Î³]\npguess = [1.0, 1.2, 2.5, 1.2]\n\nNow we bring these pieces all together to define the ODEProblem and solve it. Note that we solve this equation with the keyword argument saveat = 1 so that it saves a point at every Delta t = 1 to match our experimental data.\n\n# Set up the ODE problem with our guessed parameter values\nprob = DE.ODEProblem(lotka_volterra!, u0, tspan, pguess)\n\n# Solve the ODE problem with our guessed parameter values\ninitial_sol = DE.solve(prob, saveat = 1)\n\n# View the guessed model solution\nplt = Plots.plot(initial_sol, label = [\"x Prediction\" \"y Prediction\"])\nPlots.scatter!(plt, t_data, xy_data', label = [\"x Data\" \"y Data\"])\n\nClearly the parameter values that we guessed are not correct to model this system.\nHowever, we can use Optimization.jl together with DifferentialEquations.jl\nto fit our parameters to our training data.\n\nnote: Note\nFor more details on using DifferentialEquations.jl, check out the getting started with DifferentialEquations.jl tutorial.","category":"section"},{"location":"getting_started/fit_simulation/#Step-4:-Set-Up-the-Loss-Function-for-Optimization","page":"Fit a simulation to a dataset","title":"Step 4: Set Up the Loss Function for Optimization","text":"Now let's start the optimization process. First, let's define a loss function to be minimized. (It is also sometimes referred to as a cost function.) For our loss function, we want to take a set of parameters, create a new ODE which has everything the same except for the changed parameters, solve this ODE with new parameters, and compare the ODE solution against the provided data. In this case, the loss returned from the loss function is a quantification of the difference between the current solution and the desired solution. When this difference is minimized, our model prediction will closely approximate the observed system data.\n\nTo change our parameter values, there is a useful functionality in the SciML problems interface called remake which creates a new version of an existing SciMLProblem with the aspect you want changed. For example, if we wanted to change the initial condition u0 of our ODE, we could do remake(prob, u0 = newu0) For our case, we want to change around just the parameters, so we can do remake(prob, p = newp). It is faster to remake an existing SciMLProblem than to create a new problem every iteration.\n\nnote: Note\nremake can change multiple items at once by passing more keyword arguments, i.e., remake(prob, u0 = newu0, p = newp). This can be used to extend the example to simultaneously learn the initial conditions and parameters!\n\nNow use remake to build the loss function. After we solve the new problem, we will calculate the sum of squared errors as our loss metric. The sum of squares can be quickly written in Julia via sum(abs2,x). Using this information, our loss function looks like:\n\nfunction loss(newp)\n    newprob = DE.remake(prob, p = newp)\n    sol = DE.solve(newprob, saveat = 1)\n    l = sum(abs2, sol .- xy_data)\n    return l\nend","category":"section"},{"location":"getting_started/fit_simulation/#Step-5:-Solve-the-Optimization-Problem","page":"Fit a simulation to a dataset","title":"Step 5: Solve the Optimization Problem","text":"This step will look very similar to the first optimization tutorial, The Optimization.solve function can accept an optional callback function to monitor the optimization process using extra arguments returned from loss.\n\nThe callback syntax is always:\n\ncallback(\n    state,\n    the current loss value,\n)\n\nIn this case, we will provide the callback the arguments (state, l), since it always takes the current state of the optimization first (state) then the current loss value (l). The return value of the callback function should default to false. Optimization.solve will halt if/when the callback function returns true instead. Typically the return statement would monitor the loss value and stop once some criteria is reached, e.g. return loss < 0.0001, but we will stop after a set number of iterations instead. More details about callbacks in Optimization.jl can be found here.\n\nfunction callback(state, l)\n    display(l)\n    newprob = DE.remake(prob, p = state.u)\n    sol = DE.solve(newprob, saveat = 1)\n    plt = Plots.plot(sol, ylim = (0, 6), label = [\"Current x Prediction\" \"Current y Prediction\"])\n    Plots.scatter!(plt, t_data, xy_data', label = [\"x Data\" \"y Data\"])\n    display(plt)\n    return false\nend\n\nWith this callback function, every step of the optimization will display both the loss value and a plot of how the solution compares to the training data. Since we want to track the fit visually we plot the simulation at each iteration and compare it to the data. This is expensive since it requires an extra solve call and a plotting step for each iteration.\n\nNow, just like the first optimization tutorial, we set up our OptimizationFunction and OptimizationProblem, and then solve the OptimizationProblem. We will initialize the OptimizationProblem with the same pguess we used when setting up the ODE Model in Step 3. Observe how Optimization.solve brings the model closer to the experimental data as it iterates towards better ODE parameter values!\n\nNote that we are using the PolyOpt() solver choice here which is discussed https://docs.sciml.ai/Optimization/dev/optimization_packages/polyopt/ since parameter estimation of non-linear differential equations is generally a non-convex problem so we want to run a stochastic algorithm (Adam) to get close to the minimum and then finish off with a quasi-newton method (L-BFGS) to find the optima. Together, this looks like:\n\n# Set up the optimization problem with our loss function and initial guess\nadtype = OPT.AutoForwardDiff()\npguess = [1.0, 1.2, 2.5, 1.2]\noptf = OPT.OptimizationFunction((x, _) -> loss(x), adtype)\noptprob = OPT.OptimizationProblem(optf, pguess)\n\n# Optimize the ODE parameters for best fit to our data\npfinal = OPT.solve(optprob,\n    OptimizationPolyalgorithms.PolyOpt(),\n    callback = callback,\n    maxiters = 200)\nÎ±, Î², Î³, Î´ = round.(pfinal, digits = 1)\n\nnote: Note\nWhen referencing the documentation for DifferentialEquations.jl and Optimization.jl simultaneously, note that the variables f, u, and p will refer to different quantities.DifferentialEquations.jl:fracdudt = f(upt)f in ODEProblem is the function defining the derivative du in the ODE.\nHere: lotka_volterra!\nu in ODEProblem contains the state variables of f.\nHere: x and y\np in ODEProblem contains the parameter variables of f.\nHere: \\alpha, \\beta, \\gamma, and \\delta\nt is the independent (time) variable.\nHere: indirectly defined with tspan in ODEProblem and saveat in solveOptimization.jl:min_u f(up)f in OptimizationProblem is the function to minimize (optimize).\nHere: the anonymous function (x, _) -> loss(x)\nu in OptimizationProblem contains the state variables of f to be optimized.\nHere: the ODE parameters \\alpha, \\beta, \\gamma, and \\delta stored in p\np in OptimizationProblem contains any fixed hyperparameters of f.\nHere: our loss function does not require any hyperparameters, so we pass _ for this p.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Partial-Differential-Equations-(PDE)","page":"Partial Differential Equations (PDE)","title":"Partial Differential Equations (PDE)","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#NeuralPDE.jl:-Physics-Informed-Neural-Network-(PINN)-PDE-Solvers","page":"Partial Differential Equations (PDE)","title":"NeuralPDE.jl: Physics-Informed Neural Network (PINN) PDE Solvers","text":"NeuralPDE.jl is a partial differential equation solver library which uses physics-informed neural networks (PINNs) to solve the equations. It uses the ModelingToolkit.jl symbolic PDESystem as its input and can handle a wide variety of equation types, including systems of partial differential equations, partial differential-algebraic equations, and integro-differential equations. Its benefit is its flexibility, and it can be used to easily generate surrogate solutions over entire parameter ranges. However, its downside is solver speed: PINN solvers tend to be a lot slower than other methods for solving PDEs.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#MethodOflines.jl:-Automated-Finite-Difference-Method-(FDM)","page":"Partial Differential Equations (PDE)","title":"MethodOflines.jl: Automated Finite Difference Method (FDM)","text":"MethodOflines.jl is a partial differential equation solver library which automates the discretization of PDEs via the finite difference method. It uses the ModelingToolkit.jl symbolic PDESystem as its input, and generates AbstractSystems and SciMLProblems whose numerical solution gives the solution to the PDE.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#FEniCS.jl:-Wrappers-for-the-Finite-Element-Method-(FEM)","page":"Partial Differential Equations (PDE)","title":"FEniCS.jl: Wrappers for the Finite Element Method (FEM)","text":"FEniCS.jl is a wrapper for the popular FEniCS finite element method library.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#HighDimPDE.jl:-High-dimensional-PDE-Solvers","page":"Partial Differential Equations (PDE)","title":"HighDimPDE.jl:  High-dimensional PDE Solvers","text":"HighDimPDE.jl is a partial differential equation solver library which implements algorithms that break down the curse of dimensionality to solve the equations. It implements deep-learning based and Picard-iteration based methods to approximately solve high-dimensional, nonlinear, non-local PDEs in up to 10,000 dimensions. Its cons are accuracy: high-dimensional solvers are stochastic, and might result in wrong solutions if the solver meta-parameters are not appropriate.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#NeuralOperators.jl:-(Fourier)-Neural-Operators-and-DeepONets-for-PDE-Solving","page":"Partial Differential Equations (PDE)","title":"NeuralOperators.jl: (Fourier) Neural Operators and DeepONets for PDE Solving","text":"NeuralOperators.jl is a library for operator learning based PDE solvers. This includes techniques like:\n\nFourier Neural Operators (FNO)\nDeep Operator Networks (DeepONets)\nMarkov Neural Operators (MNO)\n\nCurrently, its connection to PDE solving must be specified manually, though an interface for ModelingToolkit PDESystems is in progress.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#DiffEqOperators.jl:-Operators-for-Finite-Difference-Method-(FDM)-Discretizations","page":"Partial Differential Equations (PDE)","title":"DiffEqOperators.jl: Operators for Finite Difference Method (FDM) Discretizations","text":"DiffEqOperators.jl is a library for defining finite difference operators to easily perform manual FDM semi-discretizations of partial differential equations. This library is fairly incomplete and most cases should receive better performance using MethodOflines.jl.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Third-Party-Libraries-to-Note","page":"Partial Differential Equations (PDE)","title":"Third-Party Libraries to Note","text":"A more exhaustive list of Julia PDE packages can be found here: https://github.com/JuliaPDE/SurveyofPDEPackages","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#ApproxFun.jl:-Automated-Spectral-Discretizations","page":"Partial Differential Equations (PDE)","title":"ApproxFun.jl: Automated Spectral Discretizations","text":"ApproxFun.jl is a package for approximating functions in basis sets. One particular use case is with spectral basis sets, such as Chebyshev functions and Fourier decompositions, making it easy to represent spectral and pseudospectral discretizations of partial differential equations as ordinary differential equations for the SciML equation solvers.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Decapodes.jl:-Discrete-Exterior-Calculus-Applied-to-Partial-and-Ordinary-Differential-Equation-Systems","page":"Partial Differential Equations (PDE)","title":"Decapodes.jl: Discrete Exterior Calculus Applied to Partial and Ordinary Differential Equation Systems","text":"Decapodes.jl is a computational physics framework based on the Discrete Exterior Calculus (DEC). It uses structure preserving discretization methods from the DEC to simulate multiphysics problems. Feature include solving PDEs on triangulated manifolds, a strongly typed equation representation that can help ensure correctness of simulations, and compositional methods for specifying multi-physics problems.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Ferrite.jl:-Finite-Element-Toolbox-for-Julia","page":"Partial Differential Equations (PDE)","title":"Ferrite.jl: Finite Element Toolbox for Julia","text":"Ferrite.jl is a performant and extensible library which provides algorithms and data structures to develop finite element software. This library aims at users which need fine grained control over all algorithmic details, as for example often necessary in research when developing new grid-based PDE discretizations or other more advanced problem formulations for example found in continuum mechanics.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Gridap.jl:-Julia-Based-Tools-for-Finite-Element-Discretizations","page":"Partial Differential Equations (PDE)","title":"Gridap.jl: Julia-Based Tools for Finite Element Discretizations","text":"Gridap.jl is a package for grid-based approximation of partial differential equations, particularly notable for its use of conforming and nonconforming finite element (FEM) discretizations.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#Trixi.jl:-Adaptive-High-Order-Numerical-Simulations-of-Hyperbolic-Equations","page":"Partial Differential Equations (PDE)","title":"Trixi.jl: Adaptive High-Order Numerical Simulations of Hyperbolic Equations","text":"Trixi.jl is a package for numerical simulation of hyperbolic conservation laws, i.e. a large set of hyperbolic partial differential equations, which interfaces and uses the SciML ordinary differential equation solvers.","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#VoronoiFVM.jl:-Tools-for-the-Voronoi-Finite-Volume-Discretizations","page":"Partial Differential Equations (PDE)","title":"VoronoiFVM.jl: Tools for the Voronoi Finite Volume Discretizations","text":"VoronoiFVM.jl is a library for generating FVM discretizations of systems of PDEs. It interfaces with many of the SciML equation solver libraries to allow for ease of discretization and flexibility in the solver choice.","category":"section"},{"location":"showcase/gpu_spde/#gpuspde","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"GPU-Accelerated Stochastic Partial Differential Equations","text":"Let's solve stochastic PDEs in Julia using GPU parallelism. To do this, we will use the type-genericness of the DifferentialEquations.jl library in order to write a code that uses within-method GPU-parallelism on the system of PDEs. The OrdinaryDiffEq.jl solvers of DifferentialEquations.jl, including implicit solvers with GMRES, etc., and the same for SDEs, DAEs, DDEs, etc. are all GPU-compatible with a fast form of broadcast.\n\nnote: Note\nThe non-native Julia solvers, like Sundials are incompatible with arbitrary input types and thus not compatible with GPUs.\n\nLet's dive into showing how to accelerate ODE solving with GPUs!","category":"section"},{"location":"showcase/gpu_spde/#Before-we-start:-the-two-ways-to-accelerate-ODE-solvers-with-GPUs","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Before we start: the two ways to accelerate ODE solvers with GPUs","text":"Before we dive deeper, let us remark that there are two very different ways that one can accelerate an ODE solution with GPUs. There is one case where u is very big and f is very expensive, but very structured, and you use GPUs to accelerate the computation of said f. The other use case is where u is very small but you want to solve the ODE f over many different initial conditions (u0) or parameters p. In that case, you can use GPUs to parallelize over different parameters and initial conditions. In other words:\n\nType of Problem SciML Solution\nAccelerate a big ODE Use CUDA.jl's CuArray as u0\nSolve the same ODE with many u0 and p Use DiffEqGPU.jl's EnsembleGPUArray and EnsembleGPUKernel\n\nThis showcase will focus on the former case. For the latter, see the massively parallel GPU ODE solving showcase.","category":"section"},{"location":"showcase/gpu_spde/#Our-Problem:-2-dimensional-Reaction-Diffusion-Equations","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Our Problem: 2-dimensional Reaction-Diffusion Equations","text":"The reaction-diffusion equation is a PDE commonly handled in systems biology, which is a diffusion equation plus a nonlinear reaction term. The dynamics are defined as:\n\nu_t = D Delta u + f(tu)\n\nBut this doesn't need to only have a single â€œreactantâ€ u: this can be a vector of reactants and the f is then the nonlinear vector equations describing how these different pieces react together. Let's settle on a specific equation to make this easier to explain. Let's use a simple model of a 3-component system where A can diffuse through space to bind with the non-diffusive B to form the complex C (also non-diffusive, assume B is too big and gets stuck in a cell which causes C=A+B to be stuck as well). Other than the binding, we make each of these undergo a simple birth-death process, and we write down the equations which result from mass-action kinetics. If this all is meaningless to you, just understand that it gives the system of PDEs:\n\nbeginalign\nA_t = D Delta A + alpha_A(x) - beta_A  A - r_1 A B + r_2 C\nB_t = alpha_B - beta_B B - r_1 A B + r_2 C\nC_t = alpha_C - beta_C C + r_1 A B - r_2 C\nendalign\n\nOne addition that was made to the model is that we let alpha_A(x) be the production of A, and we let that be a function of space so that way it only is produced on one side of our equation. Let's make it a constant when x>80, and 0 otherwise, and let our spatial domain be x in 0100 and y in 0100.\n\nThis model is spatial: each reactant u(txy) is defined at each point in space, and all of the reactions are local, meaning that f at spatial point (xy) only uses u_i(txy). This is an important fact which will come up later for parallelization.","category":"section"},{"location":"showcase/gpu_spde/#Discretizing-the-PDE-into-ODEs","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Discretizing the PDE into ODEs","text":"In order to solve this via a method of lines (MOL) approach, we need to discretize the PDE into a system of ODEs. Let's do a simple uniformly-spaced grid finite difference discretization. Choose dx = 1 and dy = 1 so that we have 100*100=10000 points for each reactant. Notice how fast that grows! Put the reactants in a matrix such that A[i,j] = A(x_j,y_i), i.e. the columns of the matrix are the x values and the rows are the y values (this way looking at the matrix is essentially like looking at the discretized space).\n\nSo now we have 3 matrices (A, B, and C) for our reactants. How do we discretize the PDE? In this case, the diffusion term simply becomes a tridiagonal matrix M where 1-21 is the central band. You can notice that MA performs diffusion along the columns of A, and so this is diffusion along the y. Similarly, AM flips the indices and thus does diffusion along the rows of A making this diffusion along x. Thus D(M_yA + AM_x) is the discretized Laplacian (we could have separate diffusion constants and dx neq dy if we want by using different constants on the M, but let's not do that for this simple example. We leave that as an exercise for the reader). We enforced a Neumann boundary condition with zero derivative (also known as a no-flux boundary condition) by reflecting the changes over the boundary. Thus the derivative operator is generated as:\n\nimport LinearAlgebra as LA\n\n# Define the constants for the PDE\nconst Î±â‚‚ = 1.0\nconst Î±â‚ƒ = 1.0\nconst Î²â‚ = 1.0\nconst Î²â‚‚ = 1.0\nconst Î²â‚ƒ = 1.0\nconst râ‚ = 1.0\nconst râ‚‚ = 1.0\nconst D = 100.0\nconst Î³â‚ = 0.1\nconst Î³â‚‚ = 0.1\nconst Î³â‚ƒ = 0.1\nconst N = 100\nconst X = reshape([i for i in 1:100 for j in 1:100], N, N)\nconst Y = reshape([j for i in 1:100 for j in 1:100], N, N)\nconst Î±â‚ = 1.0 .* (X .>= 80)\n\nconst Mx = LA.Tridiagonal([1.0 for i in 1:(N - 1)], [-2.0 for i in 1:N],\n    [1.0 for i in 1:(N - 1)])\nconst My = copy(Mx)\n# Do the reflections, different for x and y operators\nMx[2, 1] = 2.0\nMx[end - 1, end] = 2.0\nMy[1, 2] = 2.0\nMy[end, end - 1] = 2.0\n\nnote: Note\nWe could have also done these discretization steps using DiffEqOperators.jl or MethodOfLines.jl. However, we are going to keep it in this form, so we can show the full code, making it easier to see how to define GPU-ready code!\n\nSince all of the reactions are local, we only have each point in space react separately. Thus this represents itself as element-wise equations on the reactants. Thus we can write it out quite simply. The ODE which then represents the PDE is thus in pseudo Julia code:\n\nDA = D * (Mx * A + A * My)\n@. DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n@. Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n@. Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\n\nNote here that I am using Î±â‚ as a matrix (or row-vector, since that will broadcast just fine) where every point in space with x<80 has this zero, and all of the others have it as a constant. The other coefficients are all scalars.\n\nHow do we do this with the ODE solver?","category":"section"},{"location":"showcase/gpu_spde/#Our-Representation-via-Views-of-3-Tensors","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Our Representation via Views of 3-Tensors","text":"We can represent our problem with a 3-dimensional tensor, taking each 2-dimensional slice as our (A,B,C). This means that we can define:\n\nu0 = zeros(N, N, 3);\n\nNow we can decompose it like:\n\nA = @view u[:, :, 1]\nB = @view u[:, :, 2]\nC = @view u[:, :, 3]\ndA = @view du[:, :, 1]\ndB = @view du[:, :, 2]\ndC = @view du[:, :, 3]\n\nThese views will not construct new arrays and will instead just be pointers to the (contiguous) memory pieces, so this is a nice and efficient way to handle this. Together, our ODE using this tensor as its container can be written as follows:\n\nfunction f(du, u, p, t)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    DA = D * (Mx * A + A * My)\n    @. dA = DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\n\nwhere this is using @. to do inplace updates on our du to say how the full tensor should update in time. Note that we can make this more efficient by adding some cache variables to the diffusion matrix multiplications and using mul!, but let's ignore that for now.\n\nTogether, the ODE which defines our PDE is thus:\n\nimport DifferentialEquations as DE\n\nprob = DE.ODEProblem(f, u0, (0.0, 100.0))\n@time sol = DE.solve(prob, DE.ROCK2());\n\n@time sol = DE.solve(prob, DE.ROCK2());\n\nif I want to solve it on t in 0100. Done! The solution gives back our tensors (and interpolates to create new ones if you use sol(t)). We can plot it in Plots.jl:\n\nimport Plots\np1 = Plots.surface(X, Y, sol[end][:, :, 1], title = \"[A]\")\np2 = Plots.surface(X, Y, sol[end][:, :, 2], title = \"[B]\")\np3 = Plots.surface(X, Y, sol[end][:, :, 3], title = \"[C]\")\nPlots.plot(p1, p2, p3, layout = Plots.grid(3, 1))\n\nand see the pretty gradients. Using this 2nd order ROCK method we solve this equation in about 2 seconds. That's okay.","category":"section"},{"location":"showcase/gpu_spde/#Some-Optimizations","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Some Optimizations","text":"There are some optimizations that can still be done. When we do A*B as matrix multiplication, we create another temporary matrix. These allocations can bog down the system. Instead, we can pre-allocate the outputs and use the inplace functions mul! to make better use of memory. The easiest way to store these cache arrays are constant globals, but you can use closures (anonymous functions which capture data, i.e. (x)->f(x,y)) or call-overloaded types to do it without globals. The globals way (the easy way) is simply:\n\nconst MyA = zeros(N, N)\nconst AMx = zeros(N, N)\nconst DA = zeros(N, N)\nfunction f(du, u, p, t)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    LA.mul!(MyA, My, A)\n    LA.mul!(AMx, A, Mx)\n    @. DA = D * (MyA + AMx)\n    @. dA = DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\n\nFor reference, closures looks like:\n\nMyA = zeros(N, N)\nAMx = zeros(N, N)\nDA = zeros(N, N)\nfunction f_full(du, u, p, t, MyA, AMx, DA)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    LA.mul!(MyA, My, A)\n    LA.mul!(AMx, A, Mx)\n    @. DA = D * (MyA + AMx)\n    @. dA = DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\nf(du, u, p, t) = f_full(du, u, p, t, MyA, AMx, DA)\n\nand a call overloaded type looks like:\n\nstruct MyFunction{T} <: Function\n    MyA::T\n    AMx::T\n    DA::T\nend\n\n# Now define the overload\nfunction (ff::MyFunction)(du, u, p, t)\n    # This is a function which references itself via ff\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    LA.mul!(ff.MyA, My, A)\n    LA.mul!(ff.AMx, A, Mx)\n    @. ff.DA = D * (ff.MyA + ff.AMx)\n    @. dA = f.DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\n\nMyA = zeros(N, N)\nAMx = zeros(N, N)\nDA = zeros(N, N)\n\nf = MyFunction(MyA, AMx, DA)\n# Now f(du,u,p,t) is our function!\n\nThese last two ways enclose the pointer to our cache arrays locally but still present a function f(du,u,p,t) to the ODE solver.\n\nNow, since PDEs are large, many times we don't care about getting the whole timeseries. Using the output controls from DifferentialEquations.jl, we can make it only output the final timepoint.\n\nprob = ODEProblem(f, u0, (0.0, 100.0))\n@time sol = solve(prob, ROCK2(), progress = true, save_everystep = false,\n    save_start = false);\n@time sol = solve(prob, ROCK2(), progress = true, save_everystep = false,\n    save_start = false);\n\nAround 0.4 seconds. Much better. Also, if you're using VS Code, this'll give you a nice progress bar, so you can track how it's going.","category":"section"},{"location":"showcase/gpu_spde/#Quick-Note-About-Performance","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Quick Note About Performance","text":"note: Note\nWe are using the ROCK2 method here because it's a method for stiff equations with eigenvalues that are real-dominated (as opposed to dominated by the imaginary parts). If we wanted to use a more conventional implicit ODE solver, we would need to make use of the sparsity pattern. This is covered in the advanced ODE tutorial It turns out that ROCK2 is more efficient anyway (and doesn't require sparsity handling), so we will keep this setup.","category":"section"},{"location":"showcase/gpu_spde/#Quick-Summary:-full-PDE-ODE-Code","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Quick Summary: full PDE ODE Code","text":"As a summary, here's a full PDE code:\n\nimport OrdinaryDiffEq as ODE\nimport LinearAlgebra as LA\n\n# Define the constants for the PDE\nconst Î±â‚‚ = 1.0\nconst Î±â‚ƒ = 1.0\nconst Î²â‚ = 1.0\nconst Î²â‚‚ = 1.0\nconst Î²â‚ƒ = 1.0\nconst râ‚ = 1.0\nconst râ‚‚ = 1.0\nconst D = 100.0\nconst Î³â‚ = 0.1\nconst Î³â‚‚ = 0.1\nconst Î³â‚ƒ = 0.1\nconst N = 100\nconst X = reshape([i for i in 1:100 for j in 1:100], N, N)\nconst Y = reshape([j for i in 1:100 for j in 1:100], N, N)\nconst Î±â‚ = 1.0 .* (X .>= 80)\n\nconst Mx = Array(LA.Tridiagonal([1.0 for i in 1:(N - 1)], [-2.0 for i in 1:N],\n    [1.0 for i in 1:(N - 1)]))\nconst My = copy(Mx)\nMx[2, 1] = 2.0\nMx[end - 1, end] = 2.0\nMy[1, 2] = 2.0\nMy[end, end - 1] = 2.0\n\n# Define the initial condition as normal arrays\nu0 = zeros(N, N, 3)\n\nconst MyA = zeros(N, N);\nconst AMx = zeros(N, N);\nconst DA = zeros(N, N)\n# Define the discretized PDE as an ODE function\nfunction f(du, u, p, t)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    LA.mul!(MyA, My, A)\n    LA.mul!(AMx, A, Mx)\n    @. DA = D * (MyA + AMx)\n    @. dA = DA + Î±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\n\n# Solve the ODE\nprob = ODE.ODEProblem(f, u0, (0.0, 100.0))\nsol = ODE.solve(prob, ODE.ROCK2(), progress = true, save_everystep = false, save_start = false)\n\nimport Plots;\nPlots.gr();\np1 = Plots.surface(X, Y, sol[end][:, :, 1], title = \"[A]\")\np2 = Plots.surface(X, Y, sol[end][:, :, 2], title = \"[B]\")\np3 = Plots.surface(X, Y, sol[end][:, :, 3], title = \"[C]\")\nPlots.plot(p1, p2, p3, layout = Plots.grid(3, 1))","category":"section"},{"location":"showcase/gpu_spde/#Making-Use-of-GPU-Parallelism","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"Making Use of GPU Parallelism","text":"That was all using the CPU. How do we turn on GPU parallelism with DifferentialEquations.jl? Well, you don't. DifferentialEquations.jl \"doesn't have GPU bits\". So, wait... can we not do GPU parallelism? No, this is the glory of type-genericness, especially in broadcasted operations. To make things use the GPU, we simply use a CuArray from CUDA.jl. If instead of zeros(N,M) we used CuArray(zeros(N,M)), then the array lives on the GPU. CuArray naturally overrides broadcast such that dotted operations are performed on the GPU. DifferentialEquations.jl uses broadcast internally, and thus just by putting the array as a CuArray, the array-type will take over how all internal updates are performed and turn this algorithm into a fully GPU-parallelized algorithm that doesn't require copying to the CPU. Wasn't that simple?\n\nFrom that you can probably also see how to multithread everything, or how to set everything up with distributed parallelism. You can make the ODE solvers do whatever you want by defining an array type where the broadcast does whatever special behavior you want.\n\nSo to recap, the entire difference from above is changing to:\n\nimport CUDA\nconst gMx = CUDA.CuArray(Float32.(Mx))\nconst gMy = CUDA.CuArray(Float32.(My))\nconst gÎ±â‚ = CUDA.CuArray(Float32.(Î±â‚))\ngu0 = CUDA.CuArray(Float32.(u0))\n\nconst gMyA = CUDA.CuArray(zeros(Float32, N, N))\nconst AgMx = CUDA.CuArray(zeros(Float32, N, N))\nconst gDA = CUDA.CuArray(zeros(Float32, N, N))\nfunction gf(du, u, p, t)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    LA.mul!(gMyA, gMy, A)\n    LA.mul!(AgMx, A, gMx)\n    @. gDA = D * (gMyA + AgMx)\n    @. dA = gDA + gÎ±â‚ - Î²â‚ * A - râ‚ * A * B + râ‚‚ * C\n    @. dB = Î±â‚‚ - Î²â‚‚ * B - râ‚ * A * B + râ‚‚ * C\n    @. dC = Î±â‚ƒ - Î²â‚ƒ * C + râ‚ * A * B - râ‚‚ * C\nend\n\nprob2 = DE.ODEProblem(gf, gu0, (0.0, 100.0))\nCUDA.allowscalar(false) # makes sure none of the slow fallbacks are used\n@time sol = DE.solve(prob2, DE.ROCK2(), progress = true, dt = 0.003, save_everystep = false,\n    save_start = false);\n\n@time sol = DE.solve(prob2, DE.ROCK2(), progress = true, dt = 0.003, save_everystep = false,\n    save_start = false);\n\nGo have fun.","category":"section"},{"location":"showcase/gpu_spde/#And-Stochastic-PDEs?","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"And Stochastic PDEs?","text":"Why not make it an SPDE? All that we need to do is extend each of the PDE equations to have a noise function. In this case, let's use multiplicative noise on each reactant. This means that our noise update equation is:\n\nfunction g(du, u, p, t)\n    A = @view u[:, :, 1]\n    B = @view u[:, :, 2]\n    C = @view u[:, :, 3]\n    dA = @view du[:, :, 1]\n    dB = @view du[:, :, 2]\n    dC = @view du[:, :, 3]\n    @. dA = Î³â‚ * A\n    @. dB = Î³â‚‚ * A\n    @. dC = Î³â‚ƒ * A\nend\n\nNow we just define and solve the system of SDEs:\n\nprob = DE.SDEProblem(f, g, u0, (0.0, 100.0))\n@time sol = DE.solve(prob, DE.SRIW1());\n\nPlots.gr();\n\n# Use `Array` to transform the result back into a CPU-based `Array` for plotting\np1 = Plots.surface(X, Y, Array(sol[end][:, :, 1]), title = \"[A]\")\np2 = Plots.surface(X, Y, Array(sol[end][:, :, 2]), title = \"[B]\")\np3 = Plots.surface(X, Y, Array(sol[end][:, :, 3]), title = \"[C]\")\nPlots.plot(p1, p2, p3, layout = Plots.grid(3, 1))\n\nWe can see the cool effect that diffusion dampens the noise in [A] but is unable to dampen the noise in [B] which results in a very noisy [C]. The stiff SPDE takes much longer to solve even using high order plus adaptivity because stochastic problems are just that much more difficult (current research topic is to make new algorithms for this!). It gets GPU'd just by using CuArray like before. But there we go: solving systems of stochastic PDEs using high order adaptive algorithms with within-method GPU parallelism. That's gotta be a first? The cool thing is that nobody ever had to implement the GPU-parallelism either, it just exists by virtue of the Julia type system.\n\n(Note: We can also use one of the SROCK methods for better performance here, but they will require a choice of dt. This is left to the reader to try.)\n\nnote: Note\nThis can take a while to solve! An explicit Runge-Kutta algorithm isn't necessarily great here, though to use a stiff solver on a problem of this size requires once again smartly choosing sparse linear solvers. The high order adaptive method is pretty much necessary though, since something like Euler-Maruyama is simply not stable enough to solve this at a reasonable dt. Also, the current algorithms are not so great at handling this problem. Good thing there's a publication coming along with some new stuff...","category":"section"},{"location":"showcase/bayesian_neural_ode/#bnode","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Uncertainty Quantified Deep Bayesian Model Discovery","text":"In this tutorial, we show how SciML can combine the differential equation solvers seamlessly with Bayesian estimation libraries like AdvancedHMC.jl and Turing.jl. This enables converting Neural ODEs to Bayesian Neural ODEs, which enables us to estimate the error in the Neural ODE estimation and forecasting. In this tutorial, a working example of the Bayesian Neural ODE: NUTS sampler is shown.\n\nnote: Note\nFor more details, have a look at this paper: https://arxiv.org/abs/2012.07244","category":"section"},{"location":"showcase/bayesian_neural_ode/#Step-1:-Import-Libraries","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Step 1: Import Libraries","text":"For this example, we will need the following libraries:\n\n# SciML Libraries\nimport SciMLSensitivity as SMS\nimport DifferentialEquations as DE\n\n# ML Tools\nimport Lux\nimport Zygote\n\n# External Tools\nimport Random\nimport Plots\nimport AdvancedHMC\nimport MCMCChains\nimport StatsPlots\nimport ComponentArrays","category":"section"},{"location":"showcase/bayesian_neural_ode/#Setup:-Get-the-data-from-the-Spiral-ODE-example","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Setup: Get the data from the Spiral ODE example","text":"We will also need data to fit against. As a demonstration, we will generate our data using a simple cubic ODE u' = A*u^3 as follows:\n\nu0 = [2.0; 0.0]\ndatasize = 40\ntspan = (0.0, 1)\ntsteps = range(tspan[1], tspan[2], length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u .^ 3)'true_A)'\nend\nprob_trueode = DE.ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(DE.solve(prob_trueode, DE.Tsit5(), saveat = tsteps))\n\nWe will want to train a neural network to capture the dynamics that fit ode_data.","category":"section"},{"location":"showcase/bayesian_neural_ode/#Step-2:-Define-the-Neural-ODE-architecture.","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Step 2: Define the Neural ODE architecture.","text":"Note that this step potentially offers a lot of flexibility in the number of layers/ number of units in each layer. It may not necessarily be true that a 100 units architecture is better at prediction/forecasting than a 50 unit architecture. On the other hand, a complicated architecture can take a huge computational time without increasing performance.\n\ndudt2 = Lux.Chain(x -> x .^ 3,\n    Lux.Dense(2, 50, tanh),\n    Lux.Dense(50, 2))\n\nrng = Random.default_rng()\np, st = Lux.setup(rng, dudt2)\nconst _st = st\nfunction neuralodefunc(u, p, t)\n    dudt2(u, p, _st)[1]\nend\nfunction prob_neuralode(u0, p)\n    prob = DE.ODEProblem(neuralodefunc, u0, tspan, p)\n    sol = DE.solve(prob, DE.Tsit5(), saveat = tsteps)\nend\np = ComponentArrays.ComponentArray{Float64}(p)\nconst _p = p\n\nNote that the f64 is required to put the Lux neural network into Float64 precision.","category":"section"},{"location":"showcase/bayesian_neural_ode/#Step-3:-Define-the-loss-function-for-the-Neural-ODE.","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Step 3: Define the loss function for the Neural ODE.","text":"function predict_neuralode(p)\n    p = p isa ComponentArrays.ComponentArray ? p : convert(typeof(_p), p)\n    Array(prob_neuralode(u0, p))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"section"},{"location":"showcase/bayesian_neural_ode/#Step-4:-Now-we-start-integrating-the-Bayesian-estimation-workflow-as-prescribed-by-the-AdvancedHMC-interface-with-the-NeuralODE-defined-above","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Step 4: Now we start integrating the Bayesian estimation workflow as prescribed by the AdvancedHMC interface with the NeuralODE defined above","text":"The AdvancedHMC interface requires us to specify: (a) the Hamiltonian log density and its gradient , (b) the sampler and (c) the step size adaptor function.\n\nFor the Hamiltonian log density, we use the loss function. The Î¸*Î¸ term denotes the use of Gaussian priors.\n\nThe user can make several modifications to Step 4. The user can try different acceptance ratios, warmup samples and posterior samples. One can also use the Variational Inference (ADVI) framework, which doesn't work quite as well as NUTS. The SGLD (Stochastic Gradient Langevin Descent) sampler is seen to have a better performance than NUTS. Have a look at https://sebastiancallh.github.io/post/langevin/ for a brief introduction to SGLD.\n\nl(Î¸) = -sum(abs2, ode_data .- predict_neuralode(Î¸)) - sum(Î¸ .* Î¸)\nfunction dldÎ¸(Î¸)\n    x, lambda = Zygote.pullback(l, Î¸)\n    grad = first(lambda(1))\n    return x, grad\nend\n\nmetric = AdvancedHMC.DiagEuclideanMetric(oneunit.(p))\nh = AdvancedHMC.Hamiltonian(metric, l, dldÎ¸)\n\nWe use the NUTS sampler with an acceptance ratio of Î´= 0.45 in this example. In addition, we use Nesterov Dual Averaging for the Step Size adaptation.\n\nWe sample using 500 warmup samples and 500 posterior samples.\n\nintegrator = AdvancedHMC.Leapfrog(AdvancedHMC.find_good_stepsize(h, p))\nkernel = AdvancedHMC.HMCKernel(AdvancedHMC.Trajectory{AdvancedHMC.MultinomialTS}(integrator, AdvancedHMC.GeneralisedNoUTurn()))\nadaptor = AdvancedHMC.StanHMCAdaptor(AdvancedHMC.MassMatrixAdaptor(metric), AdvancedHMC.StepSizeAdaptor(0.45, integrator))\nsamples, stats = AdvancedHMC.sample(h, kernel, p, 500, adaptor, 500; progress = true)","category":"section"},{"location":"showcase/bayesian_neural_ode/#Step-5:-Plot-diagnostics","page":"Uncertainty Quantified Deep Bayesian Model Discovery","title":"Step 5: Plot diagnostics","text":"Now let's make sure the fit is good. This can be done by looking at the chain mixing plot and the autocorrelation plot. First, let's create the chain mixing plot using the plot recipes from ????\n\nsamples = hcat(samples...)\nsamples_reduced = samples[1:5, :]\nsamples_reshape = reshape(samples_reduced, (500, 5, 1))\nChain_Spiral = MCMCChains.Chains(samples_reshape)\nPlots.plot(Chain_Spiral)\n\nNow we check the autocorrelation plot:\n\nMCMCChains.autocorplot(Chain_Spiral)\n\nAs another diagnostic, let's check the result on retrodicted data. To do this, we generate solutions of the Neural ODE on samples of the neural network parameters, and check the results of the predictions against the data. Let's start by looking at the time series:\n\npl = Plots.scatter(tsteps, ode_data[1, :], color = :red, label = \"Data: Var1\", xlabel = \"t\",\n    title = \"Spiral Neural ODE\")\nPlots.scatter!(tsteps, ode_data[2, :], color = :blue, label = \"Data: Var2\")\nfor k in 1:300\n    resol = predict_neuralode(samples[:, 100:end][:, rand(1:400)])\n    Plots.plot!(tsteps, resol[1, :], alpha = 0.04, color = :red, label = \"\")\n    Plots.plot!(tsteps, resol[2, :], alpha = 0.04, color = :blue, label = \"\")\nend\n\nlosses = map(x -> loss_neuralode(x)[1], eachcol(samples))\nidx = findmin(losses)[2]\nprediction = predict_neuralode(samples[:, idx])\nPlots.plot!(tsteps, prediction[1, :], color = :black, w = 2, label = \"\")\nPlots.plot!(tsteps, prediction[2, :], color = :black, w = 2, label = \"Best fit prediction\",\n    ylims = (-2.5, 3.5))\n\nThat showed the time series form. We can similarly do a phase-space plot:\n\npl = Plots.scatter(ode_data[1, :], ode_data[2, :], color = :red, label = \"Data\", xlabel = \"Var1\",\n    ylabel = \"Var2\", title = \"Spiral Neural ODE\")\nfor k in 1:300\n    resol = predict_neuralode(samples[:, 100:end][:, rand(1:400)])\n    Plots.plot!(resol[1, :], resol[2, :], alpha = 0.04, color = :red, label = \"\")\nend\nPlots.plot!(prediction[1, :], prediction[2, :], color = :black, w = 2,\n    label = \"Best fit prediction\", ylims = (-2.5, 3))","category":"section"},{"location":"overview/#overview","page":"Detailed Overview of the SciML Software Ecosystem","title":"Detailed Overview of the SciML Software Ecosystem","text":"","category":"section"},{"location":"overview/#SciML:-Combining-High-Performance-Scientific-Computing-and-Machine-Learning","page":"Detailed Overview of the SciML Software Ecosystem","title":"SciML: Combining High-Performance Scientific Computing and Machine Learning","text":"SciML is not standard machine learning, SciML is the combination of scientific computing techniques with machine learning. Thus the SciML organization is not an organization for machine learning libraries (see FluxML for machine learning in Julia), rather SciML is an organization dedicated to the development of scientific computing tools which work seamlessly in conjunction with next-generation machine learning workflows. This includes:\n\nHigh-performance and accurate tools for standard scientific computing modeling and simulation\nCompatibility with differentiable programming and automatic differentiation\nTools for building complex multiscale models\nMethods for handling inverse problems, model calibration, controls, and Bayesian analysis\nSymbolic modeling tools for generating efficient code for numerical equation solvers\nMethods for automatic discovery of (bio)physical equations\n\nand much more. For an overview of the broad goals of the SciML organization, watch:\n\nThe Use and Practice of Scientific Machine Learning\nState of SciML Scientific Machine Learning","category":"section"},{"location":"overview/#Overview-of-Computational-Science-in-Julia-with-SciML","page":"Detailed Overview of the SciML Software Ecosystem","title":"Overview of Computational Science in Julia with SciML","text":"Below is a simplification of the user-facing packages for use in scientific computing and SciML workflows.\n\nWorkflow Element SciML-Supported Julia packages\nPlotting and Visualization Plots*, Makie*\nSparse matrix SparseArrays*\nInterpolation/approximation DataInterpolations*, ApproxFun*\nLinear system / least squares LinearSolve\nNonlinear system / rootfinding NonlinearSolve\nPolynomial roots Polynomials*\nIntegration Integrals\nNonlinear Optimization Optimization\nOther Optimization (linear, quadratic, convex, etc.) JuMP*\nInitial-value problem DifferentialEquations\nBoundary-value problem DifferentialEquations\nContinuous-Time Markov Chains (Poisson Jumps), Jump Diffusions JumpProcesses\nFinite differences FiniteDifferences*, FiniteDiff*\nAutomatic Differentiation ForwardDiff*, Enzyme*, DiffEqSensitivity\nBayesian Modeling Turing*\nDeep Learning Flux*\nAcausal Modeling / DAEs ModelingToolkit\nChemical Reaction Networks Catalyst\nSymbolic Computing Symbolics\nFast Fourier Transform FFTW*\nPartial Differential Equation Discretizations Associated Julia packages\nâ€“- â€“-\nFinite Differences MethodOfLines\nDiscontinuous Galerkin Trixi*\nFinite Element Gridap*\nPhysics-Informed Neural Networks NeuralPDE\nNeural Operators NeuralOperators\nHigh Dimensional Deep Learning HighDimPDE\n\n* Denotes a non-SciML package that is heavily tested against as part of SciML workflows and has frequent collaboration with the SciML developers.\n\n(Image: SciML Mind Map)","category":"section"},{"location":"overview/#Domains-of-SciML","page":"Detailed Overview of the SciML Software Ecosystem","title":"Domains of SciML","text":"The SciML common interface covers the following domains:\n\nLinear systems (LinearProblem)\nDirect methods for dense and sparse\nIterative solvers with preconditioning\nNonlinear Systems (NonlinearProblem)\nSystems of nonlinear equations\nScalar bracketing systems\nIntegrals (quadrature) (IntegralProblem)\nDifferential Equations\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem and JumpProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (DEProblems with callbacks and JumpProblem)\nOptimization (OptimizationProblem)\nNonlinear (constrained) optimization\n(Stochastic/Delay/Differential-Algebraic) Partial Differential Equations (PDESystem)\nFinite difference and finite volume methods\nInterfaces to finite element methods\nPhysics-Informed Neural Networks (PINNs)\nIntegro-Differential Equations\nFractional Differential Equations\nSpecialized Forms\nPartial Integro-Differential Equations (PIPDEProblem)\nData-driven modeling\nDiscrete-time data-driven dynamical systems (DiscreteDataDrivenProblem)\nContinuous-time data-driven dynamical systems (ContinuousDataDrivenProblem)\nSymbolic regression (DirectDataDrivenProblem)\nUncertainty quantification and expected values (ExpectationProblem)\n\nThe SciML common interface also includes ModelingToolkit.jl for defining such systems symbolically, allowing for optimizations like automated generation of parallel code, symbolic simplification, and generation of sparsity patterns.","category":"section"},{"location":"overview/#Inverse-Problems,-Parameter-Estimation,-and-Structural-Identification","page":"Detailed Overview of the SciML Software Ecosystem","title":"Inverse Problems, Parameter Estimation, and Structural Identification","text":"Parameter estimation and inverse problems are solved directly on their constituent problem types using tools like SciMLSensitivity.jl. Thus for example, there is no ODEInverseProblem, and instead ODEProblem is used to find the parameters p that solve the inverse problem. Check out the SciMLSensitivity documentation for a discussion on connections to automatic differentiation, optimization, and adjoints.","category":"section"},{"location":"overview/#Common-Interface-High-Level-Overview","page":"Detailed Overview of the SciML Software Ecosystem","title":"Common Interface High-Level Overview","text":"The SciML interface is common as the usage of arguments is standardized across all of the problem domains. Underlying high-level ideas include:\n\nAll domains use the same interface of defining a AbstractSciMLProblem which is then solved via solve(prob,alg;kwargs), where alg is a AbstractSciMLAlgorithm. The keyword argument namings are standardized across the organization.\nAbstractSciMLProblems are generally defined by a AbstractSciMLFunction which can define extra details about a model function, such as its analytical Jacobian, its sparsity patterns and so on.\nThere is an organization-wide method for defining linear and nonlinear solvers used within other solvers, giving maximum control of performance to the user.\nTypes used within the packages are defined by the input types. For example, packages attempt to internally use the type of the initial condition as the type for the state within differential equation solvers.\nsolve calls should be thread-safe and parallel-safe.\ninit(prob,alg;kwargs) returns an iterator which allows for directly iterating over the solution process\nHigh performance is key. Any performance that is not at the top level is considered a bug and should be reported as such.\nAll functions have an in-place and out-of-place form, where the in-place form is made to utilize mutation for high performance on large-scale problems and the out-of-place form is for compatibility with tooling like static arrays and some reverse-mode automatic differentiation systems.","category":"section"},{"location":"overview/#Flowchart-Example-for-PDE-Constrained-Optimal-Control","page":"Detailed Overview of the SciML Software Ecosystem","title":"Flowchart Example for PDE-Constrained Optimal Control","text":"The following example showcases how the pieces of the common interface connect to solve a problem that mixes inference, symbolics, and numerics.\n\n(Image: )","category":"section"},{"location":"overview/#External-Binding-Libraries","page":"Detailed Overview of the SciML Software Ecosystem","title":"External Binding Libraries","text":"diffeqr\nSolving differential equations in R using DifferentialEquations.jl with ModelingToolkit for JIT compilation and GPU-acceleration\ndiffeqpy\nSolving differential equations in Python using DifferentialEquations.jl","category":"section"},{"location":"overview/#Note-About-Third-Party-Libraries","page":"Detailed Overview of the SciML Software Ecosystem","title":"Note About Third-Party Libraries","text":"The SciML documentation references and recommends many third-party libraries for improving ones modeling, simulation, and analysis workflow in Julia. Take these as a positive affirmation of the quality of these libraries, as these libraries are commonly tested by SciML developers who are in contact with the development teams of these groups. It also documents the libraries which are commonly chosen by SciML as dependencies. Do not take omissions as negative affirmations against a given library, i.e. a library left off of the list by SciML is not a negative endorsement. Rather, it means that compatibility with SciML is untested, SciML developers may have a personal preference for another choice, or SciML developers may be simply unaware of the library's existence. If one would like to add a third-party library to the SciML documentation, open a pull request with the requested text.\n\nNote that the libraries in this documentation are only those that are meant to be used in the SciML extended universe of modeling, simulation, and analysis and thus there are many high-quality libraries in other domains (machine learning, data science, etc.) which are purposefully not included. For an overview of the Julia package ecosystem, see the JuliaHub Search Engine.","category":"section"},{"location":"highlevels/symbolic_tools/#Symbolic-Model-Tooling-and-JuliaSymbolics","page":"Symbolic Model Tooling and JuliaSymbolics","title":"Symbolic Model Tooling and JuliaSymbolics","text":"JuliaSymbolics is a sister organization of SciML. It spawned out of the symbolic modeling tools being developed within SciML (ModelingToolkit.jl) to become its own organization dedicated to building a fully-featured Julia-based Computer Algebra System (CAS). As such, the two organizations are closely aligned in terms of its developer community, and many of the SciML libraries use Symbolics.jl extensively.","category":"section"},{"location":"highlevels/symbolic_tools/#ModelOrderReduction.jl:-Automated-Model-Reduction-for-Fast-Approximations-of-Solutions","page":"Symbolic Model Tooling and JuliaSymbolics","title":"ModelOrderReduction.jl: Automated Model Reduction for Fast Approximations of Solutions","text":"ModelOrderReduction.jl is a package for automating the reduction of models. These methods function a submodel with a projection, where solving the smaller model provides approximation information about the full model. MOR.jl uses ModelingToolkit.jl as a system description and automatically transforms equations to the subform, defining the observables to automatically lazily reconstruct the full model on-demand in a fast and stable form.","category":"section"},{"location":"highlevels/symbolic_tools/#Symbolics.jl:-The-Computer-Algebra-System-(CAS)-of-the-Julia-Programming-Language","page":"Symbolic Model Tooling and JuliaSymbolics","title":"Symbolics.jl: The Computer Algebra System (CAS) of the Julia Programming Language","text":"Symbolics.jl is the CAS of the Julia programming language. If something needs to be done symbolically, most likely Symbolics.jl is the answer.","category":"section"},{"location":"highlevels/symbolic_tools/#MetaTheory.jl:-E-Graphs-to-Automate-Symbolic-Transformations","page":"Symbolic Model Tooling and JuliaSymbolics","title":"MetaTheory.jl: E-Graphs to Automate Symbolic Transformations","text":"Metatheory.jl is a library for defining e-graph rewriters for use on the common symbolic interface. This can be used to do all sorts of analysis and code transformations, such as improving code performance, numerical stability, and more. See Automated Code Optimization with E-Graphs for more details.","category":"section"},{"location":"highlevels/symbolic_tools/#SymbolicUtils.jl:-Define-Your-Own-Computer-Algebra-System","page":"Symbolic Model Tooling and JuliaSymbolics","title":"SymbolicUtils.jl: Define Your Own Computer Algebra System","text":"SymbolicUtils.jl is the underlying utility library and rule-based rewriting language on which Symbolics.jl is developed. Symbolics.jl is standardized type and rule definitions built using SymbolicUtils.jl. However, if non-standard types are required, such as symbolic computing over Fock algebras, then SymbolicUtils.jl is the library from which the new symbolic types can be implemented.","category":"section"},{"location":"#SciML:-Differentiable-Modeling-and-Simulation-Combined-with-Machine-Learning","page":"SciML: Open Source Software for Scientific Machine Learning with Julia","title":"SciML: Differentiable Modeling and Simulation Combined with Machine Learning","text":"The SciML organization is a collection of tools for solving equations and modeling systems developed in the Julia programming language with bindings to other languages such as R and Python. The organization provides well-maintained tools which compose together as a coherent ecosystem. It has a coherent development principle, unified APIs over large collections of equation solvers, pervasive differentiability and sensitivity analysis, and features many of the highest performance and parallel implementations one can find.\n\nScientific Machine Learning (SciML) = Scientific Computing + Machine Learning","category":"section"},{"location":"#Where-to-Start?","page":"SciML: Open Source Software for Scientific Machine Learning with Julia","title":"Where to Start?","text":"Want to get started running some code? Check out the Getting Started tutorials.\nWhat is SciML? Check out our Overview.\nWant to see some cool end-to-end examples? Check out the SciML Showcase.\nCurious about our performance claims? Check out the SciML Open Benchmarks.\nWant to learn more about how SciML does scientific machine learning? Check out the SciML Book (from MIT's 18.337 graduate course).\nWant to chat with someone? Check out our chat room and forums.\nWant to see our code? Check out the SciML GitHub organization.\n\nAnd for diving into the details, use the bar on the top to navigate to the submodule of interest!","category":"section"},{"location":"#Reproducibility","page":"SciML: Open Source Software for Scientific Machine Learning with Julia","title":"Reproducibility","text":"<details><summary>The documentation of the <a href=\"showcase/showcase/#showcase\">SciML Showcase</a> was built using these direct dependencies,</summary>\n\nusing Pkg # hide\nPkg.status() # hide\n\n</details>\n\n<details><summary>and using this machine and Julia version.</summary>\n\nusing InteractiveUtils # hide\nversioninfo() # hide\n\n</details>\n\n<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>\n\nusing Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide\n\n</details>\n\nusing TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"section"}]
}
